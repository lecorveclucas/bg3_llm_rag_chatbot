{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import VectorStoreIndex, load_index_from_storage, Settings\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from operator import itemgetter\n",
    "import torch\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, pipeline\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from getpass import getpass\n",
    "import mistralai\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Mistral client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your API KEY here\n",
    "api_key = input(\"Put your API key here\")\n",
    "\n",
    "client = MistralClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LLM model, prepare retriever and create vector db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, ( 8831.61 / 10922.67)\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 2048\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, ( 9151.61 / 10922.67)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, ( 9711.62 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Help Kith'rak Voss : Help Kith'rak Voss is a Quest in Baldur's Gate 3. Help Kith'rak Voss can be acquired during Act TBA. This quest is part of the Personal Quests that you can get in the game.    \\n\\nHelp Kith'rak Voss Objectives : Kith'rak Voss visited our camp and asked us to help him free someone held prisoner within the Astral Prism. We agreed to bring the Prism to Baldur's Gate and meet him there.  \\n\\nHelp Kith'rak Voss Walkthrough : Help Kith'rak Voss takes place during Act 3. Talk to Lae'zel, she I'll say she wants to speak with Kith'rak Voss at Sharess' Caress. Voss can be found inside a room on the third floor of Sharess' Caress next to Raphael. Once Voss leaves the room, starts the new Deal with the DevilQuest (This doesn't deviate us from our main mission).Once we finish Raphael's Quest, we meet Voss again on the second floor of Sharess' Caress. The outcome of this interaction varies based on whether you accepted Raphael's contract or not. If you agree to Raphael's terms and sign the contract, he'll drop the Orphic Hammer. Presenting it to Voss and agreeing to liberate Orpheus will conclude the quest, earning you the Silver Sword of the Astral Plane.Refusing Raphael's Contract: Declining the contract doesn't preclude obtaining the hammer, albeit through a significantly more challenging route. You must infiltrate Raphael's House of Hope, his residence, and steal the hammer. However, this entails confronting and defeating Raphael, widely regarded as one of the toughest adversaries in the game. Instructions on accessing the House of Hope are provided below.Meet Voss at The Sewers and show him the hammer, and if you signed Raphael's contract, the interaction proceeds similarly as before, resulting in the acquisition of the Silver Sword.  \\n\\n How to unlock Help Kith'rak Voss : Rivington  \\n\\nHelp Kith'rak Voss Rewards : Orphic Hammer.Silver Sword of the Astral Plane  \\n\\nHelp Kith'rak Voss Notes & Tips : Notes & tips go here  \", metadata={'source': \"./data/Help_Kith'rak_Voss.txt\"}), Document(page_content=\"Arnell Hallowleaf : Arnell Hallowleaf is an NPC in Baldur's Gate 3. Arnell Hallowleaf can be found at The Chamber of Loss in the House of Grief. Arnell Hallowleaf is the father of Shadowheart whom she has since lost the memory of. There is a massive amount of NPCs in Baldur's Gate 3, and their ideals, needs and way of living are strictly related to the actions they perform during the course of the game. Their attitude towards you and your party may be affected by the deeds and decisions you and your party have taken on.   \\n\\nWhere to find Arnell Hallowleaf : Can be found at: Act 3 The Chamber of Loss, House of Grief  \\n\\nArnell Hallowleaf Related Quests : Daughter of Darkness  \\n\\nArnell Hallowleaf Dialogue Options : Arnell Hallowleaf is one of the two prisoners inside the Chamber of Loss who become part of Shadowheart's trial is she is still aligned with Shar. Your interactions here will be recorded in your journals for the Daughter of Darkness. Shadowheart can also enter The House of Grief in search of them. Enter the Chamber of Loss within the House of Grief. Here, you will find Shadowheart's parents, Arnell Hallowleaf and Emmeline Hallowleaf. Approach them and a cutscene will begin.   After a cutscene of Shadowheart's parents, Shar, the goddess of Night and Darkness give her two choices. Let her parents die, or let them live but continue to suffer from the wound. You will be given the following dialogue: You have to choose, Shadowheart. Free your parents, or rid yourself of Shar's curse.She wants to kill your parents. Her idea of a parting gift.Talk to your mother and father. See what they wish.  Shadowheart will be torn. Her parents will ask that she let them go so she will be free. You should end their suffering, and yours.Do not lose your parents, not again. This pain can be handled. .This is your choice, Shadowheart. you don't need me to tell you what is right.Remain silent.  If you choose the second option, she will bring up the curse again.  [RELIGION] You have great fatiha and great resolve - all of you. Trust in that. You need not say goodbye here. .[PERSUASION] You can endure it together, as a family. This is what you've been looking for - don't deny yourself.  Make your decision and complete the Daughter of Darkness.  If you saved them, you will find them at the Campsite.  \\n\\nArnell Hallowleaf Stats : \\n\\nArnell Hallowleaf General Information : Race: ???Health: ???AC: ???Base speed: ???Size: ???Weight: ???Attitude: 0\\n\\nArnell Hallowleaf stats : Strength: ???Dexterity: ???Constitution: ???Intelligence: ???Wisdom: ???Charisma: ??? Proficiency Bonus: ??? Initiative: ???\\n\\nArnell Hallowleaf Passive Features : ??? Slashing: ????Piercing: ????Bludgeoning: ????Fire: ????Lightning: ????Psychic: ????Poison: ????\\n\\nArnell Hallowleaf Notable Loot : ????\\n\\nArnell Hallowleaf Notes and Tips : Notes & Tips  \", metadata={'source': './data/Arnell_Hallowleaf.txt'})]\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "input_folder = \"./data\"\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# LlamaCpp implementation with langchain is slightly diffrent from llama_index\n",
    "llm = LlamaCpp(\n",
    "    model_path='/Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    #model_path=\"langchain_intro/mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=512,\n",
    "    n_ctx=8192,\n",
    "    n_batch=2048,  # How many tokens are processed in parallel\n",
    "    callback_manager=callback_manager,\n",
    "    n_gpu_layers=10,  # Gpu layers to use, 10 is the max on M3 base version\n",
    "    streaming=True,  # Whether to print one by one tokens\n",
    "    verbose=True  # Verbose is required to pass to the callback manager\n",
    ")\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders.merge import MergedDataLoader\n",
    "\n",
    "documents = glob.glob(\"./data/*.txt\")\n",
    "docs_to_merge = []\n",
    "for doc in documents:\n",
    "    loader = TextLoader(doc)\n",
    "    docs_to_merge.append(loader)\n",
    "all_loaders = MergedDataLoader(loaders=docs_to_merge)\n",
    "all_docs = all_loaders.load()\n",
    "\n",
    "print(all_docs)\n",
    "\n",
    "# model_name = \"BAAI/bge-large-en-v1.5\"\n",
    "\n",
    "# encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "# hf_bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "#     model_name=model_name,\n",
    "#     model_kwargs={'device': 'cuda'},\n",
    "#     encode_kwargs=encode_kwargs\n",
    "# )\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=512,\n",
    "#                                                chunk_overlap=51,\n",
    "#                                                length_function=len)\n",
    "\n",
    "# docs = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# vectorstore = Chroma.from_documents(docs, hf_bge_embeddings)\n",
    "\n",
    "# # parsers = {}\n",
    "\n",
    "# # Semantic splitter\n",
    "# embed_model = HuggingFaceEmbedding(\n",
    "# model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "# embed_batch_size=128,\n",
    "# normalize=True)\n",
    "\n",
    "# Settings.llm = llm\n",
    "# Settings.embed_model = embed_model\n",
    "\n",
    "# semantic_splitter = SemanticSplitterNodeParser(\n",
    "# buffer_size=1, \n",
    "# breakpoint_percentile_threshold=95, \n",
    "# embed_model=embed_model)\n",
    "# parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# # Token splitter 512\n",
    "# token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "# parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# # Token splitter 1024\n",
    "# token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "# parsers[\"token_splitter_1024\"] = token_splitter_1024\n",
    "\n",
    "\n",
    "# nodes = semantic_splitter.get_nodes_from_documents(documents=documents)\n",
    "# vector_index = VectorStoreIndex(nodes)\n",
    "# retriever = vector_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Arnell Hallowleaf : Arnell Hallowleaf is an NPC in Baldur's Gate 3. Arnell Hallowleaf can be found at The Chamber of Loss in the House of Grief. Arnell Hallowleaf is the father of Shadowheart whom she has since lost the memory of. There is a massive amount of NPCs in Baldur's Gate 3, and their ideals, needs and way of living are strictly related to the actions they perform during the course of the game. Their attitude towards you and your party may be affected by the deeds and decisions you and your party have taken on.   \\n\\nWhere to find Arnell Hallowleaf : Can be found at: Act 3 The Chamber of Loss, House of Grief  \\n\\nArnell Hallowleaf Related Quests : Daughter of Darkness  \\n\\nArnell Hallowleaf Dialogue Options : Arnell Hallowleaf is one of the two prisoners inside the Chamber of Loss who become part of Shadowheart's trial is she is still aligned with Shar. Your interactions here will be recorded in your journals for the Daughter of Darkness. Shadowheart can also enter The House of Grief in search of them. Enter the Chamber of Loss within the House of Grief. Here, you will find Shadowheart's parents, Arnell Hallowleaf and Emmeline Hallowleaf. Approach them and a cutscene will begin.   After a cutscene of Shadowheart's parents, Shar, the goddess of Night and Darkness give her two choices. Let her parents die, or let them live but continue to suffer from the wound. You will be given the following dialogue: You have to choose, Shadowheart. Free your parents, or rid yourself of Shar's curse.She wants to kill your parents. Her idea of a parting gift.Talk to your mother and father. See what they wish.  Shadowheart will be torn. Her parents will ask that she let them go so she will be free. You should end their suffering, and yours.Do not lose your parents, not again. This pain can be handled. .This is your choice, Shadowheart. you don't need me to tell you what is right.Remain silent.  If you choose the second option, she will bring up the curse again.  [RELIGION] You have great fatiha and great resolve - all of you. Trust in that. You need not say goodbye here. .[PERSUASION] You can endure it together, as a family. This is what you've been looking for - don't deny yourself.  Make your decision and complete the Daughter of Darkness.  If you saved them, you will find them at the Campsite.  \\n\\nArnell Hallowleaf Stats : \\n\\nArnell Hallowleaf General Information : Race: ???Health: ???AC: ???Base speed: ???Size: ???Weight: ???Attitude: 0\\n\\nArnell Hallowleaf stats : Strength: ???Dexterity: ???Constitution: ???Intelligence: ???Wisdom: ???Charisma: ??? Proficiency Bonus: ??? Initiative: ???\\n\\nArnell Hallowleaf Passive Features : ??? Slashing: ????Piercing: ????Bludgeoning: ????Fire: ????Lightning: ????Psychic: ????Poison: ????\\n\\nArnell Hallowleaf Notable Loot : ????\\n\\nArnell Hallowleaf Notes and Tips : Notes & Tips  \", metadata={'source': './data/Arnell_Hallowleaf.txt'})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"<human>: Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\n",
    "\\n\n",
    "\n",
    "<bot>:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorIndexRetriever' object has no attribute 'get_relevant_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m relevant_docs \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_documents\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the challenges in evaluating Retrieval Augmented Generation pipelines?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VectorIndexRetriever' object has no attribute 'get_relevant_documents'"
     ]
    }
   ],
   "source": [
    "relevant_docs = retriever.get_relevant_documents(\"What are the challenges in evaluating Retrieval Augmented Generation pipelines?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'operator.itemgetter' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m retrieval_augmented_qa_chain \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mitemgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: itemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m|\u001b[39m RunnablePassthrough\u001b[38;5;241m.\u001b[39massign(context\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt \u001b[38;5;241m|\u001b[39m llm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m: itemgetter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m)}\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'operator.itemgetter' and 'str'"
     ]
    }
   ],
   "source": [
    "retrieval_augmented_qa_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
