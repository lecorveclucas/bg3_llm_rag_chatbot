{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import re\n",
    "import pandas as pd\n",
    "import jsonpickle\n",
    "import numpy as np\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter, SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, RetrieverEvaluator\n",
    "from llama_index.core.prompts import BasePromptTemplate, PromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "import nest_asyncio\n",
    "\n",
    "# To allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define constants\n",
    "results_folder = os.path.join(\"data_evaluation\", \"full_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(text):\n",
    "    # Define the regex pattern\n",
    "    pattern = r\"My score =\\s*(\\d+(?:\\.\\d*)?)(?:\\s*[\\.\\:\\,\\;\\!\\?]|\\s*\\n)\"\n",
    "    \n",
    "    # Search for the first match in the text\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    # Return the first match if found, else return None\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correctness_relevancy_answer_relevancy_faithfulness_results(eval_results_dict: dict):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    full_df = pd.DataFrame()\n",
    "    for name, eval_results in eval_results_dict.items():\n",
    "        faithfulness_score = sum(extract_score(result.feedback) for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "        relevancy_score = sum(result.score for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
    "        correctness_score = (sum(result.score for result in eval_results['correctness']) / len(eval_results['correctness'])) / 5  # Max = 5 points\n",
    "        answer_relevancy_score = sum(extract_score(result.feedback) for result in eval_results['answer_relevancy']) / len(eval_results['answer_relevancy'])\n",
    "        mean_score = np.mean([faithfulness_score, relevancy_score, correctness_score, answer_relevancy_score])\n",
    "        nodes_nbr = eval_results[\"nodes_nbr\"]\n",
    "\n",
    "        metric_df = pd.DataFrame(\n",
    "            {\"retriever_name\": [name], \"faithfulness\": [faithfulness_score], \"relevancy\": [relevancy_score],\n",
    "             \"correctness\": [correctness_score], \"answer_relevancy\": [answer_relevancy_score], \"nodes_number\": nodes_nbr,\n",
    "             \"mean_score\": mean_score}\n",
    "        )\n",
    "\n",
    "        full_df = pd.concat([full_df, metric_df])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hit_hrr_results(eval_results_dict: dict):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    full_df = pd.DataFrame()\n",
    "    for name, eval_results in eval_results_dict.items():\n",
    "        metric_dicts = []\n",
    "        for eval_result in eval_results:\n",
    "            metric_dict = eval_result.metric_vals_dict\n",
    "            metric_dicts.append(metric_dict)\n",
    "\n",
    "        df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "        hit_rate = df[\"hit_rate\"].mean()\n",
    "        mrr = df[\"mrr\"].mean()\n",
    "\n",
    "        metric_df = pd.DataFrame(\n",
    "            {\"retriever_name\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "        )\n",
    "\n",
    "        full_df = pd.concat([full_df, metric_df])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, ( 5258.59 / 10922.67)\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, ( 5579.59 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, ( 6139.61 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "input_folder = \"./data_evaluation/batch_1\"\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic_splitter \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.02 ms /   154 runs   (    0.08 ms per token, 12810.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18053.94 ms /   756 tokens (   23.88 ms per token,    41.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11712.03 ms /   153 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   29966.20 ms /   909 tokens\n",
      "  1%|          | 1/101 [00:29<49:57, 29.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    62 runs   (    0.08 ms per token, 12889.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2715.31 ms /   117 tokens (   23.21 ms per token,    43.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.40 ms /    61 runs   (   71.14 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    7136.51 ms /   178 tokens\n",
      "  2%|▏         | 2/101 [00:37<27:17, 16.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.21 ms /    99 runs   (    0.07 ms per token, 13736.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2374.18 ms /   117 tokens (   20.29 ms per token,    49.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6801.93 ms /    98 runs   (   69.41 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    9300.17 ms /   215 tokens\n",
      "  3%|▎         | 3/101 [00:46<21:37, 13.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.63 ms /    49 runs   (    0.07 ms per token, 13506.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2216.13 ms /    92 tokens (   24.09 ms per token,    41.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3145.71 ms /    48 runs   (   65.54 ms per token,    15.26 tokens per second)\n",
      "llama_print_timings:       total time =    5423.16 ms /   140 tokens\n",
      "  4%|▍         | 4/101 [00:51<16:25, 10.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.45 ms /    48 runs   (    0.07 ms per token, 13913.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2822.10 ms /   153 tokens (   18.45 ms per token,    54.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3074.91 ms /    47 runs   (   65.42 ms per token,    15.29 tokens per second)\n",
      "llama_print_timings:       total time =    5956.77 ms /   200 tokens\n",
      "  5%|▍         | 5/101 [00:57<13:49,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.06 ms /    75 runs   (    0.08 ms per token, 12374.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6111.74 ms /   452 tokens (   13.52 ms per token,    73.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5896.77 ms /    74 runs   (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =   12110.01 ms /   526 tokens\n",
      "  6%|▌         | 6/101 [01:09<15:33,  9.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.59 ms /    91 runs   (    0.08 ms per token, 11986.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4366.06 ms /   192 tokens (   22.74 ms per token,    43.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9057.28 ms /    90 runs   (  100.64 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time =   13555.55 ms /   282 tokens\n",
      "  7%|▋         | 7/101 [01:23<17:18, 11.05s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.03 ms /   170 runs   (    0.08 ms per token, 13045.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3868.34 ms /   269 tokens (   14.38 ms per token,    69.54 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.56 ms /   169 runs   (   66.83 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =   15380.27 ms /   438 tokens\n",
      "  8%|▊         | 8/101 [01:38<19:15, 12.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.73 ms /    64 runs   (    0.07 ms per token, 13536.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8644.96 ms /   624 tokens (   13.85 ms per token,    72.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4350.16 ms /    63 runs   (   69.05 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time =   13076.46 ms /   687 tokens\n",
      "  9%|▉         | 9/101 [01:51<19:22, 12.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.09 ms /    68 runs   (    0.07 ms per token, 13351.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2853.48 ms /   146 tokens (   19.54 ms per token,    51.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4613.45 ms /    67 runs   (   68.86 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    7553.73 ms /   213 tokens\n",
      " 10%|▉         | 10/101 [01:59<16:47, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.57 ms /    68 runs   (    0.08 ms per token, 12210.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3304.26 ms /   178 tokens (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6464.45 ms /    67 runs   (   96.48 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =    9861.08 ms /   245 tokens\n",
      " 11%|█         | 11/101 [02:09<16:03, 10.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.03 ms /    87 runs   (    0.08 ms per token, 12380.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.52 ms /   300 tokens (   13.97 ms per token,    71.57 tokens per second)\n",
      "llama_print_timings:        eval time =    6623.54 ms /    86 runs   (   77.02 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =   10934.55 ms /   386 tokens\n",
      " 12%|█▏        | 12/101 [02:20<15:58, 10.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /   102 runs   (    0.08 ms per token, 13058.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8880.03 ms /   618 tokens (   14.37 ms per token,    69.59 tokens per second)\n",
      "llama_print_timings:        eval time =    7994.84 ms /   101 runs   (   79.16 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   17014.98 ms /   719 tokens\n",
      " 13%|█▎        | 13/101 [02:37<18:34, 12.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    78 runs   (    0.07 ms per token, 13605.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4438.62 ms /   333 tokens (   13.33 ms per token,    75.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5137.80 ms /    77 runs   (   66.72 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =    9675.69 ms /   410 tokens\n",
      " 14%|█▍        | 14/101 [02:47<17:03, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      17.00 ms /   229 runs   (    0.07 ms per token, 13466.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8395.27 ms /   599 tokens (   14.02 ms per token,    71.35 tokens per second)\n",
      "llama_print_timings:        eval time =   15630.23 ms /   228 runs   (   68.55 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =   24327.05 ms /   827 tokens\n",
      " 15%|█▍        | 15/101 [03:11<22:17, 15.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    88 runs   (    0.07 ms per token, 13645.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9384.82 ms /   611 tokens (   15.36 ms per token,    65.11 tokens per second)\n",
      "llama_print_timings:        eval time =    6306.25 ms /    87 runs   (   72.49 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   15809.17 ms /   698 tokens\n",
      " 16%|█▌        | 16/101 [03:27<22:08, 15.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.11 ms /    81 runs   (    0.08 ms per token, 13250.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4627.50 ms /   339 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5908.62 ms /    80 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =   10644.06 ms /   419 tokens\n",
      " 17%|█▋        | 17/101 [03:37<19:47, 14.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    70 runs   (    0.08 ms per token, 12544.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9340.30 ms /   590 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5598.02 ms /    69 runs   (   81.13 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   15038.52 ms /   659 tokens\n",
      " 18%|█▊        | 18/101 [03:52<19:55, 14.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.06 ms /    63 runs   (    0.08 ms per token, 12457.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6364.06 ms /   493 tokens (   12.91 ms per token,    77.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5290.84 ms /    62 runs   (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_print_timings:       total time =   11742.35 ms /   555 tokens\n",
      " 19%|█▉        | 19/101 [04:04<18:36, 13.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    69 runs   (    0.08 ms per token, 12062.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3179.33 ms /   174 tokens (   18.27 ms per token,    54.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.85 ms /    68 runs   (   85.48 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =    9091.19 ms /   242 tokens\n",
      " 20%|█▉        | 20/101 [04:13<16:32, 12.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    90 runs   (    0.08 ms per token, 12710.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3654.85 ms /   195 tokens (   18.74 ms per token,    53.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9051.43 ms /    89 runs   (  101.70 ms per token,     9.83 tokens per second)\n",
      "llama_print_timings:       total time =   12829.68 ms /   284 tokens\n",
      " 21%|██        | 21/101 [04:26<16:34, 12.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    68 runs   (    0.07 ms per token, 13570.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12968.38 ms /   907 tokens (   14.30 ms per token,    69.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.62 ms /    67 runs   (   74.31 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =   18040.43 ms /   974 tokens\n",
      " 22%|██▏       | 22/101 [04:44<18:35, 14.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.82 ms /    77 runs   (    0.08 ms per token, 13239.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.10 ms /   182 tokens (   16.52 ms per token,    60.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5091.00 ms /    76 runs   (   66.99 ms per token,    14.93 tokens per second)\n",
      "llama_print_timings:       total time =    8197.42 ms /   258 tokens\n",
      " 23%|██▎       | 23/101 [04:52<16:02, 12.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.13 ms /    78 runs   (    0.08 ms per token, 12718.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3497.20 ms /   218 tokens (   16.04 ms per token,    62.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5649.99 ms /    77 runs   (   73.38 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =    9253.89 ms /   295 tokens\n",
      " 24%|██▍       | 24/101 [05:02<14:39, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.26 ms /    94 runs   (    0.08 ms per token, 12945.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11297.81 ms /   836 tokens (   13.51 ms per token,    74.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7583.21 ms /    93 runs   (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =   19017.22 ms /   929 tokens\n",
      " 25%|██▍       | 25/101 [05:21<17:21, 13.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.36 ms /    92 runs   (    0.08 ms per token, 12505.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5946.19 ms /   456 tokens (   13.04 ms per token,    76.69 tokens per second)\n",
      "llama_print_timings:        eval time =    7150.57 ms /    91 runs   (   78.58 ms per token,    12.73 tokens per second)\n",
      "llama_print_timings:       total time =   13232.47 ms /   547 tokens\n",
      " 26%|██▌       | 26/101 [05:34<16:57, 13.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    61 runs   (    0.09 ms per token, 11344.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.48 ms /   267 tokens (   15.24 ms per token,    65.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4890.34 ms /    60 runs   (   81.51 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    9049.37 ms /   327 tokens\n",
      " 27%|██▋       | 27/101 [05:43<15:03, 12.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    71 runs   (    0.08 ms per token, 12384.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2823.14 ms /   142 tokens (   19.88 ms per token,    50.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5084.73 ms /    70 runs   (   72.64 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    8004.18 ms /   212 tokens\n",
      " 28%|██▊       | 28/101 [05:51<13:19, 10.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    76 runs   (    0.07 ms per token, 13615.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6649.52 ms /   516 tokens (   12.89 ms per token,    77.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5149.99 ms /    75 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =   11897.63 ms /   591 tokens\n",
      " 29%|██▊       | 29/101 [06:03<13:29, 11.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    72 runs   (    0.07 ms per token, 13748.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3582.96 ms /   254 tokens (   14.11 ms per token,    70.89 tokens per second)\n",
      "llama_print_timings:        eval time =    4683.86 ms /    71 runs   (   65.97 ms per token,    15.16 tokens per second)\n",
      "llama_print_timings:       total time =    8357.64 ms /   325 tokens\n",
      " 30%|██▉       | 30/101 [06:11<12:16, 10.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    55 runs   (    0.07 ms per token, 13657.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5613.61 ms /   454 tokens (   12.36 ms per token,    80.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3629.55 ms /    54 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    9312.68 ms /   508 tokens\n",
      " 31%|███       | 31/101 [06:21<11:43, 10.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    54 runs   (    0.07 ms per token, 13650.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2506.56 ms /   118 tokens (   21.24 ms per token,    47.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3451.56 ms /    53 runs   (   65.12 ms per token,    15.36 tokens per second)\n",
      "llama_print_timings:       total time =    6024.10 ms /   171 tokens\n",
      " 32%|███▏      | 32/101 [06:27<10:10,  8.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    84 runs   (    0.07 ms per token, 14098.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10013.06 ms /   766 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5747.01 ms /    83 runs   (   69.24 ms per token,    14.44 tokens per second)\n",
      "llama_print_timings:       total time =   15868.22 ms /   849 tokens\n",
      " 33%|███▎      | 33/101 [06:42<12:24, 10.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /   108 runs   (    0.07 ms per token, 13966.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4652.02 ms /   353 tokens (   13.18 ms per token,    75.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7134.48 ms /   107 runs   (   66.68 ms per token,    15.00 tokens per second)\n",
      "llama_print_timings:       total time =   11923.06 ms /   460 tokens\n",
      " 34%|███▎      | 34/101 [06:54<12:33, 11.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    63 runs   (    0.08 ms per token, 13174.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2683.18 ms /   129 tokens (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4030.98 ms /    62 runs   (   65.02 ms per token,    15.38 tokens per second)\n",
      "llama_print_timings:       total time =    6792.14 ms /   191 tokens\n",
      " 35%|███▍      | 35/101 [07:01<10:54,  9.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    64 runs   (    0.07 ms per token, 13468.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3034.21 ms /   188 tokens (   16.14 ms per token,    61.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4119.95 ms /    63 runs   (   65.40 ms per token,    15.29 tokens per second)\n",
      "llama_print_timings:       total time =    7235.23 ms /   251 tokens\n",
      " 36%|███▌      | 36/101 [07:08<09:52,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    55 runs   (    0.07 ms per token, 13681.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15335.77 ms /  1152 tokens (   13.31 ms per token,    75.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3812.76 ms /    54 runs   (   70.61 ms per token,    14.16 tokens per second)\n",
      "llama_print_timings:       total time =   19221.02 ms /  1206 tokens\n",
      " 37%|███▋      | 37/101 [07:28<12:57, 12.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    85 runs   (    0.08 ms per token, 13032.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27712.60 ms /  2065 tokens (   13.42 ms per token,    74.51 tokens per second)\n",
      "llama_print_timings:        eval time =    7479.48 ms /    84 runs   (   89.04 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   35309.09 ms /  2149 tokens\n",
      " 38%|███▊      | 38/101 [08:03<20:03, 19.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    95 runs   (    0.08 ms per token, 13036.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11346.44 ms /   716 tokens (   15.85 ms per token,    63.10 tokens per second)\n",
      "llama_print_timings:        eval time =    6569.49 ms /    94 runs   (   69.89 ms per token,    14.31 tokens per second)\n",
      "llama_print_timings:       total time =   18041.89 ms /   810 tokens\n",
      " 39%|███▊      | 39/101 [08:21<19:24, 18.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.60 ms /    62 runs   (    0.07 ms per token, 13478.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2126.61 ms /    85 tokens (   25.02 ms per token,    39.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3956.54 ms /    61 runs   (   64.86 ms per token,    15.42 tokens per second)\n",
      "llama_print_timings:       total time =    6161.03 ms /   146 tokens\n",
      " 40%|███▉      | 40/101 [08:27<15:14, 15.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    52 runs   (    0.08 ms per token, 13147.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2756.61 ms /   133 tokens (   20.73 ms per token,    48.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3330.74 ms /    51 runs   (   65.31 ms per token,    15.31 tokens per second)\n",
      "llama_print_timings:       total time =    6152.98 ms /   184 tokens\n",
      " 41%|████      | 41/101 [08:33<12:20, 12.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    68 runs   (    0.08 ms per token, 12705.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2720.64 ms /   137 tokens (   19.86 ms per token,    50.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4464.39 ms /    67 runs   (   66.63 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7271.58 ms /   204 tokens\n",
      " 42%|████▏     | 42/101 [08:41<10:38, 10.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    72 runs   (    0.07 ms per token, 14299.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10358.97 ms /   759 tokens (   13.65 ms per token,    73.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4931.52 ms /    71 runs   (   69.46 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   15384.65 ms /   830 tokens\n",
      " 43%|████▎     | 43/101 [08:56<11:47, 12.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.71 ms /   101 runs   (    0.08 ms per token, 13101.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9635.84 ms /   697 tokens (   13.82 ms per token,    72.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6873.34 ms /   100 runs   (   68.73 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   16639.64 ms /   797 tokens\n",
      " 44%|████▎     | 44/101 [09:13<12:51, 13.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    69 runs   (    0.07 ms per token, 13487.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2337.28 ms /    79 tokens (   29.59 ms per token,    33.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4428.89 ms /    68 runs   (   65.13 ms per token,    15.35 tokens per second)\n",
      "llama_print_timings:       total time =    6854.89 ms /   147 tokens\n",
      " 45%|████▍     | 45/101 [09:19<10:45, 11.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.87 ms /    76 runs   (    0.08 ms per token, 12958.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2680.85 ms /   145 tokens (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4884.47 ms /    75 runs   (   65.13 ms per token,    15.35 tokens per second)\n",
      "llama_print_timings:       total time =    7661.71 ms /   220 tokens\n",
      " 46%|████▌     | 46/101 [09:27<09:30, 10.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    85 runs   (    0.07 ms per token, 13571.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4267.50 ms /   304 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_print_timings:        eval time =    5564.57 ms /    84 runs   (   66.24 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    9940.29 ms /   388 tokens\n",
      " 47%|████▋     | 47/101 [09:37<09:13, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    90 runs   (    0.07 ms per token, 13499.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4733.17 ms /   358 tokens (   13.22 ms per token,    75.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5987.77 ms /    89 runs   (   67.28 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =   10834.85 ms /   447 tokens\n",
      " 48%|████▊     | 48/101 [09:48<09:12, 10.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    68 runs   (    0.07 ms per token, 13798.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4540.02 ms /   347 tokens (   13.08 ms per token,    76.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4442.06 ms /    67 runs   (   66.30 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:       total time =    9068.50 ms /   414 tokens\n",
      " 49%|████▊     | 49/101 [09:57<08:40, 10.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.39 ms /   132 runs   (    0.07 ms per token, 14056.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9243.09 ms /   662 tokens (   13.96 ms per token,    71.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9025.65 ms /   131 runs   (   68.90 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =   18440.69 ms /   793 tokens\n",
      " 50%|████▉     | 50/101 [10:15<10:39, 12.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.03 ms /    40 runs   (    0.08 ms per token, 13214.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2379.99 ms /    80 tokens (   29.75 ms per token,    33.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2531.40 ms /    39 runs   (   64.91 ms per token,    15.41 tokens per second)\n",
      "llama_print_timings:       total time =    4962.16 ms /   119 tokens\n",
      " 50%|█████     | 51/101 [10:20<08:33, 10.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    66 runs   (    0.07 ms per token, 13678.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2347.44 ms /   107 tokens (   21.94 ms per token,    45.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4224.37 ms /    65 runs   (   64.99 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:       total time =    6655.26 ms /   172 tokens\n",
      " 51%|█████▏    | 52/101 [10:27<07:30,  9.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.95 ms /    97 runs   (    0.07 ms per token, 13952.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3592.46 ms /   229 tokens (   15.69 ms per token,    63.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6353.25 ms /    96 runs   (   66.18 ms per token,    15.11 tokens per second)\n",
      "llama_print_timings:       total time =   10068.10 ms /   325 tokens\n",
      " 52%|█████▏    | 53/101 [10:37<07:33,  9.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.16 ms /    71 runs   (    0.07 ms per token, 13759.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15779.51 ms /  1142 tokens (   13.82 ms per token,    72.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4944.96 ms /    70 runs   (   70.64 ms per token,    14.16 tokens per second)\n",
      "llama_print_timings:       total time =   20817.61 ms /  1212 tokens\n",
      " 53%|█████▎    | 54/101 [10:58<10:04, 12.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    66 runs   (    0.07 ms per token, 13496.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3347.24 ms /   206 tokens (   16.25 ms per token,    61.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4393.87 ms /    65 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    7825.99 ms /   271 tokens\n",
      " 54%|█████▍    | 55/101 [11:06<08:42, 11.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.88 ms /    79 runs   (    0.07 ms per token, 13430.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4533.76 ms /   334 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_print_timings:        eval time =    5193.70 ms /    78 runs   (   66.59 ms per token,    15.02 tokens per second)\n",
      "llama_print_timings:       total time =    9828.69 ms /   412 tokens\n",
      " 55%|█████▌    | 56/101 [11:16<08:10, 10.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    73 runs   (    0.07 ms per token, 13729.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6682.82 ms /   516 tokens (   12.95 ms per token,    77.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4880.73 ms /    72 runs   (   67.79 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =   11658.46 ms /   588 tokens\n",
      " 56%|█████▋    | 57/101 [11:27<08:09, 11.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    68 runs   (    0.07 ms per token, 13500.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4284.06 ms /   311 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4445.35 ms /    67 runs   (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    8814.72 ms /   378 tokens\n",
      " 57%|█████▋    | 58/101 [11:36<07:28, 10.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    45 runs   (    0.07 ms per token, 13521.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2425.11 ms /   100 tokens (   24.25 ms per token,    41.24 tokens per second)\n",
      "llama_print_timings:        eval time =    2858.54 ms /    44 runs   (   64.97 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:       total time =    5340.26 ms /   144 tokens\n",
      " 58%|█████▊    | 59/101 [11:41<06:14,  8.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    63 runs   (    0.07 ms per token, 13840.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10063.49 ms /   762 tokens (   13.21 ms per token,    75.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4284.13 ms /    62 runs   (   69.10 ms per token,    14.47 tokens per second)\n",
      "llama_print_timings:       total time =   14429.31 ms /   824 tokens\n",
      " 59%|█████▉    | 60/101 [11:56<07:13, 10.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.83 ms /    75 runs   (    0.08 ms per token, 12862.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3070.44 ms /   166 tokens (   18.50 ms per token,    54.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4842.31 ms /    74 runs   (   65.44 ms per token,    15.28 tokens per second)\n",
      "llama_print_timings:       total time =    8008.11 ms /   240 tokens\n",
      " 60%|██████    | 61/101 [12:04<06:31,  9.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    63 runs   (    0.07 ms per token, 13639.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6585.83 ms /   515 tokens (   12.79 ms per token,    78.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4194.18 ms /    62 runs   (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time =   10861.43 ms /   577 tokens\n",
      " 61%|██████▏   | 62/101 [12:15<06:34, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    72 runs   (    0.08 ms per token, 12598.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8551.43 ms /   573 tokens (   14.92 ms per token,    67.01 tokens per second)\n",
      "llama_print_timings:        eval time =   70224.48 ms /    71 runs   (  989.08 ms per token,     1.01 tokens per second)\n",
      "llama_print_timings:       total time =   78887.62 ms /   644 tokens\n",
      " 62%|██████▏   | 63/101 [13:34<19:28, 30.75s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.56 ms /    88 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9738.12 ms /   729 tokens (   13.36 ms per token,    74.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5999.55 ms /    87 runs   (   68.96 ms per token,    14.50 tokens per second)\n",
      "llama_print_timings:       total time =   15852.73 ms /   816 tokens\n",
      " 63%|██████▎   | 64/101 [13:49<16:12, 26.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    54 runs   (    0.07 ms per token, 13636.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5696.74 ms /   453 tokens (   12.58 ms per token,    79.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3678.43 ms /    53 runs   (   69.40 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    9444.78 ms /   506 tokens\n",
      " 64%|██████▍   | 65/101 [13:59<12:44, 21.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.69 ms /   107 runs   (    0.07 ms per token, 13923.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14913.51 ms /  1076 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =    7521.71 ms /   106 runs   (   70.96 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =   22574.97 ms /  1182 tokens\n",
      " 65%|██████▌   | 66/101 [14:21<12:37, 21.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.64 ms /    61 runs   (    0.08 ms per token, 13140.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3448.49 ms /   218 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =    3962.92 ms /    60 runs   (   66.05 ms per token,    15.14 tokens per second)\n",
      "llama_print_timings:       total time =    7488.64 ms /   278 tokens\n",
      " 66%|██████▋   | 67/101 [14:29<09:51, 17.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    54 runs   (    0.07 ms per token, 13892.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2358.95 ms /   124 tokens (   19.02 ms per token,    52.57 tokens per second)\n",
      "llama_print_timings:        eval time =    3441.22 ms /    53 runs   (   64.93 ms per token,    15.40 tokens per second)\n",
      "llama_print_timings:       total time =    5868.16 ms /   177 tokens\n",
      " 67%|██████▋   | 68/101 [14:35<07:39, 13.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.70 ms /    64 runs   (    0.07 ms per token, 13611.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2971.13 ms /   167 tokens (   17.79 ms per token,    56.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4106.99 ms /    63 runs   (   65.19 ms per token,    15.34 tokens per second)\n",
      "llama_print_timings:       total time =    7158.34 ms /   230 tokens\n",
      " 68%|██████▊   | 69/101 [14:42<06:20, 11.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.44 ms /   158 runs   (    0.07 ms per token, 13812.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10063.91 ms /   739 tokens (   13.62 ms per token,    73.43 tokens per second)\n",
      "llama_print_timings:        eval time =   10837.10 ms /   157 runs   (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   21108.87 ms /   896 tokens\n",
      " 69%|██████▉   | 70/101 [15:03<07:34, 14.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    80 runs   (    0.07 ms per token, 13665.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8523.91 ms /   593 tokens (   14.37 ms per token,    69.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5372.03 ms /    79 runs   (   68.00 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =   13998.87 ms /   672 tokens\n",
      " 70%|███████   | 71/101 [15:17<07:14, 14.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    87 runs   (    0.07 ms per token, 13644.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11457.66 ms /   897 tokens (   12.77 ms per token,    78.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5998.21 ms /    86 runs   (   69.75 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =   17568.60 ms /   983 tokens\n",
      " 71%|███████▏  | 72/101 [15:35<07:26, 15.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    74 runs   (    0.07 ms per token, 13915.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5639.66 ms /   473 tokens (   11.92 ms per token,    83.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5956.81 ms /    73 runs   (   81.60 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =   11692.81 ms /   546 tokens\n",
      " 72%|███████▏  | 73/101 [15:46<06:40, 14.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /   132 runs   (    0.07 ms per token, 14369.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  356557.04 ms /   244 tokens ( 1461.30 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:        eval time =    8954.80 ms /   131 runs   (   68.36 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =  365678.39 ms /   375 tokens\n",
      " 73%|███████▎  | 74/101 [21:52<53:52, 119.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.75 ms /   149 runs   (    0.08 ms per token, 12676.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11508.01 ms /   859 tokens (   13.40 ms per token,    74.64 tokens per second)\n",
      "llama_print_timings:        eval time =   11060.66 ms /   148 runs   (   74.73 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =   22766.81 ms /  1007 tokens\n",
      " 74%|███████▍  | 75/101 [22:15<39:16, 90.63s/it] Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.42 ms /    61 runs   (    0.07 ms per token, 13816.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4074.67 ms /   303 tokens (   13.45 ms per token,    74.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4168.39 ms /    60 runs   (   69.47 ms per token,    14.39 tokens per second)\n",
      "llama_print_timings:       total time =    8319.99 ms /   363 tokens\n",
      " 75%|███████▌  | 76/101 [22:23<27:28, 65.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    61 runs   (    0.09 ms per token, 11570.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24307.61 ms /  1584 tokens (   15.35 ms per token,    65.16 tokens per second)\n",
      "llama_print_timings:        eval time =   16621.23 ms /    60 runs   (  277.02 ms per token,     3.61 tokens per second)\n",
      "llama_print_timings:       total time =   41044.39 ms /  1644 tokens\n",
      " 76%|███████▌  | 77/101 [23:06<23:34, 58.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.61 ms /   179 runs   (    0.09 ms per token, 11468.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7013.49 ms /   506 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =   18646.15 ms /   178 runs   (  104.75 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =   25931.07 ms /   684 tokens\n",
      " 77%|███████▋  | 78/101 [23:32<18:48, 49.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /    38 runs   (    0.09 ms per token, 11695.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2525.99 ms /    99 tokens (   25.52 ms per token,    39.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3814.23 ms /    37 runs   (  103.09 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =    6401.95 ms /   136 tokens\n",
      " 78%|███████▊  | 79/101 [23:38<13:17, 36.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    71 runs   (    0.08 ms per token, 12434.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15360.18 ms /  1019 tokens (   15.07 ms per token,    66.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5798.19 ms /    70 runs   (   82.83 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   21262.04 ms /  1089 tokens\n",
      " 79%|███████▉  | 80/101 [23:59<11:06, 31.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.72 ms /    83 runs   (    0.08 ms per token, 12356.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4700.67 ms /   335 tokens (   14.03 ms per token,    71.27 tokens per second)\n",
      "llama_print_timings:        eval time =    6717.41 ms /    82 runs   (   81.92 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   11533.44 ms /   417 tokens\n",
      " 80%|████████  | 81/101 [24:11<08:33, 25.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.15 ms /    89 runs   (    0.08 ms per token, 12452.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17058.64 ms /  1161 tokens (   14.69 ms per token,    68.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7136.93 ms /    88 runs   (   81.10 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   24328.10 ms /  1249 tokens\n",
      " 81%|████████  | 82/101 [24:35<08:00, 25.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    82 runs   (    0.08 ms per token, 13022.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25919.98 ms /  1758 tokens (   14.74 ms per token,    67.82 tokens per second)\n",
      "llama_print_timings:        eval time =    6725.02 ms /    81 runs   (   83.02 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   32769.70 ms /  1839 tokens\n",
      " 82%|████████▏ | 83/101 [25:08<08:15, 27.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    50 runs   (    0.08 ms per token, 12623.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2859.20 ms /   121 tokens (   23.63 ms per token,    42.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4765.58 ms /    49 runs   (   97.26 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =    7714.10 ms /   170 tokens\n",
      " 83%|████████▎ | 84/101 [25:16<06:07, 21.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.73 ms /    86 runs   (    0.08 ms per token, 12782.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12521.34 ms /   844 tokens (   14.84 ms per token,    67.40 tokens per second)\n",
      "llama_print_timings:        eval time =    6325.04 ms /    85 runs   (   74.41 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   18967.12 ms /   929 tokens\n",
      " 84%|████████▍ | 85/101 [25:35<05:32, 20.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      18.30 ms /   231 runs   (    0.08 ms per token, 12623.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5423.32 ms /   442 tokens (   12.27 ms per token,    81.50 tokens per second)\n",
      "llama_print_timings:        eval time =   17240.24 ms /   230 runs   (   74.96 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =   22977.78 ms /   672 tokens\n",
      " 85%|████████▌ | 86/101 [25:58<05:21, 21.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.55 ms /   120 runs   (    0.08 ms per token, 12560.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3063.80 ms /   188 tokens (   16.30 ms per token,    61.36 tokens per second)\n",
      "llama_print_timings:        eval time =    8128.14 ms /   119 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   11345.97 ms /   307 tokens\n",
      " 86%|████████▌ | 87/101 [26:09<04:17, 18.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    82 runs   (    0.07 ms per token, 14238.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8081.68 ms /   566 tokens (   14.28 ms per token,    70.03 tokens per second)\n",
      "llama_print_timings:        eval time =    5899.44 ms /    81 runs   (   72.83 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time =   14082.55 ms /   647 tokens\n",
      " 87%|████████▋ | 88/101 [26:23<03:42, 17.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    83 runs   (    0.07 ms per token, 13828.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5635.79 ms /   417 tokens (   13.52 ms per token,    73.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6015.82 ms /    82 runs   (   73.36 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   11757.59 ms /   499 tokens\n",
      " 88%|████████▊ | 89/101 [26:35<03:06, 15.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    86 runs   (    0.07 ms per token, 14266.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4664.00 ms /   379 tokens (   12.31 ms per token,    81.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5959.27 ms /    85 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   10729.90 ms /   464 tokens\n",
      " 89%|████████▉ | 90/101 [26:46<02:34, 14.08s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    67 runs   (    0.08 ms per token, 13214.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2893.83 ms /   169 tokens (   17.12 ms per token,    58.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4405.55 ms /    66 runs   (   66.75 ms per token,    14.98 tokens per second)\n",
      "llama_print_timings:       total time =    7389.63 ms /   235 tokens\n",
      " 90%|█████████ | 91/101 [26:53<02:00, 12.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.08 ms /   125 runs   (    0.07 ms per token, 13768.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11879.31 ms /   947 tokens (   12.54 ms per token,    79.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9665.86 ms /   124 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   21711.44 ms /  1071 tokens\n",
      " 91%|█████████ | 92/101 [27:15<02:14, 14.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    55 runs   (    0.08 ms per token, 13205.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4606.09 ms /   337 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_print_timings:        eval time =    3958.55 ms /    54 runs   (   73.31 ms per token,    13.64 tokens per second)\n",
      "llama_print_timings:       total time =    8636.29 ms /   391 tokens\n",
      " 92%|█████████▏| 93/101 [27:23<01:44, 13.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    67 runs   (    0.07 ms per token, 14096.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3499.24 ms /   249 tokens (   14.05 ms per token,    71.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4537.54 ms /    66 runs   (   68.75 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =    8120.09 ms /   315 tokens\n",
      " 93%|█████████▎| 94/101 [27:32<01:21, 11.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.63 ms /    51 runs   (    0.07 ms per token, 14061.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2899.99 ms /   169 tokens (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:        eval time =    3309.40 ms /    50 runs   (   66.19 ms per token,    15.11 tokens per second)\n",
      "llama_print_timings:       total time =    6273.09 ms /   219 tokens\n",
      " 94%|█████████▍| 95/101 [27:38<00:59,  9.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.56 ms /   103 runs   (    0.07 ms per token, 13622.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11306.60 ms /   889 tokens (   12.72 ms per token,    78.63 tokens per second)\n",
      "llama_print_timings:        eval time =    8064.82 ms /   102 runs   (   79.07 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =   19508.79 ms /   991 tokens\n",
      " 95%|█████████▌| 96/101 [27:57<01:04, 12.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.68 ms /   105 runs   (    0.07 ms per token, 13668.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9216.07 ms /   648 tokens (   14.22 ms per token,    70.31 tokens per second)\n",
      "llama_print_timings:        eval time =    7695.00 ms /   104 runs   (   73.99 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   17051.61 ms /   752 tokens\n",
      " 96%|█████████▌| 97/101 [28:14<00:56, 14.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.59 ms /    80 runs   (    0.08 ms per token, 12135.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5456.51 ms /   438 tokens (   12.46 ms per token,    80.27 tokens per second)\n",
      "llama_print_timings:        eval time =    8668.05 ms /    79 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =   14244.87 ms /   517 tokens\n",
      " 97%|█████████▋| 98/101 [28:29<00:42, 14.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    96 runs   (    0.07 ms per token, 13588.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33454.15 ms /  2127 tokens (   15.73 ms per token,    63.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9081.20 ms /    95 runs   (   95.59 ms per token,    10.46 tokens per second)\n",
      "llama_print_timings:       total time =   42674.84 ms /  2222 tokens\n",
      " 98%|█████████▊| 99/101 [29:11<00:45, 22.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    69 runs   (    0.09 ms per token, 11318.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5112.34 ms /   399 tokens (   12.81 ms per token,    78.05 tokens per second)\n",
      "llama_print_timings:        eval time =   11435.72 ms /    68 runs   (  168.17 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:       total time =   16669.98 ms /   467 tokens\n",
      " 99%|█████████▉| 100/101 [29:28<00:20, 20.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.52 ms /    60 runs   (    0.08 ms per token, 13286.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3736.08 ms /   228 tokens (   16.39 ms per token,    61.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4187.31 ms /    59 runs   (   70.97 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    8002.00 ms /   287 tokens\n",
      "100%|██████████| 101/101 [29:36<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_splitter_512 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/261 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    65 runs   (    0.08 ms per token, 11909.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10631.16 ms /   254 tokens (   41.85 ms per token,    23.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5145.43 ms /    64 runs   (   80.40 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   15882.94 ms /   318 tokens\n",
      "  0%|          | 1/261 [00:15<1:08:51, 15.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.19 ms /    52 runs   (    0.08 ms per token, 12419.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7575.13 ms /   539 tokens (   14.05 ms per token,    71.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3721.38 ms /    51 runs   (   72.97 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =   11428.87 ms /   590 tokens\n",
      "  1%|          | 2/261 [00:27<57:16, 13.27s/it]  Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.71 ms /    81 runs   (    0.19 ms per token,  5156.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3974.54 ms /   269 tokens (   14.78 ms per token,    67.68 tokens per second)\n",
      "llama_print_timings:        eval time =    6267.09 ms /    80 runs   (   78.34 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =   10398.67 ms /   349 tokens\n",
      "  1%|          | 3/261 [00:37<51:26, 11.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    69 runs   (    0.08 ms per token, 12916.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8241.73 ms /   565 tokens (   14.59 ms per token,    68.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4935.57 ms /    68 runs   (   72.58 ms per token,    13.78 tokens per second)\n",
      "llama_print_timings:       total time =   13309.67 ms /   633 tokens\n",
      "  2%|▏         | 4/261 [00:51<53:31, 12.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    70 runs   (    0.07 ms per token, 13894.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4323.99 ms /   338 tokens (   12.79 ms per token,    78.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5043.81 ms /    69 runs   (   73.10 ms per token,    13.68 tokens per second)\n",
      "llama_print_timings:       total time =    9505.97 ms /   407 tokens\n",
      "  2%|▏         | 5/261 [01:00<48:43, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    84 runs   (    0.07 ms per token, 13480.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6294.07 ms /   512 tokens (   12.29 ms per token,    81.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5996.12 ms /    83 runs   (   72.24 ms per token,    13.84 tokens per second)\n",
      "llama_print_timings:       total time =   12421.82 ms /   595 tokens\n",
      "  2%|▏         | 6/261 [01:12<49:59, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.72 ms /    63 runs   (    0.07 ms per token, 13336.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.36 ms /   223 tokens (   14.58 ms per token,    68.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4227.03 ms /    62 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    7558.20 ms /   285 tokens\n",
      "  3%|▎         | 7/261 [01:20<43:58, 10.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.84 ms /    64 runs   (    0.08 ms per token, 13209.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4918.22 ms /   400 tokens (   12.30 ms per token,    81.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4890.90 ms /    63 runs   (   77.63 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    9891.32 ms /   463 tokens\n",
      "  3%|▎         | 8/261 [01:30<43:08, 10.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.96 ms /   141 runs   (    0.08 ms per token, 12861.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4109.07 ms /   306 tokens (   13.43 ms per token,    74.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9915.92 ms /   140 runs   (   70.83 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:       total time =   14206.90 ms /   446 tokens\n",
      "  3%|▎         | 9/261 [01:44<48:11, 11.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    68 runs   (    0.07 ms per token, 13473.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3242.05 ms /   205 tokens (   15.81 ms per token,    63.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4733.33 ms /    67 runs   (   70.65 ms per token,    14.15 tokens per second)\n",
      "llama_print_timings:       total time =    8085.81 ms /   272 tokens\n",
      "  4%|▍         | 10/261 [01:52<43:38, 10.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    68 runs   (    0.08 ms per token, 13099.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3251.81 ms /   193 tokens (   16.85 ms per token,    59.35 tokens per second)\n",
      "llama_print_timings:        eval time =    4668.30 ms /    67 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =    8018.88 ms /   260 tokens\n",
      "  4%|▍         | 11/261 [02:00<40:23,  9.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    68 runs   (    0.07 ms per token, 13829.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3169.41 ms /   197 tokens (   16.09 ms per token,    62.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4502.80 ms /    67 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    7757.01 ms /   264 tokens\n",
      "  5%|▍         | 12/261 [02:08<37:47,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    67 runs   (    0.08 ms per token, 12786.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2907.96 ms /   174 tokens (   16.71 ms per token,    59.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4693.95 ms /    66 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    7705.81 ms /   240 tokens\n",
      "  5%|▍         | 13/261 [02:16<35:53,  8.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    65 runs   (    0.08 ms per token, 13249.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3228.57 ms /   196 tokens (   16.47 ms per token,    60.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4378.58 ms /    64 runs   (   68.42 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    7690.11 ms /   260 tokens\n",
      "  5%|▌         | 14/261 [02:23<34:30,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.45 ms /    62 runs   (    0.07 ms per token, 13926.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2982.27 ms /   188 tokens (   15.86 ms per token,    63.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4048.48 ms /    61 runs   (   66.37 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    7108.21 ms /   249 tokens\n",
      "  6%|▌         | 15/261 [02:31<32:48,  8.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.40 ms /    69 runs   (    0.08 ms per token, 12780.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3946.05 ms /   191 tokens (   20.66 ms per token,    48.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5208.14 ms /    68 runs   (   76.59 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =    9245.16 ms /   259 tokens\n",
      "  6%|▌         | 16/261 [02:40<34:12,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    95 runs   (    0.08 ms per token, 12496.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3959.28 ms /   238 tokens (   16.64 ms per token,    60.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8742.84 ms /    94 runs   (   93.01 ms per token,    10.75 tokens per second)\n",
      "llama_print_timings:       total time =   12827.67 ms /   332 tokens\n",
      "  7%|▋         | 17/261 [02:53<39:31,  9.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.51 ms /   131 runs   (    0.08 ms per token, 12470.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3925.85 ms /   225 tokens (   17.45 ms per token,    57.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9114.75 ms /   130 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   13215.00 ms /   355 tokens\n",
      "  7%|▋         | 18/261 [03:06<43:37, 10.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.57 ms /    58 runs   (    0.08 ms per token, 12680.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3488.18 ms /   234 tokens (   14.91 ms per token,    67.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4293.73 ms /    57 runs   (   75.33 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =    7856.09 ms /   291 tokens\n",
      "  7%|▋         | 19/261 [03:14<39:54,  9.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    80 runs   (    0.08 ms per token, 13251.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3442.11 ms /   221 tokens (   15.58 ms per token,    64.20 tokens per second)\n",
      "llama_print_timings:        eval time =    5498.26 ms /    79 runs   (   69.60 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =    9047.02 ms /   300 tokens\n",
      "  8%|▊         | 20/261 [03:23<38:43,  9.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.47 ms /   155 runs   (    0.07 ms per token, 13511.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3483.27 ms /   225 tokens (   15.48 ms per token,    64.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10694.69 ms /   154 runs   (   69.45 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   14380.89 ms /   379 tokens\n",
      "  8%|▊         | 21/261 [03:37<44:15, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.32 ms /   110 runs   (    0.08 ms per token, 13217.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4329.61 ms /   325 tokens (   13.32 ms per token,    75.06 tokens per second)\n",
      "llama_print_timings:        eval time =    8062.68 ms /   109 runs   (   73.97 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   12537.27 ms /   434 tokens\n",
      "  8%|▊         | 22/261 [03:50<45:50, 11.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.53 ms /    99 runs   (    0.08 ms per token, 13154.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8185.48 ms /   575 tokens (   14.24 ms per token,    70.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7170.99 ms /    98 runs   (   73.17 ms per token,    13.67 tokens per second)\n",
      "llama_print_timings:       total time =   15529.99 ms /   673 tokens\n",
      "  9%|▉         | 23/261 [04:05<50:26, 12.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.62 ms /   100 runs   (    0.08 ms per token, 13125.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4947.78 ms /   406 tokens (   12.19 ms per token,    82.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7052.95 ms /    99 runs   (   71.24 ms per token,    14.04 tokens per second)\n",
      "llama_print_timings:       total time =   12187.47 ms /   505 tokens\n",
      "  9%|▉         | 24/261 [04:17<49:36, 12.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    68 runs   (    0.08 ms per token, 12722.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8457.57 ms /   588 tokens (   14.38 ms per token,    69.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5332.14 ms /    67 runs   (   79.58 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =   13879.12 ms /   655 tokens\n",
      " 10%|▉         | 25/261 [04:31<50:57, 12.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    64 runs   (    0.07 ms per token, 13813.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.51 ms /   270 tokens (   15.59 ms per token,    64.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4366.68 ms /    63 runs   (   69.31 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =    8657.59 ms /   333 tokens\n",
      " 10%|▉         | 26/261 [04:40<45:41, 11.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    63 runs   (    0.07 ms per token, 13645.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.49 ms /   198 tokens (   16.68 ms per token,    59.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4204.22 ms /    62 runs   (   67.81 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =    7589.21 ms /   260 tokens\n",
      " 10%|█         | 27/261 [04:48<40:44, 10.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    69 runs   (    0.09 ms per token, 10989.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2976.72 ms /   191 tokens (   15.58 ms per token,    64.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5045.40 ms /    68 runs   (   74.20 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =    8113.48 ms /   259 tokens\n",
      " 11%|█         | 28/261 [04:56<37:50,  9.75s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    73 runs   (    0.08 ms per token, 13089.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3256.15 ms /   198 tokens (   16.45 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =    4993.65 ms /    72 runs   (   69.36 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =    8346.09 ms /   270 tokens\n",
      " 11%|█         | 29/261 [05:04<36:04,  9.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    71 runs   (    0.07 ms per token, 13797.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3186.58 ms /   205 tokens (   15.54 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4794.13 ms /    70 runs   (   68.49 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =    8071.14 ms /   275 tokens\n",
      " 11%|█▏        | 30/261 [05:12<34:27,  8.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    65 runs   (    0.08 ms per token, 13067.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3377.01 ms /   201 tokens (   16.80 ms per token,    59.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.98 ms /    64 runs   (   68.92 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =    7872.31 ms /   265 tokens\n",
      " 12%|█▏        | 31/261 [05:20<33:04,  8.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /    69 runs   (    0.08 ms per token, 13246.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3308.14 ms /   222 tokens (   14.90 ms per token,    67.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4636.48 ms /    68 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    8033.95 ms /   290 tokens\n",
      " 12%|█▏        | 32/261 [05:28<32:15,  8.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    73 runs   (    0.07 ms per token, 13576.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3187.01 ms /   205 tokens (   15.55 ms per token,    64.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5036.58 ms /    72 runs   (   69.95 ms per token,    14.30 tokens per second)\n",
      "llama_print_timings:       total time =    8317.36 ms /   277 tokens\n",
      " 13%|█▎        | 33/261 [05:36<31:57,  8.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.12 ms /    97 runs   (    0.07 ms per token, 13629.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3169.31 ms /   196 tokens (   16.17 ms per token,    61.84 tokens per second)\n",
      "llama_print_timings:        eval time =    6578.85 ms /    96 runs   (   68.53 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    9871.90 ms /   292 tokens\n",
      " 13%|█▎        | 34/261 [05:46<33:29,  8.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.50 ms /    59 runs   (    0.08 ms per token, 13114.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.02 ms /   192 tokens (   15.68 ms per token,    63.79 tokens per second)\n",
      "llama_print_timings:        eval time =    3978.30 ms /    58 runs   (   68.59 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    7064.70 ms /   250 tokens\n",
      " 13%|█▎        | 35/261 [05:53<31:19,  8.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    69 runs   (    0.08 ms per token, 13188.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2954.97 ms /   185 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4857.67 ms /    68 runs   (   71.44 ms per token,    14.00 tokens per second)\n",
      "llama_print_timings:       total time =    7902.68 ms /   253 tokens\n",
      " 14%|█▍        | 36/261 [06:01<30:43,  8.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    84 runs   (    0.07 ms per token, 13723.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3054.22 ms /   191 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5637.88 ms /    83 runs   (   67.93 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    8800.81 ms /   274 tokens\n",
      " 14%|█▍        | 37/261 [06:10<31:16,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.81 ms /    75 runs   (    0.08 ms per token, 12899.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3241.15 ms /   200 tokens (   16.21 ms per token,    61.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4995.33 ms /    74 runs   (   67.50 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    8332.55 ms /   274 tokens\n",
      " 15%|█▍        | 38/261 [06:18<31:05,  8.36s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    68 runs   (    0.07 ms per token, 13401.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3166.35 ms /   196 tokens (   16.15 ms per token,    61.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4484.63 ms /    67 runs   (   66.93 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =    7736.37 ms /   263 tokens\n",
      " 15%|█▍        | 39/261 [06:26<30:15,  8.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.35 ms /    68 runs   (    0.11 ms per token,  9247.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4052.97 ms /   197 tokens (   20.57 ms per token,    48.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7927.53 ms /    67 runs   (  118.32 ms per token,     8.45 tokens per second)\n",
      "llama_print_timings:       total time =   12082.13 ms /   264 tokens\n",
      " 15%|█▌        | 40/261 [06:38<34:26,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.54 ms /    70 runs   (    0.08 ms per token, 12646.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3273.02 ms /   185 tokens (   17.69 ms per token,    56.52 tokens per second)\n",
      "llama_print_timings:        eval time =    6584.79 ms /    69 runs   (   95.43 ms per token,    10.48 tokens per second)\n",
      "llama_print_timings:       total time =    9958.66 ms /   254 tokens\n",
      " 16%|█▌        | 41/261 [06:48<34:57,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    62 runs   (    0.08 ms per token, 12678.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3923.63 ms /   242 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4745.37 ms /    61 runs   (   77.79 ms per token,    12.85 tokens per second)\n",
      "llama_print_timings:       total time =    8754.01 ms /   303 tokens\n",
      " 16%|█▌        | 42/261 [06:57<33:57,  9.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.51 ms /    71 runs   (    0.08 ms per token, 12888.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7748.72 ms /   538 tokens (   14.40 ms per token,    69.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4855.68 ms /    70 runs   (   69.37 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =   12701.05 ms /   608 tokens\n",
      " 16%|█▋        | 43/261 [07:10<37:30, 10.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    68 runs   (    0.07 ms per token, 13452.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3044.58 ms /   189 tokens (   16.11 ms per token,    62.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4415.92 ms /    67 runs   (   65.91 ms per token,    15.17 tokens per second)\n",
      "llama_print_timings:       total time =    7549.73 ms /   256 tokens\n",
      " 17%|█▋        | 44/261 [07:17<34:19,  9.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.58 ms /    94 runs   (    0.08 ms per token, 12405.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.54 ms /   585 tokens (   14.84 ms per token,    67.41 tokens per second)\n",
      "llama_print_timings:        eval time =    8413.40 ms /    93 runs   (   90.47 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =   17239.89 ms /   678 tokens\n",
      " 17%|█▋        | 45/261 [07:34<42:32, 11.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    94 runs   (    0.08 ms per token, 12374.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4009.57 ms /   246 tokens (   16.30 ms per token,    61.35 tokens per second)\n",
      "llama_print_timings:        eval time =    7817.61 ms /    93 runs   (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   11965.44 ms /   339 tokens\n",
      " 18%|█▊        | 46/261 [07:46<42:30, 11.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.08 ms /    56 runs   (    0.07 ms per token, 13718.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5019.11 ms /   394 tokens (   12.74 ms per token,    78.50 tokens per second)\n",
      "llama_print_timings:        eval time =    3832.22 ms /    55 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =    8925.29 ms /   449 tokens\n",
      " 18%|█▊        | 47/261 [07:55<39:10, 10.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.67 ms /    75 runs   (    0.08 ms per token, 13225.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2960.61 ms /   187 tokens (   15.83 ms per token,    63.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5006.40 ms /    74 runs   (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time =    8063.43 ms /   261 tokens\n",
      " 18%|█▊        | 48/261 [08:03<35:56, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.53 ms /    64 runs   (    0.07 ms per token, 14121.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2935.23 ms /   187 tokens (   15.70 ms per token,    63.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4264.65 ms /    63 runs   (   67.69 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =    7280.66 ms /   250 tokens\n",
      " 19%|█▉        | 49/261 [08:11<32:45,  9.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    64 runs   (    0.07 ms per token, 13400.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2945.36 ms /   192 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4196.65 ms /    63 runs   (   66.61 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7223.21 ms /   255 tokens\n",
      " 19%|█▉        | 50/261 [08:18<30:26,  8.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      20.50 ms /   275 runs   (    0.07 ms per token, 13413.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2944.09 ms /   192 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_print_timings:        eval time =   19488.19 ms /   274 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =   22809.72 ms /   466 tokens\n",
      " 20%|█▉        | 51/261 [08:41<45:10, 12.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.92 ms /    79 runs   (    0.07 ms per token, 13349.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3317.10 ms /   197 tokens (   16.84 ms per token,    59.39 tokens per second)\n",
      "llama_print_timings:        eval time =    5336.55 ms /    78 runs   (   68.42 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    8756.78 ms /   275 tokens\n",
      " 20%|█▉        | 52/261 [08:49<40:37, 11.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.38 ms /    96 runs   (    0.08 ms per token, 13004.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3203.32 ms /   208 tokens (   15.40 ms per token,    64.93 tokens per second)\n",
      "llama_print_timings:        eval time =    6438.42 ms /    95 runs   (   67.77 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    9763.62 ms /   303 tokens\n",
      " 20%|██        | 53/261 [08:59<38:27, 11.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    61 runs   (    0.08 ms per token, 12482.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3234.08 ms /   196 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    4543.36 ms /    60 runs   (   75.72 ms per token,    13.21 tokens per second)\n",
      "llama_print_timings:       total time =    7856.53 ms /   256 tokens\n",
      " 21%|██        | 54/261 [09:07<34:55, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.42 ms /    60 runs   (    0.09 ms per token, 11074.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3018.79 ms /   189 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6476.65 ms /    59 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =    9584.51 ms /   248 tokens\n",
      " 21%|██        | 55/261 [09:17<34:12,  9.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.72 ms /   102 runs   (    0.08 ms per token, 13209.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4543.17 ms /   350 tokens (   12.98 ms per token,    77.04 tokens per second)\n",
      "llama_print_timings:        eval time =    7363.59 ms /   101 runs   (   72.91 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =   12043.96 ms /   451 tokens\n",
      " 21%|██▏       | 56/261 [09:29<36:10, 10.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.70 ms /   150 runs   (    0.08 ms per token, 12821.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3580.31 ms /   238 tokens (   15.04 ms per token,    66.47 tokens per second)\n",
      "llama_print_timings:        eval time =   11303.96 ms /   149 runs   (   75.87 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =   15085.55 ms /   387 tokens\n",
      " 22%|██▏       | 57/261 [09:44<40:35, 11.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.70 ms /    92 runs   (    0.07 ms per token, 13731.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10223.72 ms /   596 tokens (   17.15 ms per token,    58.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6712.64 ms /    91 runs   (   73.77 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   17069.02 ms /   687 tokens\n",
      " 22%|██▏       | 58/261 [10:01<45:36, 13.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.26 ms /    59 runs   (    0.07 ms per token, 13849.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3833.09 ms /   272 tokens (   14.09 ms per token,    70.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4021.08 ms /    58 runs   (   69.33 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =    7930.65 ms /   330 tokens\n",
      " 23%|██▎       | 59/261 [10:09<39:46, 11.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    58 runs   (    0.08 ms per token, 13299.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5554.52 ms /   460 tokens (   12.08 ms per token,    82.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4055.45 ms /    57 runs   (   71.15 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    9694.16 ms /   517 tokens\n",
      " 23%|██▎       | 60/261 [10:19<37:27, 11.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    76 runs   (    0.08 ms per token, 13279.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2950.53 ms /   178 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5081.89 ms /    75 runs   (   67.76 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    8129.13 ms /   253 tokens\n",
      " 23%|██▎       | 61/261 [10:27<34:13, 10.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.39 ms /    87 runs   (    0.07 ms per token, 13619.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2971.99 ms /   190 tokens (   15.64 ms per token,    63.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5813.53 ms /    86 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    8893.57 ms /   276 tokens\n",
      " 24%|██▍       | 62/261 [10:36<32:41,  9.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    58 runs   (    0.08 ms per token, 12666.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2955.04 ms /   191 tokens (   15.47 ms per token,    64.64 tokens per second)\n",
      "llama_print_timings:        eval time =    3914.25 ms /    57 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    6941.59 ms /   248 tokens\n",
      " 24%|██▍       | 63/261 [10:43<29:38,  8.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    66 runs   (    0.07 ms per token, 13333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3150.23 ms /   185 tokens (   17.03 ms per token,    58.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4475.13 ms /    65 runs   (   68.85 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    7710.30 ms /   250 tokens\n",
      " 25%|██▍       | 64/261 [10:50<28:14,  8.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    60 runs   (    0.08 ms per token, 12412.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2977.97 ms /   184 tokens (   16.18 ms per token,    61.79 tokens per second)\n",
      "llama_print_timings:        eval time =    5099.58 ms /    59 runs   (   86.43 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =    8157.34 ms /   243 tokens\n",
      " 25%|██▍       | 65/261 [10:58<27:40,  8.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.90 ms /    52 runs   (    0.08 ms per token, 13329.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3159.40 ms /   186 tokens (   16.99 ms per token,    58.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3478.34 ms /    51 runs   (   68.20 ms per token,    14.66 tokens per second)\n",
      "llama_print_timings:       total time =    6702.54 ms /   237 tokens\n",
      " 25%|██▌       | 66/261 [11:05<25:48,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    74 runs   (    0.07 ms per token, 13831.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3167.79 ms /   196 tokens (   16.16 ms per token,    61.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4901.41 ms /    73 runs   (   67.14 ms per token,    14.89 tokens per second)\n",
      "llama_print_timings:       total time =    8162.90 ms /   269 tokens\n",
      " 26%|██▌       | 67/261 [11:13<25:53,  8.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    72 runs   (    0.07 ms per token, 13483.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2929.41 ms /   188 tokens (   15.58 ms per token,    64.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4710.77 ms /    71 runs   (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    7729.51 ms /   259 tokens\n",
      " 26%|██▌       | 68/261 [11:21<25:29,  7.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    60 runs   (    0.07 ms per token, 13808.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2892.72 ms /   173 tokens (   16.72 ms per token,    59.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3913.36 ms /    59 runs   (   66.33 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:       total time =    6881.00 ms /   232 tokens\n",
      " 26%|██▋       | 69/261 [11:28<24:21,  7.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    67 runs   (    0.07 ms per token, 13554.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2910.90 ms /   182 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.21 ms /    66 runs   (   66.82 ms per token,    14.97 tokens per second)\n",
      "llama_print_timings:       total time =    7403.86 ms /   248 tokens\n",
      " 27%|██▋       | 70/261 [11:35<24:02,  7.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    83 runs   (    0.08 ms per token, 11869.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2919.82 ms /   183 tokens (   15.96 ms per token,    62.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7144.80 ms /    82 runs   (   87.13 ms per token,    11.48 tokens per second)\n",
      "llama_print_timings:       total time =   10180.37 ms /   265 tokens\n",
      " 27%|██▋       | 71/261 [11:45<26:24,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.62 ms /    50 runs   (    0.07 ms per token, 13812.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3078.51 ms /   180 tokens (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3241.37 ms /    49 runs   (   66.15 ms per token,    15.12 tokens per second)\n",
      "llama_print_timings:       total time =    6385.00 ms /   229 tokens\n",
      " 28%|██▊       | 72/261 [11:52<24:25,  7.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.03 ms /    67 runs   (    0.08 ms per token, 13325.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2998.15 ms /   187 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4335.02 ms /    66 runs   (   65.68 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:       total time =    7416.69 ms /   253 tokens\n",
      " 28%|██▊       | 73/261 [11:59<23:59,  7.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    63 runs   (    0.08 ms per token, 13288.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3224.07 ms /   198 tokens (   16.28 ms per token,    61.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4066.30 ms /    62 runs   (   65.59 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    7370.51 ms /   260 tokens\n",
      " 28%|██▊       | 74/261 [12:07<23:35,  7.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.47 ms /    72 runs   (    0.08 ms per token, 13160.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2994.69 ms /   191 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4655.77 ms /    71 runs   (   65.57 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    7740.98 ms /   262 tokens\n",
      " 29%|██▊       | 75/261 [12:14<23:37,  7.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    55 runs   (    0.07 ms per token, 13691.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3005.75 ms /   187 tokens (   16.07 ms per token,    62.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3540.37 ms /    54 runs   (   65.56 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    6613.81 ms /   241 tokens\n",
      " 29%|██▉       | 76/261 [12:21<22:34,  7.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    65 runs   (    0.08 ms per token, 13289.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2990.47 ms /   187 tokens (   15.99 ms per token,    62.53 tokens per second)\n",
      "llama_print_timings:        eval time =    4895.93 ms /    64 runs   (   76.50 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:       total time =    7969.42 ms /   251 tokens\n",
      " 30%|██▉       | 77/261 [12:29<23:03,  7.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    78 runs   (    0.08 ms per token, 13209.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3466.39 ms /   197 tokens (   17.60 ms per token,    56.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5064.98 ms /    77 runs   (   65.78 ms per token,    15.20 tokens per second)\n",
      "llama_print_timings:       total time =    8630.85 ms /   274 tokens\n",
      " 30%|██▉       | 78/261 [12:38<23:56,  7.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.97 ms /    81 runs   (    0.07 ms per token, 13570.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3000.61 ms /   189 tokens (   15.88 ms per token,    62.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5249.49 ms /    80 runs   (   65.62 ms per token,    15.24 tokens per second)\n",
      "llama_print_timings:       total time =    8351.43 ms /   269 tokens\n",
      " 30%|███       | 79/261 [12:46<24:16,  8.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    78 runs   (    0.08 ms per token, 12926.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.88 ms /   189 tokens (   15.93 ms per token,    62.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5115.89 ms /    77 runs   (   66.44 ms per token,    15.05 tokens per second)\n",
      "llama_print_timings:       total time =    8224.18 ms /   266 tokens\n",
      " 31%|███       | 80/261 [12:54<24:20,  8.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.57 ms /    63 runs   (    0.07 ms per token, 13776.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3323.29 ms /   187 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4106.81 ms /    62 runs   (   66.24 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    7508.11 ms /   249 tokens\n",
      " 31%|███       | 81/261 [13:02<23:42,  7.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    62 runs   (    0.07 ms per token, 13638.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3026.99 ms /   185 tokens (   16.36 ms per token,    61.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4078.33 ms /    61 runs   (   66.86 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =    7182.68 ms /   246 tokens\n",
      " 31%|███▏      | 82/261 [13:09<22:56,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.91 ms /    50 runs   (    0.08 ms per token, 12787.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3094.47 ms /   178 tokens (   17.38 ms per token,    57.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3628.64 ms /    49 runs   (   74.05 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =    6788.08 ms /   227 tokens\n",
      " 32%|███▏      | 83/261 [13:16<22:00,  7.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.77 ms /    91 runs   (    0.07 ms per token, 13443.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6695.61 ms /   520 tokens (   12.88 ms per token,    77.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6570.00 ms /    90 runs   (   73.00 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =   13383.19 ms /   610 tokens\n",
      " 32%|███▏      | 84/261 [13:29<27:10,  9.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.53 ms /   128 runs   (    0.07 ms per token, 13429.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5304.31 ms /   426 tokens (   12.45 ms per token,    80.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9074.01 ms /   127 runs   (   71.45 ms per token,    14.00 tokens per second)\n",
      "llama_print_timings:       total time =   14542.72 ms /   553 tokens\n",
      " 33%|███▎      | 85/261 [13:44<31:42, 10.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    86 runs   (    0.08 ms per token, 13216.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8219.22 ms /   561 tokens (   14.65 ms per token,    68.25 tokens per second)\n",
      "llama_print_timings:        eval time =    6193.99 ms /    85 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =   14522.39 ms /   646 tokens\n",
      " 33%|███▎      | 86/261 [13:58<34:46, 11.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.77 ms /    66 runs   (    0.07 ms per token, 13833.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4039.18 ms /   289 tokens (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4518.98 ms /    65 runs   (   69.52 ms per token,    14.38 tokens per second)\n",
      "llama_print_timings:       total time =    8639.30 ms /   354 tokens\n",
      " 33%|███▎      | 87/261 [14:07<31:43, 10.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.97 ms /   131 runs   (    0.08 ms per token, 11946.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5838.42 ms /   483 tokens (   12.09 ms per token,    82.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9374.86 ms /   130 runs   (   72.11 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =   15377.58 ms /   613 tokens\n",
      " 34%|███▎      | 88/261 [14:22<35:23, 12.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    84 runs   (    0.08 ms per token, 13015.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4616.75 ms /   371 tokens (   12.44 ms per token,    80.36 tokens per second)\n",
      "llama_print_timings:        eval time =    6177.16 ms /    83 runs   (   74.42 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   10902.26 ms /   454 tokens\n",
      " 34%|███▍      | 89/261 [14:33<34:00, 11.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.09 ms /    95 runs   (    0.09 ms per token, 11737.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8348.77 ms /   606 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10342.80 ms /    94 runs   (  110.03 ms per token,     9.09 tokens per second)\n",
      "llama_print_timings:       total time =   18853.94 ms /   700 tokens\n",
      " 34%|███▍      | 90/261 [14:52<39:47, 13.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.61 ms /    63 runs   (    0.10 ms per token,  9528.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2767.79 ms /   143 tokens (   19.36 ms per token,    51.67 tokens per second)\n",
      "llama_print_timings:        eval time =   13509.22 ms /    62 runs   (  217.89 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =   16400.01 ms /   205 tokens\n",
      " 35%|███▍      | 91/261 [15:08<41:38, 14.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    85 runs   (    0.07 ms per token, 13485.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8293.32 ms /   569 tokens (   14.58 ms per token,    68.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6212.67 ms /    84 runs   (   73.96 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   14615.07 ms /   653 tokens\n",
      " 35%|███▌      | 92/261 [15:23<41:19, 14.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.69 ms /   124 runs   (    0.08 ms per token, 12799.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.85 ms /   192 tokens (   16.12 ms per token,    62.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8882.12 ms /   123 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =   12138.74 ms /   315 tokens\n",
      " 36%|███▌      | 93/261 [15:35<38:57, 13.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.60 ms /    70 runs   (    0.08 ms per token, 12504.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3208.89 ms /   194 tokens (   16.54 ms per token,    60.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5235.59 ms /    69 runs   (   75.88 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =    8534.89 ms /   263 tokens\n",
      " 36%|███▌      | 94/261 [15:44<34:14, 12.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.83 ms /    49 runs   (    0.08 ms per token, 12790.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3239.14 ms /   182 tokens (   17.80 ms per token,    56.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3631.40 ms /    48 runs   (   75.65 ms per token,    13.22 tokens per second)\n",
      "llama_print_timings:       total time =    6934.28 ms /   230 tokens\n",
      " 36%|███▋      | 95/261 [15:51<29:35, 10.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    57 runs   (    0.08 ms per token, 12250.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3159.65 ms /   180 tokens (   17.55 ms per token,    56.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4593.45 ms /    56 runs   (   82.03 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    7827.63 ms /   236 tokens\n",
      " 37%|███▋      | 96/261 [15:58<27:03,  9.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    66 runs   (    0.08 ms per token, 13030.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3564.94 ms /   199 tokens (   17.91 ms per token,    55.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4485.41 ms /    65 runs   (   69.01 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =    8133.34 ms /   264 tokens\n",
      " 37%|███▋      | 97/261 [16:07<25:29,  9.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    80 runs   (    0.07 ms per token, 13335.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2957.73 ms /   187 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5416.39 ms /    79 runs   (   68.56 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    8474.08 ms /   266 tokens\n",
      " 38%|███▊      | 98/261 [16:15<24:38,  9.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    51 runs   (    0.08 ms per token, 13253.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2990.91 ms /   187 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3441.53 ms /    50 runs   (   68.83 ms per token,    14.53 tokens per second)\n",
      "llama_print_timings:       total time =    6494.40 ms /   237 tokens\n",
      " 38%|███▊      | 99/261 [16:22<22:24,  8.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.77 ms /    46 runs   (    0.08 ms per token, 12188.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2948.21 ms /   187 tokens (   15.77 ms per token,    63.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3627.53 ms /    45 runs   (   80.61 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =    6634.80 ms /   232 tokens\n",
      " 38%|███▊      | 100/261 [16:28<20:56,  7.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    57 runs   (    0.09 ms per token, 11143.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3277.28 ms /   183 tokens (   17.91 ms per token,    55.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.84 ms /    56 runs   (   81.00 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    7892.76 ms /   239 tokens\n",
      " 39%|███▊      | 101/261 [16:36<20:53,  7.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.16 ms /    60 runs   (    0.10 ms per token,  9733.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3408.51 ms /   192 tokens (   17.75 ms per token,    56.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6198.84 ms /    59 runs   (  105.06 ms per token,     9.52 tokens per second)\n",
      "llama_print_timings:       total time =    9695.78 ms /   251 tokens\n",
      " 39%|███▉      | 102/261 [16:46<22:14,  8.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.77 ms /    69 runs   (    0.11 ms per token,  8879.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3492.83 ms /   194 tokens (   18.00 ms per token,    55.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7976.07 ms /    68 runs   (  117.30 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:       total time =   11575.91 ms /   262 tokens\n",
      " 39%|███▉      | 103/261 [16:57<24:37,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    61 runs   (    0.09 ms per token, 10718.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3789.78 ms /   199 tokens (   19.04 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6055.88 ms /    60 runs   (  100.93 ms per token,     9.91 tokens per second)\n",
      "llama_print_timings:       total time =    9931.88 ms /   259 tokens\n",
      " 40%|███▉      | 104/261 [17:07<24:56,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    69 runs   (    0.08 ms per token, 11987.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3226.79 ms /   190 tokens (   16.98 ms per token,    58.88 tokens per second)\n",
      "llama_print_timings:        eval time =    5602.34 ms /    68 runs   (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    8920.83 ms /   258 tokens\n",
      " 40%|████      | 105/261 [17:16<24:18,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.57 ms /    75 runs   (    0.07 ms per token, 13474.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2989.90 ms /   190 tokens (   15.74 ms per token,    63.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4975.89 ms /    74 runs   (   67.24 ms per token,    14.87 tokens per second)\n",
      "llama_print_timings:       total time =    8059.69 ms /   264 tokens\n",
      " 41%|████      | 106/261 [17:24<23:09,  8.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    71 runs   (    0.08 ms per token, 13278.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3236.85 ms /   199 tokens (   16.27 ms per token,    61.48 tokens per second)\n",
      "llama_print_timings:        eval time =    4791.88 ms /    70 runs   (   68.46 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =    8117.23 ms /   269 tokens\n",
      " 41%|████      | 107/261 [17:32<22:21,  8.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /   100 runs   (    0.08 ms per token, 13331.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3367.40 ms /   195 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6709.68 ms /    99 runs   (   67.77 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =   10201.19 ms /   294 tokens\n",
      " 41%|████▏     | 108/261 [17:43<23:21,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.38 ms /    59 runs   (    0.07 ms per token, 13482.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.82 ms /   190 tokens (   15.85 ms per token,    63.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3927.00 ms /    58 runs   (   67.71 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =    7009.13 ms /   248 tokens\n",
      " 42%|████▏     | 109/261 [17:50<21:34,  8.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.41 ms /    73 runs   (    0.07 ms per token, 13503.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7069.57 ms /   526 tokens (   13.44 ms per token,    74.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5209.11 ms /    72 runs   (   72.35 ms per token,    13.82 tokens per second)\n",
      "llama_print_timings:       total time =   12371.09 ms /   598 tokens\n",
      " 42%|████▏     | 110/261 [18:02<24:20,  9.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    71 runs   (    0.07 ms per token, 13383.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4016.35 ms /   292 tokens (   13.75 ms per token,    72.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4849.58 ms /    70 runs   (   69.28 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =    8954.08 ms /   362 tokens\n",
      " 43%|████▎     | 111/261 [18:11<23:38,  9.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.76 ms /    93 runs   (    0.07 ms per token, 13751.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3154.79 ms /   196 tokens (   16.10 ms per token,    62.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6192.40 ms /    92 runs   (   67.31 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =    9459.27 ms /   288 tokens\n",
      " 43%|████▎     | 112/261 [18:20<23:29,  9.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.52 ms /    48 runs   (    0.07 ms per token, 13648.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3218.66 ms /   193 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3112.81 ms /    47 runs   (   66.23 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    6389.06 ms /   240 tokens\n",
      " 43%|████▎     | 113/261 [18:27<21:03,  8.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    52 runs   (    0.08 ms per token, 12351.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2960.85 ms /   192 tokens (   15.42 ms per token,    64.85 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.23 ms /    51 runs   (   88.93 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =    7573.56 ms /   243 tokens\n",
      " 44%|████▎     | 114/261 [18:34<20:13,  8.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    59 runs   (    0.08 ms per token, 12452.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3754.95 ms /   193 tokens (   19.46 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4043.73 ms /    58 runs   (   69.72 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =    7877.23 ms /   251 tokens\n",
      " 44%|████▍     | 115/261 [18:42<19:48,  8.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    75 runs   (    0.08 ms per token, 13171.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3100.61 ms /   185 tokens (   16.76 ms per token,    59.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4974.69 ms /    74 runs   (   67.23 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    8170.25 ms /   259 tokens\n",
      " 44%|████▍     | 116/261 [18:50<19:41,  8.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    63 runs   (    0.08 ms per token, 12670.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3185.35 ms /   189 tokens (   16.85 ms per token,    59.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4240.80 ms /    62 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    7506.49 ms /   251 tokens\n",
      " 45%|████▍     | 117/261 [18:58<19:06,  7.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    72 runs   (    0.08 ms per token, 13026.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.22 ms /   191 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5332.62 ms /    71 runs   (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =    8724.57 ms /   262 tokens\n",
      " 45%|████▌     | 118/261 [19:07<19:31,  8.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.82 ms /    84 runs   (    0.08 ms per token, 12322.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3089.73 ms /   176 tokens (   17.56 ms per token,    56.96 tokens per second)\n",
      "llama_print_timings:        eval time =    6574.77 ms /    83 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    9774.57 ms /   259 tokens\n",
      " 46%|████▌     | 119/261 [19:17<20:30,  8.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    58 runs   (    0.07 ms per token, 13364.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3340.44 ms /   196 tokens (   17.04 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =    3838.63 ms /    57 runs   (   67.34 ms per token,    14.85 tokens per second)\n",
      "llama_print_timings:       total time =    7252.83 ms /   253 tokens\n",
      " 46%|████▌     | 120/261 [19:24<19:22,  8.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    51 runs   (    0.08 ms per token, 12318.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3096.86 ms /   191 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4108.34 ms /    50 runs   (   82.17 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    7272.65 ms /   241 tokens\n",
      " 46%|████▋     | 121/261 [19:31<18:33,  7.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    58 runs   (    0.07 ms per token, 13504.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3116.21 ms /   190 tokens (   16.40 ms per token,    60.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3868.67 ms /    57 runs   (   67.87 ms per token,    14.73 tokens per second)\n",
      "llama_print_timings:       total time =    7057.97 ms /   247 tokens\n",
      " 47%|████▋     | 122/261 [19:38<17:48,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    62 runs   (    0.08 ms per token, 12304.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3271.08 ms /   193 tokens (   16.95 ms per token,    59.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4675.45 ms /    61 runs   (   76.65 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:       total time =    8028.12 ms /   254 tokens\n",
      " 47%|████▋     | 123/261 [19:46<17:55,  7.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    74 runs   (    0.10 ms per token, 10456.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3444.88 ms /   198 tokens (   17.40 ms per token,    57.48 tokens per second)\n",
      "llama_print_timings:        eval time =    8575.06 ms /    73 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:       total time =   12154.83 ms /   271 tokens\n",
      " 48%|████▊     | 124/261 [19:58<20:46,  9.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    69 runs   (    0.08 ms per token, 12421.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3463.82 ms /   197 tokens (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4964.07 ms /    68 runs   (   73.00 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =    8516.58 ms /   265 tokens\n",
      " 48%|████▊     | 125/261 [20:07<20:14,  8.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    73 runs   (    0.08 ms per token, 12746.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3316.05 ms /   194 tokens (   17.09 ms per token,    58.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5600.63 ms /    72 runs   (   77.79 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =    9011.85 ms /   266 tokens\n",
      " 48%|████▊     | 126/261 [20:16<20:08,  8.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    56 runs   (    0.08 ms per token, 13068.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3225.04 ms /   202 tokens (   15.97 ms per token,    62.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3941.98 ms /    55 runs   (   71.67 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =    7240.00 ms /   257 tokens\n",
      " 49%|████▊     | 127/261 [20:23<18:51,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.56 ms /    64 runs   (    0.07 ms per token, 14028.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3193.22 ms /   196 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4211.77 ms /    63 runs   (   66.85 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =    7482.89 ms /   259 tokens\n",
      " 49%|████▉     | 128/261 [20:31<18:04,  8.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.16 ms /    82 runs   (    0.08 ms per token, 13320.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.08 ms /   195 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5620.04 ms /    81 runs   (   69.38 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    8897.83 ms /   276 tokens\n",
      " 49%|████▉     | 129/261 [20:39<18:26,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    59 runs   (    0.08 ms per token, 12159.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3201.65 ms /   190 tokens (   16.85 ms per token,    59.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4945.29 ms /    58 runs   (   85.26 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    8229.56 ms /   248 tokens\n",
      " 50%|████▉     | 130/261 [20:48<18:11,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    71 runs   (    0.08 ms per token, 11807.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2974.02 ms /   180 tokens (   16.52 ms per token,    60.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5550.24 ms /    70 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =    8617.09 ms /   250 tokens\n",
      " 50%|█████     | 131/261 [20:56<18:14,  8.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.36 ms /    78 runs   (    0.09 ms per token, 10597.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.31 ms /   204 tokens (   19.95 ms per token,    50.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8142.18 ms /    77 runs   (  105.74 ms per token,     9.46 tokens per second)\n",
      "llama_print_timings:       total time =   12342.45 ms /   281 tokens\n",
      " 51%|█████     | 132/261 [21:09<20:38,  9.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    85 runs   (    0.07 ms per token, 13558.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3210.48 ms /   193 tokens (   16.63 ms per token,    60.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5859.90 ms /    84 runs   (   69.76 ms per token,    14.33 tokens per second)\n",
      "llama_print_timings:       total time =    9176.46 ms /   277 tokens\n",
      " 51%|█████     | 133/261 [21:18<20:12,  9.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    68 runs   (    0.08 ms per token, 12905.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3189.17 ms /   202 tokens (   15.79 ms per token,    63.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4989.03 ms /    67 runs   (   74.46 ms per token,    13.43 tokens per second)\n",
      "llama_print_timings:       total time =    8262.82 ms /   269 tokens\n",
      " 51%|█████▏    | 134/261 [21:26<19:17,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.82 ms /    77 runs   (    0.08 ms per token, 13232.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2962.83 ms /   186 tokens (   15.93 ms per token,    62.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5215.62 ms /    76 runs   (   68.63 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =    8272.26 ms /   262 tokens\n",
      " 52%|█████▏    | 135/261 [21:34<18:36,  8.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    72 runs   (    0.07 ms per token, 13875.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3172.84 ms /   198 tokens (   16.02 ms per token,    62.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4774.68 ms /    71 runs   (   67.25 ms per token,    14.87 tokens per second)\n",
      "llama_print_timings:       total time =    8033.20 ms /   269 tokens\n",
      " 52%|█████▏    | 136/261 [21:42<17:56,  8.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.36 ms /    72 runs   (    0.07 ms per token, 13430.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3172.77 ms /   204 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4817.56 ms /    71 runs   (   67.85 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =    8077.05 ms /   275 tokens\n",
      " 52%|█████▏    | 137/261 [21:51<17:28,  8.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    74 runs   (    0.08 ms per token, 12774.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3235.19 ms /   201 tokens (   16.10 ms per token,    62.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5391.93 ms /    73 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =    8721.30 ms /   274 tokens\n",
      " 53%|█████▎    | 138/261 [21:59<17:29,  8.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.67 ms /   157 runs   (    0.08 ms per token, 12390.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3526.27 ms /   197 tokens (   17.90 ms per token,    55.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11894.97 ms /   156 runs   (   76.25 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =   15633.79 ms /   353 tokens\n",
      " 53%|█████▎    | 139/261 [22:15<21:41, 10.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.66 ms /    97 runs   (    0.08 ms per token, 12669.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3240.64 ms /   193 tokens (   16.79 ms per token,    59.56 tokens per second)\n",
      "llama_print_timings:        eval time =    7065.49 ms /    96 runs   (   73.60 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   10433.05 ms /   289 tokens\n",
      " 54%|█████▎    | 140/261 [22:25<21:22, 10.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    58 runs   (    0.08 ms per token, 12218.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3207.56 ms /   196 tokens (   16.37 ms per token,    61.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4347.15 ms /    57 runs   (   76.27 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =    7632.97 ms /   253 tokens\n",
      " 54%|█████▍    | 141/261 [22:33<19:25,  9.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    52 runs   (    0.08 ms per token, 11956.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3347.73 ms /   201 tokens (   16.66 ms per token,    60.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4573.40 ms /    51 runs   (   89.67 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =    7992.38 ms /   252 tokens\n",
      " 54%|█████▍    | 142/261 [22:41<18:14,  9.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    62 runs   (    0.08 ms per token, 12548.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3681.87 ms /   197 tokens (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:        eval time =    4829.19 ms /    61 runs   (   79.17 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =    8592.79 ms /   258 tokens\n",
      " 55%|█████▍    | 143/261 [22:50<17:44,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.71 ms /    54 runs   (    0.09 ms per token, 11457.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3285.25 ms /   190 tokens (   17.29 ms per token,    57.83 tokens per second)\n",
      "llama_print_timings:        eval time =    4481.73 ms /    53 runs   (   84.56 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    7841.26 ms /   243 tokens\n",
      " 55%|█████▌    | 144/261 [22:57<16:54,  8.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    60 runs   (    0.08 ms per token, 12911.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3368.95 ms /   190 tokens (   17.73 ms per token,    56.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5341.18 ms /    59 runs   (   90.53 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    8789.57 ms /   249 tokens\n",
      " 56%|█████▌    | 145/261 [23:06<16:49,  8.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.20 ms /    68 runs   (    0.08 ms per token, 13079.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2991.62 ms /   189 tokens (   15.83 ms per token,    63.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4976.39 ms /    67 runs   (   74.27 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =    8063.11 ms /   256 tokens\n",
      " 56%|█████▌    | 146/261 [23:14<16:19,  8.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    72 runs   (    0.08 ms per token, 13196.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3528.01 ms /   204 tokens (   17.29 ms per token,    57.82 tokens per second)\n",
      "llama_print_timings:        eval time =    5040.13 ms /    71 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    8657.80 ms /   275 tokens\n",
      " 56%|█████▋    | 147/261 [23:23<16:15,  8.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    72 runs   (    0.07 ms per token, 13587.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2954.75 ms /   190 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4795.10 ms /    71 runs   (   67.54 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    7838.53 ms /   261 tokens\n",
      " 57%|█████▋    | 148/261 [23:31<15:42,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.75 ms /   106 runs   (    0.07 ms per token, 13672.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2931.22 ms /   188 tokens (   15.59 ms per token,    64.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7608.36 ms /   105 runs   (   72.46 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   10670.52 ms /   293 tokens\n",
      " 57%|█████▋    | 149/261 [23:41<16:52,  9.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.42 ms /    61 runs   (    0.07 ms per token, 13816.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3312.52 ms /   193 tokens (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4313.56 ms /    60 runs   (   71.89 ms per token,    13.91 tokens per second)\n",
      "llama_print_timings:       total time =    7706.62 ms /   253 tokens\n",
      " 57%|█████▋    | 150/261 [23:49<15:59,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.22 ms /    73 runs   (    0.07 ms per token, 13992.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.86 ms /   191 tokens (   16.21 ms per token,    61.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4852.53 ms /    72 runs   (   67.40 ms per token,    14.84 tokens per second)\n",
      "llama_print_timings:       total time =    8038.32 ms /   263 tokens\n",
      " 58%|█████▊    | 151/261 [23:57<15:30,  8.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    57 runs   (    0.09 ms per token, 10586.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3419.24 ms /   185 tokens (   18.48 ms per token,    54.11 tokens per second)\n",
      "llama_print_timings:        eval time =    7764.24 ms /    56 runs   (  138.65 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:       total time =   11271.02 ms /   241 tokens\n",
      " 58%|█████▊    | 152/261 [24:08<16:54,  9.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    68 runs   (    0.08 ms per token, 12442.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3768.54 ms /   193 tokens (   19.53 ms per token,    51.21 tokens per second)\n",
      "llama_print_timings:        eval time =    5825.70 ms /    67 runs   (   86.95 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =    9684.11 ms /   260 tokens\n",
      " 59%|█████▊    | 153/261 [24:18<16:57,  9.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.80 ms /   100 runs   (    0.08 ms per token, 12815.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3293.65 ms /   202 tokens (   16.31 ms per token,    61.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7374.70 ms /    99 runs   (   74.49 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   10798.79 ms /   301 tokens\n",
      " 59%|█████▉    | 154/261 [24:29<17:30,  9.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    72 runs   (    0.08 ms per token, 12841.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3032.42 ms /   191 tokens (   15.88 ms per token,    62.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5448.99 ms /    71 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =    8576.79 ms /   262 tokens\n",
      " 59%|█████▉    | 155/261 [24:37<16:41,  9.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.25 ms /    57 runs   (    0.07 ms per token, 13408.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.67 ms /   198 tokens (   16.66 ms per token,    60.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3975.23 ms /    56 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    7347.05 ms /   254 tokens\n",
      " 60%|█████▉    | 156/261 [24:45<15:25,  8.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.29 ms /    63 runs   (    0.08 ms per token, 11904.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3224.87 ms /   185 tokens (   17.43 ms per token,    57.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4911.93 ms /    62 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    8221.62 ms /   247 tokens\n",
      " 60%|██████    | 157/261 [24:53<14:58,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.86 ms /    51 runs   (    0.08 ms per token, 13202.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3146.43 ms /   191 tokens (   16.47 ms per token,    60.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3321.75 ms /    50 runs   (   66.44 ms per token,    15.05 tokens per second)\n",
      "llama_print_timings:       total time =    6533.44 ms /   241 tokens\n",
      " 61%|██████    | 158/261 [25:00<13:45,  8.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    85 runs   (    0.07 ms per token, 13423.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3352.28 ms /   196 tokens (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5622.42 ms /    84 runs   (   66.93 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =    9084.17 ms /   280 tokens\n",
      " 61%|██████    | 159/261 [25:09<14:10,  8.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    70 runs   (    0.07 ms per token, 13765.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3288.13 ms /   203 tokens (   16.20 ms per token,    61.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.24 ms /    69 runs   (   65.73 ms per token,    15.21 tokens per second)\n",
      "llama_print_timings:       total time =    7910.93 ms /   272 tokens\n",
      " 61%|██████▏   | 160/261 [25:17<13:48,  8.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.87 ms /    51 runs   (    0.08 ms per token, 13181.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.83 ms /   192 tokens (   15.67 ms per token,    63.83 tokens per second)\n",
      "llama_print_timings:        eval time =    3273.49 ms /    50 runs   (   65.47 ms per token,    15.27 tokens per second)\n",
      "llama_print_timings:       total time =    6344.66 ms /   242 tokens\n",
      " 62%|██████▏   | 161/261 [25:23<12:44,  7.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.84 ms /   168 runs   (    0.08 ms per token, 13085.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3259.47 ms /   198 tokens (   16.46 ms per token,    60.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11143.29 ms /   167 runs   (   66.73 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =   14617.62 ms /   365 tokens\n",
      " 62%|██████▏   | 162/261 [25:38<16:04,  9.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    75 runs   (    0.08 ms per token, 12244.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3202.06 ms /   184 tokens (   17.40 ms per token,    57.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5328.82 ms /    74 runs   (   72.01 ms per token,    13.89 tokens per second)\n",
      "llama_print_timings:       total time =    8630.73 ms /   258 tokens\n",
      " 62%|██████▏   | 163/261 [25:46<15:22,  9.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.22 ms /    68 runs   (    0.08 ms per token, 13031.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3538.78 ms /   194 tokens (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4709.15 ms /    67 runs   (   70.29 ms per token,    14.23 tokens per second)\n",
      "llama_print_timings:       total time =    8340.31 ms /   261 tokens\n",
      " 63%|██████▎   | 164/261 [25:55<14:41,  9.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.02 ms /    70 runs   (    0.07 ms per token, 13935.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3175.78 ms /   198 tokens (   16.04 ms per token,    62.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5176.07 ms /    69 runs   (   75.02 ms per token,    13.33 tokens per second)\n",
      "llama_print_timings:       total time =    8439.25 ms /   267 tokens\n",
      " 63%|██████▎   | 165/261 [26:03<14:13,  8.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.51 ms /    78 runs   (    0.07 ms per token, 14163.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3236.38 ms /   193 tokens (   16.77 ms per token,    59.63 tokens per second)\n",
      "llama_print_timings:        eval time =    5199.53 ms /    77 runs   (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    8531.65 ms /   270 tokens\n",
      " 64%|██████▎   | 166/261 [26:12<13:54,  8.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.07 ms /    76 runs   (    0.08 ms per token, 12522.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2943.13 ms /   184 tokens (   16.00 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5039.74 ms /    75 runs   (   67.20 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    8078.13 ms /   259 tokens\n",
      " 64%|██████▍   | 167/261 [26:20<13:26,  8.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /   140 runs   (    0.08 ms per token, 12821.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3262.28 ms /   198 tokens (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_print_timings:        eval time =    9564.14 ms /   139 runs   (   68.81 ms per token,    14.53 tokens per second)\n",
      "llama_print_timings:       total time =   13006.52 ms /   337 tokens\n",
      " 64%|██████▍   | 168/261 [26:33<15:21,  9.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.09 ms /    84 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3316.79 ms /   201 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5604.65 ms /    83 runs   (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    9027.80 ms /   284 tokens\n",
      " 65%|██████▍   | 169/261 [26:42<14:47,  9.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.59 ms /    68 runs   (    0.08 ms per token, 12153.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4272.47 ms /   193 tokens (   22.14 ms per token,    45.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5467.36 ms /    67 runs   (   81.60 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =    9829.59 ms /   260 tokens\n",
      " 65%|██████▌   | 170/261 [26:52<14:43,  9.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    67 runs   (    0.07 ms per token, 13378.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3273.53 ms /   193 tokens (   16.96 ms per token,    58.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4517.24 ms /    66 runs   (   68.44 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =    7878.62 ms /   259 tokens\n",
      " 66%|██████▌   | 171/261 [26:59<13:44,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    64 runs   (    0.07 ms per token, 13386.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3200.67 ms /   194 tokens (   16.50 ms per token,    60.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4223.64 ms /    63 runs   (   67.04 ms per token,    14.92 tokens per second)\n",
      "llama_print_timings:       total time =    7503.55 ms /   257 tokens\n",
      " 66%|██████▌   | 172/261 [27:07<12:50,  8.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    69 runs   (    0.08 ms per token, 12839.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3318.14 ms /   199 tokens (   16.67 ms per token,    59.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4954.85 ms /    68 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =    8361.56 ms /   267 tokens\n",
      " 66%|██████▋   | 173/261 [27:15<12:34,  8.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.29 ms /   141 runs   (    0.08 ms per token, 12483.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3000.85 ms /   192 tokens (   15.63 ms per token,    63.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9610.98 ms /   140 runs   (   68.65 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =   12795.79 ms /   332 tokens\n",
      " 67%|██████▋   | 174/261 [27:28<14:16,  9.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    60 runs   (    0.08 ms per token, 12376.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2978.93 ms /   190 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5405.87 ms /    59 runs   (   91.62 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =    8466.56 ms /   249 tokens\n",
      " 67%|██████▋   | 175/261 [27:37<13:31,  9.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    77 runs   (    0.08 ms per token, 12618.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3429.40 ms /   194 tokens (   17.68 ms per token,    56.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5599.42 ms /    76 runs   (   73.68 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =    9134.10 ms /   270 tokens\n",
      " 67%|██████▋   | 176/261 [27:46<13:14,  9.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    54 runs   (    0.09 ms per token, 10627.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3495.79 ms /   192 tokens (   18.21 ms per token,    54.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5070.98 ms /    53 runs   (   95.68 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:       total time =    8655.89 ms /   245 tokens\n",
      " 68%|██████▊   | 177/261 [27:54<12:47,  9.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    63 runs   (    0.08 ms per token, 13105.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3388.38 ms /   201 tokens (   16.86 ms per token,    59.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4603.73 ms /    62 runs   (   74.25 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =    8073.51 ms /   263 tokens\n",
      " 68%|██████▊   | 178/261 [28:02<12:12,  8.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    51 runs   (    0.08 ms per token, 12384.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2373.30 ms /   124 tokens (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3392.01 ms /    50 runs   (   67.84 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =    5850.33 ms /   174 tokens\n",
      " 69%|██████▊   | 179/261 [28:08<10:50,  7.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.06 ms /    82 runs   (    0.07 ms per token, 13540.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5725.02 ms /   469 tokens (   12.21 ms per token,    81.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5734.69 ms /    81 runs   (   70.80 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:       total time =   11571.14 ms /   550 tokens\n",
      " 69%|██████▉   | 180/261 [28:20<12:10,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    81 runs   (    0.07 ms per token, 13982.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3027.55 ms /   187 tokens (   16.19 ms per token,    61.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5396.90 ms /    80 runs   (   67.46 ms per token,    14.82 tokens per second)\n",
      "llama_print_timings:       total time =    8526.67 ms /   267 tokens\n",
      " 69%|██████▉   | 181/261 [28:28<11:50,  8.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    65 runs   (    0.07 ms per token, 14051.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.12 ms /   187 tokens (   15.65 ms per token,    63.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4262.53 ms /    64 runs   (   66.60 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7268.78 ms /   251 tokens\n",
      " 70%|██████▉   | 182/261 [28:36<11:03,  8.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    82 runs   (    0.07 ms per token, 13582.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3335.45 ms /   200 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5555.10 ms /    81 runs   (   68.58 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    8992.59 ms /   281 tokens\n",
      " 70%|███████   | 183/261 [28:45<11:08,  8.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    71 runs   (    0.07 ms per token, 13682.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3222.89 ms /   208 tokens (   15.49 ms per token,    64.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4808.29 ms /    70 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    8121.29 ms /   278 tokens\n",
      " 70%|███████   | 184/261 [28:53<10:49,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    89 runs   (    0.07 ms per token, 13660.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.93 ms /   200 tokens (   16.52 ms per token,    60.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6019.23 ms /    88 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    9436.25 ms /   288 tokens\n",
      " 71%|███████   | 185/261 [29:02<11:04,  8.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.96 ms /    65 runs   (    0.08 ms per token, 13091.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3180.40 ms /   212 tokens (   15.00 ms per token,    66.66 tokens per second)\n",
      "llama_print_timings:        eval time =    4354.76 ms /    64 runs   (   68.04 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =    7616.37 ms /   276 tokens\n",
      " 71%|███████▏  | 186/261 [29:10<10:30,  8.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.43 ms /    60 runs   (    0.07 ms per token, 13544.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3258.36 ms /   196 tokens (   16.62 ms per token,    60.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4056.44 ms /    59 runs   (   68.75 ms per token,    14.54 tokens per second)\n",
      "llama_print_timings:       total time =    7390.30 ms /   255 tokens\n",
      " 72%|███████▏  | 187/261 [29:17<09:59,  8.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    67 runs   (    0.09 ms per token, 10780.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.17 ms /   190 tokens (   17.39 ms per token,    57.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5019.44 ms /    66 runs   (   76.05 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:       total time =    8419.12 ms /   256 tokens\n",
      " 72%|███████▏  | 188/261 [29:26<09:58,  8.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    64 runs   (    0.08 ms per token, 11911.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3588.28 ms /   198 tokens (   18.12 ms per token,    55.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4747.46 ms /    63 runs   (   75.36 ms per token,    13.27 tokens per second)\n",
      "llama_print_timings:       total time =    8420.98 ms /   261 tokens\n",
      " 72%|███████▏  | 189/261 [29:34<09:55,  8.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       2.95 ms /    41 runs   (    0.07 ms per token, 13907.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3184.12 ms /   200 tokens (   15.92 ms per token,    62.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2722.53 ms /    40 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    5959.17 ms /   240 tokens\n",
      " 73%|███████▎  | 190/261 [29:40<08:57,  7.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.38 ms /    57 runs   (    0.08 ms per token, 13001.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3226.00 ms /   198 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4654.29 ms /    56 runs   (   83.11 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =    7955.80 ms /   254 tokens\n",
      " 73%|███████▎  | 191/261 [29:48<08:58,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    66 runs   (    0.08 ms per token, 12607.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3216.83 ms /   189 tokens (   17.02 ms per token,    58.75 tokens per second)\n",
      "llama_print_timings:        eval time =    4720.36 ms /    65 runs   (   72.62 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    8026.08 ms /   254 tokens\n",
      " 74%|███████▎  | 192/261 [29:56<08:57,  7.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    71 runs   (    0.07 ms per token, 13583.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3223.17 ms /   196 tokens (   16.44 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =    5036.25 ms /    70 runs   (   71.95 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =    8353.11 ms /   266 tokens\n",
      " 74%|███████▍  | 193/261 [30:04<09:01,  7.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    46 runs   (    0.08 ms per token, 11960.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2952.78 ms /   191 tokens (   15.46 ms per token,    64.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3279.14 ms /    45 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =    6292.76 ms /   236 tokens\n",
      " 74%|███████▍  | 194/261 [30:11<08:20,  7.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.09 ms /    94 runs   (    0.08 ms per token, 13252.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3304.44 ms /   194 tokens (   17.03 ms per token,    58.71 tokens per second)\n",
      "llama_print_timings:        eval time =    6551.38 ms /    93 runs   (   70.44 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:       total time =    9977.38 ms /   287 tokens\n",
      " 75%|███████▍  | 195/261 [30:21<09:02,  8.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.25 ms /    84 runs   (    0.07 ms per token, 13442.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3022.11 ms /   189 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.02 ms /    83 runs   (   70.02 ms per token,    14.28 tokens per second)\n",
      "llama_print_timings:       total time =    8940.99 ms /   272 tokens\n",
      " 75%|███████▌  | 196/261 [30:30<09:08,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.95 ms /    53 runs   (    0.07 ms per token, 13407.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2701.47 ms /   152 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3541.85 ms /    52 runs   (   68.11 ms per token,    14.68 tokens per second)\n",
      "llama_print_timings:       total time =    6310.35 ms /   204 tokens\n",
      " 75%|███████▌  | 197/261 [30:36<08:19,  7.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.31 ms /    71 runs   (    0.09 ms per token, 11259.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3781.91 ms /   266 tokens (   14.22 ms per token,    70.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5655.17 ms /    70 runs   (   80.79 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    9539.32 ms /   336 tokens\n",
      " 76%|███████▌  | 198/261 [30:45<08:44,  8.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.39 ms /   134 runs   (    0.09 ms per token, 11763.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3459.18 ms /   203 tokens (   17.04 ms per token,    58.68 tokens per second)\n",
      "llama_print_timings:        eval time =   13104.31 ms /   133 runs   (   98.53 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =   16748.76 ms /   336 tokens\n",
      " 76%|███████▌  | 199/261 [31:02<11:12, 10.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   171 runs   (    0.08 ms per token, 12620.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4112.25 ms /   210 tokens (   19.58 ms per token,    51.07 tokens per second)\n",
      "llama_print_timings:        eval time =   12933.35 ms /   170 runs   (   76.08 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:       total time =   17278.11 ms /   380 tokens\n",
      " 77%|███████▋  | 200/261 [31:20<12:59, 12.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /    91 runs   (    0.09 ms per token, 11656.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3535.66 ms /   233 tokens (   15.17 ms per token,    65.90 tokens per second)\n",
      "llama_print_timings:        eval time =    6905.32 ms /    90 runs   (   76.73 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =   10564.91 ms /   323 tokens\n",
      " 77%|███████▋  | 201/261 [31:30<12:07, 12.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    79 runs   (    0.11 ms per token,  9364.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4459.49 ms /   235 tokens (   18.98 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:        eval time =    8446.66 ms /    78 runs   (  108.29 ms per token,     9.23 tokens per second)\n",
      "llama_print_timings:       total time =   13023.63 ms /   313 tokens\n",
      " 77%|███████▋  | 202/261 [31:43<12:11, 12.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.82 ms /    66 runs   (    0.07 ms per token, 13704.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5424.11 ms /   200 tokens (   27.12 ms per token,    36.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5664.81 ms /    65 runs   (   87.15 ms per token,    11.47 tokens per second)\n",
      "llama_print_timings:       total time =   11178.22 ms /   265 tokens\n",
      " 78%|███████▊  | 203/261 [31:54<11:37, 12.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    81 runs   (    0.08 ms per token, 12490.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3421.03 ms /   194 tokens (   17.63 ms per token,    56.71 tokens per second)\n",
      "llama_print_timings:        eval time =    7011.94 ms /    80 runs   (   87.65 ms per token,    11.41 tokens per second)\n",
      "llama_print_timings:       total time =   10547.12 ms /   274 tokens\n",
      " 78%|███████▊  | 204/261 [32:05<11:00, 11.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.14 ms /    82 runs   (    0.07 ms per token, 13363.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3366.81 ms /   193 tokens (   17.44 ms per token,    57.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5750.02 ms /    81 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    9232.52 ms /   274 tokens\n",
      " 79%|███████▊  | 205/261 [32:14<10:09, 10.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    85 runs   (    0.07 ms per token, 13492.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3458.45 ms /   204 tokens (   16.95 ms per token,    58.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6065.78 ms /    84 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =    9642.93 ms /   288 tokens\n",
      " 79%|███████▉  | 206/261 [32:24<09:38, 10.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      16.78 ms /   210 runs   (    0.08 ms per token, 12516.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4006.66 ms /   227 tokens (   17.65 ms per token,    56.66 tokens per second)\n",
      "llama_print_timings:        eval time =   15515.04 ms /   209 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   19836.37 ms /   436 tokens\n",
      " 79%|███████▉  | 207/261 [32:44<11:58, 13.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.08 ms /   137 runs   (    0.08 ms per token, 12361.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3295.34 ms /   221 tokens (   14.91 ms per token,    67.06 tokens per second)\n",
      "llama_print_timings:        eval time =   11429.36 ms /   136 runs   (   84.04 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   14927.99 ms /   357 tokens\n",
      " 80%|███████▉  | 208/261 [32:59<12:11, 13.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.31 ms /    59 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3015.04 ms /   191 tokens (   15.79 ms per token,    63.35 tokens per second)\n",
      "llama_print_timings:        eval time =    3940.80 ms /    58 runs   (   67.94 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    7036.83 ms /   249 tokens\n",
      " 80%|████████  | 209/261 [33:06<10:12, 11.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    80 runs   (    0.07 ms per token, 13538.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3190.69 ms /   193 tokens (   16.53 ms per token,    60.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6507.59 ms /    79 runs   (   82.37 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    9813.43 ms /   272 tokens\n",
      " 80%|████████  | 210/261 [33:15<09:30, 11.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.10 ms /    65 runs   (    0.08 ms per token, 12747.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3350.39 ms /   201 tokens (   16.67 ms per token,    59.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5793.10 ms /    64 runs   (   90.52 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    9239.76 ms /   265 tokens\n",
      " 81%|████████  | 211/261 [33:25<08:50, 10.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    53 runs   (    0.07 ms per token, 13621.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3185.87 ms /   187 tokens (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3855.06 ms /    52 runs   (   74.14 ms per token,    13.49 tokens per second)\n",
      "llama_print_timings:       total time =    7115.15 ms /   239 tokens\n",
      " 81%|████████  | 212/261 [33:32<07:48,  9.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.98 ms /    81 runs   (    0.07 ms per token, 13554.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2970.02 ms /   188 tokens (   15.80 ms per token,    63.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6024.85 ms /    80 runs   (   75.31 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =    9106.79 ms /   268 tokens\n",
      " 82%|████████▏ | 213/261 [33:41<07:32,  9.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    71 runs   (    0.07 ms per token, 13477.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3286.08 ms /   205 tokens (   16.03 ms per token,    62.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5756.99 ms /    70 runs   (   82.24 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:       total time =    9144.60 ms /   275 tokens\n",
      " 82%|████████▏ | 214/261 [33:50<07:19,  9.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    61 runs   (    0.08 ms per token, 12861.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3349.74 ms /   194 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5294.32 ms /    60 runs   (   88.24 ms per token,    11.33 tokens per second)\n",
      "llama_print_timings:       total time =    8730.82 ms /   254 tokens\n",
      " 82%|████████▏ | 215/261 [33:59<07:01,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    81 runs   (    0.07 ms per token, 13511.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3239.36 ms /   195 tokens (   16.61 ms per token,    60.20 tokens per second)\n",
      "llama_print_timings:        eval time =    5485.20 ms /    80 runs   (   68.56 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    8833.57 ms /   275 tokens\n",
      " 83%|████████▎ | 216/261 [34:08<06:47,  9.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    69 runs   (    0.07 ms per token, 13497.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.82 ms /   196 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4635.19 ms /    68 runs   (   68.16 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    7905.82 ms /   264 tokens\n",
      " 83%|████████▎ | 217/261 [34:16<06:23,  8.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.45 ms /    76 runs   (    0.07 ms per token, 13934.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3206.68 ms /   208 tokens (   15.42 ms per token,    64.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5166.87 ms /    75 runs   (   68.89 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    8476.99 ms /   283 tokens\n",
      " 84%|████████▎ | 218/261 [34:24<06:11,  8.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    85 runs   (    0.07 ms per token, 13939.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.44 ms /   201 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5770.20 ms /    84 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    9063.78 ms /   285 tokens\n",
      " 84%|████████▍ | 219/261 [34:33<06:08,  8.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.27 ms /    59 runs   (    0.07 ms per token, 13827.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.31 ms /   201 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3947.50 ms /    58 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    7204.73 ms /   259 tokens\n",
      " 84%|████████▍ | 220/261 [34:40<05:40,  8.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.80 ms /    65 runs   (    0.07 ms per token, 13527.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2656.89 ms /   158 tokens (   16.82 ms per token,    59.47 tokens per second)\n",
      "llama_print_timings:        eval time =    4350.16 ms /    64 runs   (   67.97 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =    7096.98 ms /   222 tokens\n",
      " 85%|████████▍ | 221/261 [34:47<05:17,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.10 ms /    69 runs   (    0.07 ms per token, 13529.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3220.41 ms /   210 tokens (   15.34 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4647.50 ms /    68 runs   (   68.35 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =    7961.11 ms /   278 tokens\n",
      " 85%|████████▌ | 222/261 [34:55<05:10,  7.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    73 runs   (    0.07 ms per token, 14172.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2936.64 ms /   187 tokens (   15.70 ms per token,    63.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4889.72 ms /    72 runs   (   67.91 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    7923.92 ms /   259 tokens\n",
      " 85%|████████▌ | 223/261 [35:03<05:01,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.28 ms /    60 runs   (    0.07 ms per token, 14015.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.50 ms /   187 tokens (   15.65 ms per token,    63.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4044.84 ms /    59 runs   (   68.56 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    7053.99 ms /   246 tokens\n",
      " 86%|████████▌ | 224/261 [35:10<04:44,  7.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.77 ms /   131 runs   (    0.07 ms per token, 13405.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3178.65 ms /   196 tokens (   16.22 ms per token,    61.66 tokens per second)\n",
      "llama_print_timings:        eval time =    8969.51 ms /   130 runs   (   69.00 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   12330.77 ms /   326 tokens\n",
      " 86%|████████▌ | 225/261 [35:23<05:26,  9.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    72 runs   (    0.07 ms per token, 13556.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2656.36 ms /   160 tokens (   16.60 ms per token,    60.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.89 ms /    71 runs   (   67.51 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    7546.81 ms /   231 tokens\n",
      " 87%|████████▋ | 226/261 [35:30<05:01,  8.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      30.05 ms /   406 runs   (    0.07 ms per token, 13513.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5545.48 ms /   460 tokens (   12.06 ms per token,    82.95 tokens per second)\n",
      "llama_print_timings:        eval time =   30338.58 ms /   405 runs   (   74.91 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =   36668.67 ms /   865 tokens\n",
      " 87%|████████▋ | 227/261 [36:07<09:39, 17.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    87 runs   (    0.08 ms per token, 13039.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4218.53 ms /   230 tokens (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:        eval time =  194155.29 ms /    86 runs   ( 2257.62 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time =  198522.77 ms /   316 tokens\n",
      " 87%|████████▋ | 228/261 [39:25<39:18, 71.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.18 ms /    66 runs   (    0.08 ms per token, 12746.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7586.46 ms /   538 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_print_timings:        eval time =    4909.96 ms /    65 runs   (   75.54 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   12592.50 ms /   603 tokens\n",
      " 88%|████████▊ | 229/261 [39:38<28:42, 53.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      16.74 ms /   214 runs   (    0.08 ms per token, 12783.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8122.72 ms /   532 tokens (   15.27 ms per token,    65.50 tokens per second)\n",
      "llama_print_timings:        eval time =   17438.91 ms /   213 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   25872.42 ms /   745 tokens\n",
      " 88%|████████▊ | 230/261 [40:04<23:28, 45.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    75 runs   (    0.07 ms per token, 13943.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9411.16 ms /   567 tokens (   16.60 ms per token,    60.25 tokens per second)\n",
      "llama_print_timings:        eval time =    5391.56 ms /    74 runs   (   72.86 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time =   14909.19 ms /   641 tokens\n",
      " 89%|████████▊ | 231/261 [40:19<18:08, 36.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    72 runs   (    0.08 ms per token, 12190.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.81 ms /   153 tokens (   20.23 ms per token,    49.42 tokens per second)\n",
      "llama_print_timings:        eval time =    7307.02 ms /    71 runs   (  102.92 ms per token,     9.72 tokens per second)\n",
      "llama_print_timings:       total time =   10515.69 ms /   224 tokens\n",
      " 89%|████████▉ | 232/261 [40:29<13:48, 28.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /   102 runs   (    0.07 ms per token, 13517.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10286.93 ms /   585 tokens (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7586.20 ms /   101 runs   (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   18016.36 ms /   686 tokens\n",
      " 89%|████████▉ | 233/261 [40:47<11:51, 25.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    80 runs   (    0.08 ms per token, 13260.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6930.84 ms /   514 tokens (   13.48 ms per token,    74.16 tokens per second)\n",
      "llama_print_timings:        eval time =    6048.76 ms /    79 runs   (   76.57 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   13095.42 ms /   593 tokens\n",
      " 90%|████████▉ | 234/261 [41:00<09:46, 21.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    66 runs   (    0.07 ms per token, 13349.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8656.37 ms /   598 tokens (   14.48 ms per token,    69.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4796.28 ms /    65 runs   (   73.79 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   13543.96 ms /   663 tokens\n",
      " 90%|█████████ | 235/261 [41:14<08:20, 19.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    80 runs   (    0.08 ms per token, 13247.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4298.72 ms /   312 tokens (   13.78 ms per token,    72.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5820.29 ms /    79 runs   (   73.67 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   10231.68 ms /   391 tokens\n",
      " 90%|█████████ | 236/261 [41:24<06:53, 16.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.72 ms /   131 runs   (    0.07 ms per token, 13480.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6501.85 ms /   499 tokens (   13.03 ms per token,    76.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9908.00 ms /   130 runs   (   76.22 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:       total time =   16589.37 ms /   629 tokens\n",
      " 91%|█████████ | 237/261 [41:41<06:37, 16.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    74 runs   (    0.07 ms per token, 13754.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3231.57 ms /   192 tokens (   16.83 ms per token,    59.41 tokens per second)\n",
      "llama_print_timings:        eval time =    6232.46 ms /    73 runs   (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =    9574.67 ms /   265 tokens\n",
      " 91%|█████████ | 238/261 [41:50<05:32, 14.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.96 ms /    63 runs   (    0.08 ms per token, 12704.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3457.18 ms /   203 tokens (   17.03 ms per token,    58.72 tokens per second)\n",
      "llama_print_timings:        eval time =    5631.75 ms /    62 runs   (   90.83 ms per token,    11.01 tokens per second)\n",
      "llama_print_timings:       total time =    9179.49 ms /   265 tokens\n",
      " 92%|█████████▏| 239/261 [42:00<04:43, 12.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.87 ms /    92 runs   (    0.07 ms per token, 13399.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3502.60 ms /   211 tokens (   16.60 ms per token,    60.24 tokens per second)\n",
      "llama_print_timings:        eval time =    6406.11 ms /    91 runs   (   70.40 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =   10033.55 ms /   302 tokens\n",
      " 92%|█████████▏| 240/261 [42:10<04:12, 12.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.63 ms /    77 runs   (    0.07 ms per token, 13671.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3436.93 ms /   199 tokens (   17.27 ms per token,    57.90 tokens per second)\n",
      "llama_print_timings:        eval time =    5300.31 ms /    76 runs   (   69.74 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =    8840.47 ms /   275 tokens\n",
      " 92%|█████████▏| 241/261 [42:18<03:41, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    55 runs   (    0.07 ms per token, 13356.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.94 ms /   195 tokens (   16.92 ms per token,    59.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4047.95 ms /    54 runs   (   74.96 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =    7422.87 ms /   249 tokens\n",
      " 93%|█████████▎| 242/261 [42:26<03:09,  9.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.19 ms /    57 runs   (    0.07 ms per token, 13616.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3269.84 ms /   208 tokens (   15.72 ms per token,    63.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3730.95 ms /    56 runs   (   66.62 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7077.74 ms /   264 tokens\n",
      " 93%|█████████▎| 243/261 [42:33<02:43,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.39 ms /    61 runs   (    0.07 ms per token, 13901.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3408.52 ms /   195 tokens (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4056.30 ms /    60 runs   (   67.61 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    7545.32 ms /   255 tokens\n",
      " 93%|█████████▎| 244/261 [42:41<02:26,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    81 runs   (    0.07 ms per token, 13484.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.48 ms /   201 tokens (   16.17 ms per token,    61.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5997.32 ms /    80 runs   (   74.97 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =    9356.41 ms /   281 tokens\n",
      " 94%|█████████▍| 245/261 [42:50<02:21,  8.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.46 ms /    59 runs   (    0.08 ms per token, 13225.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3539.99 ms /   200 tokens (   17.70 ms per token,    56.50 tokens per second)\n",
      "llama_print_timings:        eval time =    4691.89 ms /    58 runs   (   80.89 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    8317.48 ms /   258 tokens\n",
      " 94%|█████████▍| 246/261 [42:58<02:10,  8.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.65 ms /    98 runs   (    0.09 ms per token, 11325.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3498.59 ms /   190 tokens (   18.41 ms per token,    54.31 tokens per second)\n",
      "llama_print_timings:        eval time =   10496.63 ms /    97 runs   (  108.21 ms per token,     9.24 tokens per second)\n",
      "llama_print_timings:       total time =   14141.59 ms /   287 tokens\n",
      " 95%|█████████▍| 247/261 [43:12<02:24, 10.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    63 runs   (    0.08 ms per token, 12497.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3910.30 ms /   202 tokens (   19.36 ms per token,    51.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6953.48 ms /    62 runs   (  112.15 ms per token,     8.92 tokens per second)\n",
      "llama_print_timings:       total time =   10958.13 ms /   264 tokens\n",
      " 95%|█████████▌| 248/261 [43:23<02:16, 10.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.93 ms /    51 runs   (    0.08 ms per token, 12993.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3198.86 ms /   191 tokens (   16.75 ms per token,    59.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3935.09 ms /    50 runs   (   78.70 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =    7205.78 ms /   241 tokens\n",
      " 95%|█████████▌| 249/261 [43:31<01:54,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.00 ms /    68 runs   (    0.07 ms per token, 13605.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3457.43 ms /   196 tokens (   17.64 ms per token,    56.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5000.84 ms /    67 runs   (   74.64 ms per token,    13.40 tokens per second)\n",
      "llama_print_timings:       total time =    8550.76 ms /   263 tokens\n",
      " 96%|█████████▌| 250/261 [43:39<01:41,  9.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    85 runs   (    0.07 ms per token, 13576.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3170.47 ms /   186 tokens (   17.05 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =    6024.18 ms /    84 runs   (   71.72 ms per token,    13.94 tokens per second)\n",
      "llama_print_timings:       total time =    9311.59 ms /   270 tokens\n",
      " 96%|█████████▌| 251/261 [43:48<01:32,  9.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.14 ms /    67 runs   (    0.08 ms per token, 13040.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3291.78 ms /   189 tokens (   17.42 ms per token,    57.42 tokens per second)\n",
      "llama_print_timings:        eval time =    5075.55 ms /    66 runs   (   76.90 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:       total time =    8459.48 ms /   255 tokens\n",
      " 97%|█████████▋| 252/261 [43:57<01:21,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.20 ms /    52 runs   (    0.08 ms per token, 12375.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3581.89 ms /   198 tokens (   18.09 ms per token,    55.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6144.84 ms /    51 runs   (  120.49 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =    9803.88 ms /   249 tokens\n",
      " 97%|█████████▋| 253/261 [44:07<01:14,  9.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    66 runs   (    0.08 ms per token, 12913.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.97 ms /   261 tokens (   16.07 ms per token,    62.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5379.33 ms /    65 runs   (   82.76 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    9665.62 ms /   326 tokens\n",
      " 97%|█████████▋| 254/261 [44:16<01:05,  9.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /   107 runs   (    0.08 ms per token, 13323.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8971.55 ms /   585 tokens (   15.34 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8394.06 ms /   106 runs   (   79.19 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   17520.47 ms /   691 tokens\n",
      " 98%|█████████▊| 255/261 [44:34<01:10, 11.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.83 ms /   143 runs   (    0.08 ms per token, 13202.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9004.50 ms /   597 tokens (   15.08 ms per token,    66.30 tokens per second)\n",
      "llama_print_timings:        eval time =   11337.83 ms /   142 runs   (   79.84 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   20548.34 ms /   739 tokens\n",
      " 98%|█████████▊| 256/261 [44:54<01:12, 14.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.20 ms /    69 runs   (    0.08 ms per token, 13269.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7775.48 ms /   533 tokens (   14.59 ms per token,    68.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5727.12 ms /    68 runs   (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =   13611.20 ms /   601 tokens\n",
      " 98%|█████████▊| 257/261 [45:08<00:56, 14.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      14.52 ms /   195 runs   (    0.07 ms per token, 13427.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5961.61 ms /   487 tokens (   12.24 ms per token,    81.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15081.12 ms /   194 runs   (   77.74 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   21322.46 ms /   681 tokens\n",
      " 99%|█████████▉| 258/261 [45:29<00:49, 16.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   176 runs   (    0.08 ms per token, 12646.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8205.51 ms /   555 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:        eval time =   12901.10 ms /   175 runs   (   73.72 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   21351.56 ms /   730 tokens\n",
      " 99%|█████████▉| 259/261 [45:51<00:35, 17.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    88 runs   (    0.07 ms per token, 13499.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8538.24 ms /   589 tokens (   14.50 ms per token,    68.98 tokens per second)\n",
      "llama_print_timings:        eval time =    6381.37 ms /    87 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   15039.46 ms /   676 tokens\n",
      "100%|█████████▉| 260/261 [46:06<00:17, 17.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    83 runs   (    0.07 ms per token, 13404.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8633.82 ms /   609 tokens (   14.18 ms per token,    70.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7969.59 ms /    82 runs   (   97.19 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   16725.52 ms /   691 tokens\n",
      "100%|██████████| 261/261 [46:23<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_splitter_1024 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      14.24 ms /   177 runs   (    0.08 ms per token, 12428.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20554.30 ms /   857 tokens (   23.98 ms per token,    41.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15493.65 ms /   176 runs   (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   36469.64 ms /  1033 tokens\n",
      "  1%|          | 1/105 [00:36<1:03:13, 36.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    67 runs   (    0.08 ms per token, 13096.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9079.79 ms /   565 tokens (   16.07 ms per token,    62.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6003.13 ms /    66 runs   (   90.96 ms per token,    10.99 tokens per second)\n",
      "llama_print_timings:       total time =   15245.31 ms /   631 tokens\n",
      "  2%|▏         | 2/105 [00:51<41:11, 23.99s/it]  Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    78 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11197.73 ms /   882 tokens (   12.70 ms per token,    78.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5666.11 ms /    77 runs   (   73.59 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   16966.86 ms /   959 tokens\n",
      "  3%|▎         | 3/105 [01:08<35:20, 20.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    61 runs   (    0.08 ms per token, 13310.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4921.91 ms /   400 tokens (   12.30 ms per token,    81.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4297.47 ms /    60 runs   (   71.62 ms per token,    13.96 tokens per second)\n",
      "llama_print_timings:       total time =    9362.03 ms /   460 tokens\n",
      "  4%|▍         | 4/105 [01:18<27:24, 16.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.55 ms /    85 runs   (    0.08 ms per token, 12975.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15085.05 ms /  1102 tokens (   13.69 ms per token,    73.05 tokens per second)\n",
      "llama_print_timings:        eval time =    6155.90 ms /    84 runs   (   73.28 ms per token,    13.65 tokens per second)\n",
      "llama_print_timings:       total time =   21442.97 ms /  1186 tokens\n",
      "  5%|▍         | 5/105 [01:39<30:14, 18.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.42 ms /   116 runs   (    0.07 ms per token, 13784.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12298.48 ms /   934 tokens (   13.17 ms per token,    75.94 tokens per second)\n",
      "llama_print_timings:        eval time =   10598.08 ms /   115 runs   (   92.16 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   23073.97 ms /  1049 tokens\n",
      "  6%|▌         | 6/105 [02:02<32:42, 19.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    88 runs   (    0.07 ms per token, 13732.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10763.48 ms /   850 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6412.32 ms /    87 runs   (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   17300.87 ms /   937 tokens\n",
      "  7%|▋         | 7/105 [02:19<31:01, 19.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    68 runs   (    0.08 ms per token, 12741.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9308.88 ms /   588 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5804.33 ms /    67 runs   (   86.63 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =   15232.36 ms /   655 tokens\n",
      "  8%|▊         | 8/105 [02:35<28:46, 17.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.51 ms /    65 runs   (    0.07 ms per token, 14418.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10800.46 ms /   853 tokens (   12.66 ms per token,    78.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4700.40 ms /    64 runs   (   73.44 ms per token,    13.62 tokens per second)\n",
      "llama_print_timings:       total time =   15585.06 ms /   917 tokens\n",
      "  9%|▊         | 9/105 [02:50<27:22, 17.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    96 runs   (    0.08 ms per token, 12917.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5084.15 ms /   358 tokens (   14.20 ms per token,    70.41 tokens per second)\n",
      "llama_print_timings:        eval time =   13159.93 ms /    95 runs   (  138.53 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:       total time =   18405.04 ms /   453 tokens\n",
      " 10%|▉         | 10/105 [03:09<27:43, 17.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    63 runs   (    0.08 ms per token, 12778.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4320.70 ms /   315 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4887.71 ms /    62 runs   (   78.83 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =    9326.21 ms /   377 tokens\n",
      " 10%|█         | 11/105 [03:18<23:30, 15.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.94 ms /    77 runs   (    0.08 ms per token, 12954.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4824.52 ms /   322 tokens (   14.98 ms per token,    66.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6467.26 ms /    76 runs   (   85.10 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   11399.64 ms /   398 tokens\n",
      " 11%|█▏        | 12/105 [03:29<21:33, 13.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    71 runs   (    0.08 ms per token, 13080.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4767.07 ms /   323 tokens (   14.76 ms per token,    67.76 tokens per second)\n",
      "llama_print_timings:        eval time =    6075.86 ms /    70 runs   (   86.80 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =   10945.08 ms /   393 tokens\n",
      " 12%|█▏        | 13/105 [03:40<19:57, 13.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.15 ms /    71 runs   (    0.09 ms per token, 11550.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5151.63 ms /   378 tokens (   13.63 ms per token,    73.37 tokens per second)\n",
      "llama_print_timings:        eval time =   14830.27 ms /    70 runs   (  211.86 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   20112.81 ms /   448 tokens\n",
      " 13%|█▎        | 14/105 [04:00<22:59, 15.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.16 ms /    67 runs   (    0.08 ms per token, 12984.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9398.53 ms /   644 tokens (   14.59 ms per token,    68.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5722.45 ms /    66 runs   (   86.70 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =   15259.00 ms /   710 tokens\n",
      " 14%|█▍        | 15/105 [04:16<22:47, 15.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.84 ms /    82 runs   (    0.08 ms per token, 11993.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11035.31 ms /   756 tokens (   14.60 ms per token,    68.51 tokens per second)\n",
      "llama_print_timings:        eval time =    8669.82 ms /    81 runs   (  107.03 ms per token,     9.34 tokens per second)\n",
      "llama_print_timings:       total time =   19885.55 ms /   837 tokens\n",
      " 15%|█▌        | 16/105 [04:36<24:38, 16.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    56 runs   (    0.07 ms per token, 13526.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4582.43 ms /   338 tokens (   13.56 ms per token,    73.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4088.23 ms /    55 runs   (   74.33 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =    8745.56 ms /   393 tokens\n",
      " 16%|█▌        | 17/105 [04:44<20:53, 14.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    82 runs   (    0.07 ms per token, 13746.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12017.75 ms /   949 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6062.76 ms /    81 runs   (   74.85 ms per token,    13.36 tokens per second)\n",
      "llama_print_timings:       total time =   18207.33 ms /  1030 tokens\n",
      " 17%|█▋        | 18/105 [05:03<22:23, 15.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    81 runs   (    0.07 ms per token, 13846.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4484.09 ms /   341 tokens (   13.15 ms per token,    76.05 tokens per second)\n",
      "llama_print_timings:        eval time =    5660.04 ms /    80 runs   (   70.75 ms per token,    14.13 tokens per second)\n",
      "llama_print_timings:       total time =   10251.57 ms /   421 tokens\n",
      " 18%|█▊        | 19/105 [05:13<19:53, 13.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   182 runs   (    0.09 ms per token, 11735.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3515.39 ms /   238 tokens (   14.77 ms per token,    67.70 tokens per second)\n",
      "llama_print_timings:        eval time =   15927.05 ms /   181 runs   (   87.99 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   19766.08 ms /   419 tokens\n",
      " 19%|█▉        | 20/105 [05:33<22:10, 15.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.07 ms /   101 runs   (    0.08 ms per token, 12518.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15536.59 ms /  1083 tokens (   14.35 ms per token,    69.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9227.53 ms /   100 runs   (   92.28 ms per token,    10.84 tokens per second)\n",
      "llama_print_timings:       total time =   24946.59 ms /  1183 tokens\n",
      " 20%|██        | 21/105 [05:58<25:49, 18.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.85 ms /   110 runs   (    0.08 ms per token, 12432.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12172.04 ms /   808 tokens (   15.06 ms per token,    66.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9465.85 ms /   109 runs   (   86.84 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =   21811.00 ms /   917 tokens\n",
      " 21%|██        | 22/105 [06:19<26:54, 19.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    71 runs   (    0.07 ms per token, 13373.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4226.15 ms /   293 tokens (   14.42 ms per token,    69.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5265.66 ms /    70 runs   (   75.22 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:       total time =    9590.16 ms /   363 tokens\n",
      " 22%|██▏       | 23/105 [06:29<22:32, 16.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.70 ms /    78 runs   (    0.07 ms per token, 13696.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4247.79 ms /   296 tokens (   14.35 ms per token,    69.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5520.36 ms /    77 runs   (   71.69 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =    9869.79 ms /   373 tokens\n",
      " 23%|██▎       | 24/105 [06:39<19:35, 14.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    64 runs   (    0.07 ms per token, 13976.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4120.01 ms /   308 tokens (   13.38 ms per token,    74.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.20 ms /    63 runs   (   68.88 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    8540.67 ms /   371 tokens\n",
      " 24%|██▍       | 25/105 [06:47<16:57, 12.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    78 runs   (    0.07 ms per token, 13499.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4065.81 ms /   315 tokens (   12.91 ms per token,    77.48 tokens per second)\n",
      "llama_print_timings:        eval time =    5348.35 ms /    77 runs   (   69.46 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =    9512.26 ms /   392 tokens\n",
      " 25%|██▍       | 26/105 [06:57<15:28, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.22 ms /    56 runs   (    0.08 ms per token, 13263.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3847.68 ms /   259 tokens (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_print_timings:        eval time =    5008.56 ms /    55 runs   (   91.06 ms per token,    10.98 tokens per second)\n",
      "llama_print_timings:       total time =    8936.40 ms /   314 tokens\n",
      " 26%|██▌       | 27/105 [07:06<14:11, 10.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.99 ms /    81 runs   (    0.07 ms per token, 13518.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17521.50 ms /  1057 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7041.06 ms /    80 runs   (   88.01 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   24709.36 ms /  1137 tokens\n",
      " 27%|██▋       | 28/105 [07:31<19:19, 15.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.92 ms /   123 runs   (    0.08 ms per token, 12396.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12603.20 ms /   930 tokens (   13.55 ms per token,    73.79 tokens per second)\n",
      "llama_print_timings:        eval time =   10309.66 ms /   122 runs   (   84.51 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =   23095.25 ms /  1052 tokens\n",
      " 28%|██▊       | 29/105 [07:54<22:07, 17.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.87 ms /   104 runs   (    0.08 ms per token, 13211.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9833.53 ms /   697 tokens (   14.11 ms per token,    70.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7737.02 ms /   103 runs   (   75.12 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   17768.61 ms /   800 tokens\n",
      " 29%|██▊       | 30/105 [08:11<21:57, 17.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    84 runs   (    0.07 ms per token, 13401.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4805.73 ms /   371 tokens (   12.95 ms per token,    77.20 tokens per second)\n",
      "llama_print_timings:        eval time =    6118.11 ms /    83 runs   (   73.71 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   11038.59 ms /   454 tokens\n",
      " 30%|██▉       | 31/105 [08:23<19:14, 15.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.20 ms /   111 runs   (    0.07 ms per token, 13531.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9006.80 ms /   626 tokens (   14.39 ms per token,    69.50 tokens per second)\n",
      "llama_print_timings:        eval time =    8161.48 ms /   110 runs   (   74.20 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =   17318.64 ms /   736 tokens\n",
      " 30%|███       | 32/105 [08:40<19:36, 16.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.11 ms /   127 runs   (    0.07 ms per token, 13939.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9731.61 ms /   711 tokens (   13.69 ms per token,    73.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9324.93 ms /   126 runs   (   74.01 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   19227.06 ms /   837 tokens\n",
      " 31%|███▏      | 33/105 [08:59<20:27, 17.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    61 runs   (    0.07 ms per token, 13994.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4126.23 ms /   304 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4175.11 ms /    60 runs   (   69.59 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =    8379.34 ms /   364 tokens\n",
      " 32%|███▏      | 34/105 [09:07<17:06, 14.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    72 runs   (    0.07 ms per token, 13667.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4071.03 ms /   316 tokens (   12.88 ms per token,    77.62 tokens per second)\n",
      "llama_print_timings:        eval time =    5331.68 ms /    71 runs   (   75.09 ms per token,    13.32 tokens per second)\n",
      "llama_print_timings:       total time =    9496.14 ms /   387 tokens\n",
      " 33%|███▎      | 35/105 [09:17<15:07, 12.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.88 ms /    61 runs   (    0.08 ms per token, 12487.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4322.47 ms /   314 tokens (   13.77 ms per token,    72.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5330.58 ms /    60 runs   (   88.84 ms per token,    11.26 tokens per second)\n",
      "llama_print_timings:       total time =    9741.41 ms /   374 tokens\n",
      " 34%|███▍      | 36/105 [09:27<13:48, 12.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    68 runs   (    0.07 ms per token, 13580.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4607.35 ms /   310 tokens (   14.86 ms per token,    67.28 tokens per second)\n",
      "llama_print_timings:        eval time =    5317.82 ms /    67 runs   (   79.37 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   10021.82 ms /   377 tokens\n",
      " 35%|███▌      | 37/105 [09:37<12:55, 11.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.14 ms /    98 runs   (    0.07 ms per token, 13721.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4367.67 ms /   328 tokens (   13.32 ms per token,    75.10 tokens per second)\n",
      "llama_print_timings:        eval time =    7200.18 ms /    97 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   11699.65 ms /   425 tokens\n",
      " 36%|███▌      | 38/105 [09:48<12:50, 11.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.70 ms /    76 runs   (    0.13 ms per token,  7836.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4418.74 ms /   314 tokens (   14.07 ms per token,    71.06 tokens per second)\n",
      "llama_print_timings:        eval time =    6237.05 ms /    75 runs   (   83.16 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =   10771.34 ms /   389 tokens\n",
      " 37%|███▋      | 39/105 [09:59<12:24, 11.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.60 ms /    86 runs   (    0.08 ms per token, 13022.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4441.21 ms /   324 tokens (   13.71 ms per token,    72.95 tokens per second)\n",
      "llama_print_timings:        eval time =    6506.57 ms /    85 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   11060.68 ms /   409 tokens\n",
      " 38%|███▊      | 40/105 [10:10<12:09, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    62 runs   (    0.07 ms per token, 13632.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3620.27 ms /   194 tokens (   18.66 ms per token,    53.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4190.22 ms /    61 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    7942.11 ms /   255 tokens\n",
      " 39%|███▉      | 41/105 [10:18<10:55, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.80 ms /    68 runs   (    0.07 ms per token, 14157.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10070.95 ms /   775 tokens (   12.99 ms per token,    76.95 tokens per second)\n",
      "llama_print_timings:        eval time =    4937.98 ms /    67 runs   (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   15098.97 ms /   842 tokens\n",
      " 40%|████      | 42/105 [10:33<12:16, 11.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.33 ms /    73 runs   (    0.07 ms per token, 13693.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4436.19 ms /   315 tokens (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_print_timings:        eval time =    5065.36 ms /    72 runs   (   70.35 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =    9597.98 ms /   387 tokens\n",
      " 41%|████      | 43/105 [10:43<11:26, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    69 runs   (    0.07 ms per token, 13942.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4486.76 ms /   311 tokens (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4871.93 ms /    68 runs   (   71.65 ms per token,    13.96 tokens per second)\n",
      "llama_print_timings:       total time =    9452.34 ms /   379 tokens\n",
      " 42%|████▏     | 44/105 [10:52<10:45, 10.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    64 runs   (    0.07 ms per token, 13736.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4213.04 ms /   310 tokens (   13.59 ms per token,    73.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.82 ms /    63 runs   (   79.03 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    9285.74 ms /   373 tokens\n",
      " 43%|████▎     | 45/105 [11:02<10:11, 10.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.06 ms /    55 runs   (    0.07 ms per token, 13546.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4146.68 ms /   309 tokens (   13.42 ms per token,    74.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4321.90 ms /    54 runs   (   80.04 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =    8542.50 ms /   363 tokens\n",
      " 44%|████▍     | 46/105 [11:10<09:32,  9.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    64 runs   (    0.08 ms per token, 12497.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4440.80 ms /   319 tokens (   13.92 ms per token,    71.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5867.98 ms /    63 runs   (   93.14 ms per token,    10.74 tokens per second)\n",
      "llama_print_timings:       total time =   10408.36 ms /   382 tokens\n",
      " 45%|████▍     | 47/105 [11:21<09:35,  9.92s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    81 runs   (    0.07 ms per token, 13477.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4493.69 ms /   324 tokens (   13.87 ms per token,    72.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5919.42 ms /    80 runs   (   73.99 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   10522.54 ms /   404 tokens\n",
      " 46%|████▌     | 48/105 [11:31<09:35, 10.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    76 runs   (    0.07 ms per token, 13556.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4740.74 ms /   324 tokens (   14.63 ms per token,    68.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5555.56 ms /    75 runs   (   74.07 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =   10400.82 ms /   399 tokens\n",
      " 47%|████▋     | 49/105 [11:42<09:31, 10.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    66 runs   (    0.07 ms per token, 13403.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4572.53 ms /   333 tokens (   13.73 ms per token,    72.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5701.34 ms /    65 runs   (   87.71 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =   10362.27 ms /   398 tokens\n",
      " 48%|████▊     | 50/105 [11:52<09:23, 10.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.62 ms /    77 runs   (    0.07 ms per token, 13688.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.80 ms /   314 tokens (   13.35 ms per token,    74.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5755.48 ms /    76 runs   (   75.73 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:       total time =   10053.69 ms /   390 tokens\n",
      " 49%|████▊     | 51/105 [12:02<09:10, 10.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.26 ms /   131 runs   (    0.08 ms per token, 12768.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4514.67 ms /   332 tokens (   13.60 ms per token,    73.54 tokens per second)\n",
      "llama_print_timings:        eval time =    9574.88 ms /   130 runs   (   73.65 ms per token,    13.58 tokens per second)\n",
      "llama_print_timings:       total time =   14270.87 ms /   462 tokens\n",
      " 50%|████▉     | 52/105 [12:16<10:05, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.77 ms /    81 runs   (    0.07 ms per token, 14043.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.86 ms /   320 tokens (   13.11 ms per token,    76.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5613.53 ms /    80 runs   (   70.17 ms per token,    14.25 tokens per second)\n",
      "llama_print_timings:       total time =    9914.02 ms /   400 tokens\n",
      " 50%|█████     | 53/105 [12:26<09:30, 10.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    67 runs   (    0.07 ms per token, 13546.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4648.98 ms /   331 tokens (   14.05 ms per token,    71.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4758.04 ms /    66 runs   (   72.09 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =    9496.04 ms /   397 tokens\n",
      " 51%|█████▏    | 54/105 [12:36<08:56, 10.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    91 runs   (    0.07 ms per token, 14110.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4488.54 ms /   338 tokens (   13.28 ms per token,    75.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6494.47 ms /    90 runs   (   72.16 ms per token,    13.86 tokens per second)\n",
      "llama_print_timings:       total time =   11102.74 ms /   428 tokens\n",
      " 52%|█████▏    | 55/105 [12:47<08:55, 10.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    71 runs   (    0.08 ms per token, 13001.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4220.82 ms /   319 tokens (   13.23 ms per token,    75.58 tokens per second)\n",
      "llama_print_timings:        eval time =    7029.03 ms /    70 runs   (  100.41 ms per token,     9.96 tokens per second)\n",
      "llama_print_timings:       total time =   11354.66 ms /   389 tokens\n",
      " 53%|█████▎    | 56/105 [12:58<08:54, 10.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.00 ms /    67 runs   (    0.07 ms per token, 13389.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4883.53 ms /   335 tokens (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5277.21 ms /    66 runs   (   79.96 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =   10257.99 ms /   401 tokens\n",
      " 54%|█████▍    | 57/105 [13:08<08:34, 10.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    61 runs   (    0.09 ms per token, 11026.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6070.87 ms /   315 tokens (   19.27 ms per token,    51.89 tokens per second)\n",
      "llama_print_timings:        eval time =   10581.76 ms /    60 runs   (  176.36 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   16765.97 ms /   375 tokens\n",
      " 55%|█████▌    | 58/105 [13:25<09:48, 12.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    75 runs   (    0.08 ms per token, 12481.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4882.95 ms /   328 tokens (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:        eval time =    6817.42 ms /    74 runs   (   92.13 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   11813.78 ms /   402 tokens\n",
      " 56%|█████▌    | 59/105 [13:37<09:26, 12.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    69 runs   (    0.08 ms per token, 11977.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4382.81 ms /   310 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_print_timings:        eval time =    8225.75 ms /    68 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   12738.98 ms /   378 tokens\n",
      " 57%|█████▋    | 60/105 [13:50<09:20, 12.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.26 ms /    68 runs   (    0.08 ms per token, 12920.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4881.43 ms /   323 tokens (   15.11 ms per token,    66.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5568.03 ms /    67 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   10549.72 ms /   390 tokens\n",
      " 58%|█████▊    | 61/105 [14:00<08:42, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    88 runs   (    0.08 ms per token, 12454.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5372.31 ms /   313 tokens (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:        eval time =    8534.59 ms /    87 runs   (   98.10 ms per token,    10.19 tokens per second)\n",
      "llama_print_timings:       total time =   14034.08 ms /   400 tokens\n",
      " 59%|█████▉    | 62/105 [14:14<08:58, 12.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.22 ms /    79 runs   (    0.08 ms per token, 12705.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5511.11 ms /   321 tokens (   17.17 ms per token,    58.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7583.33 ms /    78 runs   (   97.22 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   13208.67 ms /   399 tokens\n",
      " 60%|██████    | 63/105 [14:28<08:54, 12.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.18 ms /    68 runs   (    0.08 ms per token, 13137.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5034.60 ms /   314 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5964.39 ms /    67 runs   (   89.02 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   11094.65 ms /   381 tokens\n",
      " 61%|██████    | 64/105 [14:39<08:22, 12.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    67 runs   (    0.08 ms per token, 13196.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4702.23 ms /   328 tokens (   14.34 ms per token,    69.75 tokens per second)\n",
      "llama_print_timings:        eval time =    5756.94 ms /    66 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =   10554.38 ms /   394 tokens\n",
      " 62%|██████▏   | 65/105 [14:49<07:49, 11.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.98 ms /    70 runs   (    0.07 ms per token, 14061.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4376.77 ms /   322 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4817.30 ms /    69 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =    9284.69 ms /   391 tokens\n",
      " 63%|██████▎   | 66/105 [14:59<07:09, 11.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /   145 runs   (    0.08 ms per token, 13274.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5049.51 ms /   320 tokens (   15.78 ms per token,    63.37 tokens per second)\n",
      "llama_print_timings:        eval time =   13034.02 ms /   144 runs   (   90.51 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =   18287.23 ms /   464 tokens\n",
      " 64%|██████▍   | 67/105 [15:17<08:21, 13.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    62 runs   (    0.07 ms per token, 13540.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4503.94 ms /   322 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_print_timings:        eval time =    5259.46 ms /    61 runs   (   86.22 ms per token,    11.60 tokens per second)\n",
      "llama_print_timings:       total time =    9850.95 ms /   383 tokens\n",
      " 65%|██████▍   | 68/105 [15:27<07:31, 12.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    85 runs   (    0.07 ms per token, 13634.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4479.93 ms /   305 tokens (   14.69 ms per token,    68.08 tokens per second)\n",
      "llama_print_timings:        eval time =    6544.20 ms /    84 runs   (   77.91 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =   11138.60 ms /   389 tokens\n",
      " 66%|██████▌   | 69/105 [15:38<07:07, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.44 ms /   122 runs   (    0.08 ms per token, 12919.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15791.34 ms /  1139 tokens (   13.86 ms per token,    72.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8874.82 ms /   121 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   24903.48 ms /  1260 tokens\n",
      " 67%|██████▋   | 70/105 [16:03<09:12, 15.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    78 runs   (    0.08 ms per token, 13087.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8680.26 ms /   589 tokens (   14.74 ms per token,    67.86 tokens per second)\n",
      "llama_print_timings:        eval time =    6454.06 ms /    77 runs   (   83.82 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   15250.63 ms /   666 tokens\n",
      " 68%|██████▊   | 71/105 [16:18<08:51, 15.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.60 ms /   173 runs   (    0.07 ms per token, 13732.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4675.94 ms /   327 tokens (   14.30 ms per token,    69.93 tokens per second)\n",
      "llama_print_timings:        eval time =   12378.50 ms /   172 runs   (   71.97 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =   17326.14 ms /   499 tokens\n",
      " 69%|██████▊   | 72/105 [16:35<08:52, 16.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.78 ms /   158 runs   (    0.07 ms per token, 13408.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4552.41 ms /   350 tokens (   13.01 ms per token,    76.88 tokens per second)\n",
      "llama_print_timings:        eval time =   12186.29 ms /   157 runs   (   77.62 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =   16960.40 ms /   507 tokens\n",
      " 70%|██████▉   | 73/105 [16:52<08:44, 16.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.07 ms /    84 runs   (    0.07 ms per token, 13831.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4379.36 ms /   336 tokens (   13.03 ms per token,    76.72 tokens per second)\n",
      "llama_print_timings:        eval time =    5774.36 ms /    83 runs   (   69.57 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =   10261.96 ms /   419 tokens\n",
      " 70%|███████   | 74/105 [17:03<07:31, 14.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    60 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4432.49 ms /   337 tokens (   13.15 ms per token,    76.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4128.95 ms /    59 runs   (   69.98 ms per token,    14.29 tokens per second)\n",
      "llama_print_timings:       total time =    8640.77 ms /   396 tokens\n",
      " 71%|███████▏  | 75/105 [17:11<06:23, 12.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.16 ms /    58 runs   (    0.07 ms per token, 13949.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4367.05 ms /   327 tokens (   13.35 ms per token,    74.88 tokens per second)\n",
      "llama_print_timings:        eval time =    3929.28 ms /    57 runs   (   68.93 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =    8370.63 ms /   384 tokens\n",
      " 72%|███████▏  | 76/105 [17:20<05:32, 11.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.05 ms /    75 runs   (    0.08 ms per token, 12400.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5002.88 ms /   320 tokens (   15.63 ms per token,    63.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5766.64 ms /    74 runs   (   77.93 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   10875.72 ms /   394 tokens\n",
      " 73%|███████▎  | 77/105 [17:31<05:15, 11.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.97 ms /    76 runs   (    0.08 ms per token, 12732.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4720.40 ms /   318 tokens (   14.84 ms per token,    67.37 tokens per second)\n",
      "llama_print_timings:        eval time =    6327.79 ms /    75 runs   (   84.37 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =   11162.61 ms /   393 tokens\n",
      " 74%|███████▍  | 78/105 [17:42<05:03, 11.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.80 ms /    52 runs   (    0.07 ms per token, 13691.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2654.93 ms /   146 tokens (   18.18 ms per token,    54.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3493.33 ms /    51 runs   (   68.50 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =    6273.22 ms /   197 tokens\n",
      " 75%|███████▌  | 79/105 [17:48<04:13,  9.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    92 runs   (    0.07 ms per token, 13795.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5193.46 ms /   407 tokens (   12.76 ms per token,    78.37 tokens per second)\n",
      "llama_print_timings:        eval time =    7173.16 ms /    91 runs   (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =   12492.71 ms /   498 tokens\n",
      " 76%|███████▌  | 80/105 [18:00<04:24, 10.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    59 runs   (    0.07 ms per token, 13597.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5549.97 ms /   387 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4262.49 ms /    58 runs   (   73.49 ms per token,    13.61 tokens per second)\n",
      "llama_print_timings:       total time =    9893.96 ms /   445 tokens\n",
      " 77%|███████▋  | 81/105 [18:10<04:09, 10.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    75 runs   (    0.07 ms per token, 13748.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4683.48 ms /   355 tokens (   13.19 ms per token,    75.80 tokens per second)\n",
      "llama_print_timings:        eval time =    5188.92 ms /    74 runs   (   70.12 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =    9971.89 ms /   429 tokens\n",
      " 78%|███████▊  | 82/105 [18:20<03:55, 10.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.88 ms /    76 runs   (    0.08 ms per token, 12929.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4334.12 ms /   334 tokens (   12.98 ms per token,    77.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5577.45 ms /    75 runs   (   74.37 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =   10013.67 ms /   409 tokens\n",
      " 79%|███████▉  | 83/105 [18:30<03:44, 10.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.35 ms /   126 runs   (    0.07 ms per token, 13477.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4757.94 ms /   380 tokens (   12.52 ms per token,    79.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9321.91 ms /   125 runs   (   74.58 ms per token,    13.41 tokens per second)\n",
      "llama_print_timings:       total time =   14254.99 ms /   505 tokens\n",
      " 80%|████████  | 84/105 [18:45<03:59, 11.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    90 runs   (    0.08 ms per token, 13090.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4541.60 ms /   327 tokens (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7609.56 ms /    89 runs   (   85.50 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   12289.77 ms /   416 tokens\n",
      " 81%|████████  | 85/105 [18:57<03:53, 11.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.35 ms /    88 runs   (    0.07 ms per token, 13862.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4376.23 ms /   320 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6058.78 ms /    87 runs   (   69.64 ms per token,    14.36 tokens per second)\n",
      "llama_print_timings:       total time =   10552.40 ms /   407 tokens\n",
      " 82%|████████▏ | 86/105 [19:07<03:35, 11.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    62 runs   (    0.08 ms per token, 13301.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4415.28 ms /   325 tokens (   13.59 ms per token,    73.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4560.50 ms /    61 runs   (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =    9060.40 ms /   386 tokens\n",
      " 83%|████████▎ | 87/105 [19:17<03:11, 10.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.05 ms /    54 runs   (    0.08 ms per token, 13326.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4559.60 ms /   321 tokens (   14.20 ms per token,    70.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4064.79 ms /    53 runs   (   76.69 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:       total time =    8701.39 ms /   374 tokens\n",
      " 84%|████████▍ | 88/105 [19:25<02:51, 10.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    78 runs   (    0.08 ms per token, 12590.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4573.68 ms /   341 tokens (   13.41 ms per token,    74.56 tokens per second)\n",
      "llama_print_timings:        eval time =    6698.34 ms /    77 runs   (   86.99 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =   11387.57 ms /   418 tokens\n",
      " 85%|████████▍ | 89/105 [19:37<02:47, 10.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.17 ms /    67 runs   (    0.08 ms per token, 12961.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4819.09 ms /   365 tokens (   13.20 ms per token,    75.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.91 ms /    66 runs   (   72.62 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    9707.50 ms /   431 tokens\n",
      " 86%|████████▌ | 90/105 [19:46<02:33, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.65 ms /    75 runs   (    0.08 ms per token, 13281.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14919.27 ms /  1064 tokens (   14.02 ms per token,    71.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5777.17 ms /    74 runs   (   78.07 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =   20799.71 ms /  1138 tokens\n",
      " 87%|████████▋ | 91/105 [20:07<03:07, 13.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    91 runs   (    0.07 ms per token, 13982.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15692.59 ms /  1106 tokens (   14.19 ms per token,    70.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6643.57 ms /    90 runs   (   73.82 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   22460.06 ms /  1196 tokens\n",
      " 88%|████████▊ | 92/105 [20:30<03:29, 16.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.85 ms /   106 runs   (    0.07 ms per token, 13503.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15377.62 ms /  1147 tokens (   13.41 ms per token,    74.59 tokens per second)\n",
      "llama_print_timings:        eval time =    8463.62 ms /   105 runs   (   80.61 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   23991.32 ms /  1252 tokens\n",
      " 89%|████████▊ | 93/105 [20:54<03:41, 18.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.89 ms /    90 runs   (    0.08 ms per token, 13062.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17751.96 ms /  1205 tokens (   14.73 ms per token,    67.88 tokens per second)\n",
      "llama_print_timings:        eval time =    8864.87 ms /    89 runs   (   99.61 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   26761.35 ms /  1294 tokens\n",
      " 90%|████████▉ | 94/105 [21:20<03:50, 20.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.60 ms /   113 runs   (    0.08 ms per token, 11769.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15265.20 ms /  1086 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9282.77 ms /   112 runs   (   82.88 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   24721.91 ms /  1198 tokens\n",
      " 90%|█████████ | 95/105 [21:45<03:41, 22.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    64 runs   (    0.08 ms per token, 13195.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4773.98 ms /   336 tokens (   14.21 ms per token,    70.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5235.06 ms /    63 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   10118.43 ms /   399 tokens\n",
      " 91%|█████████▏| 96/105 [21:55<02:46, 18.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.73 ms /    84 runs   (    0.08 ms per token, 12477.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4878.62 ms /   330 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:        eval time =    7240.31 ms /    83 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =   12242.22 ms /   413 tokens\n",
      " 92%|█████████▏| 97/105 [22:08<02:13, 16.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    73 runs   (    0.07 ms per token, 14070.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4952.04 ms /   331 tokens (   14.96 ms per token,    66.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5470.87 ms /    72 runs   (   75.98 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time =   10520.57 ms /   403 tokens\n",
      " 93%|█████████▎| 98/105 [22:18<01:43, 14.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.40 ms /    89 runs   (    0.07 ms per token, 13901.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4116.96 ms /   320 tokens (   12.87 ms per token,    77.73 tokens per second)\n",
      "llama_print_timings:        eval time =    6144.33 ms /    88 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =   10377.72 ms /   408 tokens\n",
      " 94%|█████████▍| 99/105 [22:28<01:20, 13.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    67 runs   (    0.07 ms per token, 13640.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4136.23 ms /   320 tokens (   12.93 ms per token,    77.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5365.87 ms /    66 runs   (   81.30 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    9592.95 ms /   386 tokens\n",
      " 95%|█████████▌| 100/105 [22:38<01:01, 12.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    66 runs   (    0.08 ms per token, 13163.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3915.14 ms /   273 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4644.90 ms /    65 runs   (   71.46 ms per token,    13.99 tokens per second)\n",
      "llama_print_timings:       total time =    8652.12 ms /   338 tokens\n",
      " 96%|█████████▌| 101/105 [22:47<00:44, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    91 runs   (    0.08 ms per token, 12250.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15110.19 ms /  1108 tokens (   13.64 ms per token,    73.33 tokens per second)\n",
      "llama_print_timings:        eval time =    9447.23 ms /    90 runs   (  104.97 ms per token,     9.53 tokens per second)\n",
      "llama_print_timings:       total time =   24695.74 ms /  1198 tokens\n",
      " 97%|█████████▋| 102/105 [23:11<00:45, 15.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.87 ms /   163 runs   (    0.07 ms per token, 13735.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11897.76 ms /   946 tokens (   12.58 ms per token,    79.51 tokens per second)\n",
      "llama_print_timings:        eval time =   12070.48 ms /   162 runs   (   74.51 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   24195.03 ms /  1108 tokens\n",
      " 98%|█████████▊| 103/105 [23:36<00:35, 17.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.10 ms /    98 runs   (    0.07 ms per token, 13806.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14670.87 ms /  1086 tokens (   13.51 ms per token,    74.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8289.89 ms /    97 runs   (   85.46 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   23099.07 ms /  1183 tokens\n",
      " 99%|█████████▉| 104/105 [23:59<00:19, 19.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    99 runs   (    0.08 ms per token, 13201.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10012.97 ms /   681 tokens (   14.70 ms per token,    68.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7857.40 ms /    98 runs   (   80.18 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =   18013.18 ms /   779 tokens\n",
      "100%|██████████| 105/105 [24:17<00:00, 13.88s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results_dict = {}\n",
    "for parser_name, parser in parsers.items():\n",
    "    print(parser_name, \"\\n\")\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    qa_dataset = generate_question_context_pairs(\n",
    "        nodes,\n",
    "        llm=llm,\n",
    "        num_questions_per_chunk=2\n",
    "    )\n",
    "\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever)\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)  # Can't put this line in a function otherwise it raises an error\n",
    "    eval_results_dict[parser_name] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter       1.0  0.935185\n",
       "1   token_splitter_512       1.0  0.892857\n",
       "2  token_splitter_1024       1.0  0.875000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1_results = os.path.join(results_folder, \"results_2_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = pd.read_csv(batch_1_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Splitter accuracy was way below others that is why it is not going to be used (cf results_2_docs_Sentence_Semantic_Token.csv)\n",
    "\n",
    "On two documents (7 chunks for SemanticSplitter, 5 for Token512 and 2 for Token1024) : Semantic has the higher score and increased token size for token splitter seems to lower HR and MRR\n",
    "\n",
    "Try to evaluate on more documents/chunks to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.698276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.622363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter  0.801724  0.698276\n",
       "1   token_splitter_512  0.717143  0.607143\n",
       "2  token_splitter_1024  0.721519  0.622363"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2_results = os.path.join(results_folder, \"results_10_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = pd.read_csv(batch_2_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 10 documents (49 chunks for Semantic, 121 for Token512 and 52 for Token1024) : Semantic splitter is still better than TokenSplitter, but the Token1024 is slightly better than the 512\n",
    "\n",
    "Timer : 10.51 for Semantic, 20.40 for Token512, 14.57 for Token1024 ==> Meaning that, with increased database volume the Semantic Splitter is going to be more and more slow than the TokenSplitter BUT the results are way better so it is a trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.684739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>0.766615</td>\n",
       "      <td>0.649923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.685751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter  0.783133  0.684739\n",
       "1   token_splitter_512  0.766615  0.649923\n",
       "2  token_splitter_1024  0.763359  0.685751"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_3_results = os.path.join(results_folder, \"results_23_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = compute_hit_hrr_results(eval_results_dict)\n",
    "df.to_csv(batch_3_results, index=False, sep=\";\")\n",
    "\n",
    "df = pd.read_csv(batch_3_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, on 23 documents (101 chunks for Semantic, 261 for Token512 and 105 for Token 1024) : Semantic splitter is still better than TokenSplitter but TokenSPlitter has increased its HitRate and MRR\n",
    "\n",
    "Timer : 26.36 for Semantic, 46.23 for Token512 and 24.17 for Token1024 ==> Meaning that, with increased database volume the Semantic Splitter is going to be more and more slow than the TokenSplitter and the results tend to decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = token_splitter_512.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jsonpickle\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "import re\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import VectorStoreIndex, load_index_from_storage, Settings\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, RetrieverEvaluator, FaithfulnessEvaluator, RelevancyEvaluator, AnswerRelevancyEvaluator, BatchEvalRunner, CorrectnessEvaluator\n",
    "import nest_asyncio\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "# To allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_engine(sentence_index, similarity_top_k=3, rerank_top_n=2):\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\", device=\"mps\"\n",
    "    )\n",
    "    engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[rerank]\n",
    "    )\n",
    "\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, (11414.77 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 10\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, (11734.77 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, (12294.78 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "batch = \"batch_2\"\n",
    "input_folder = f\"./data_evaluation/{batch}/files/\"\n",
    "batch_folder = os.path.join(\"data_evaluation\", batch)\n",
    "results_folder = os.path.join(batch_folder, \"results\")\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,  # Change to put 1024 ?\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "# service_context_llm_base = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your API KEY here\n",
    "api_key = input(\"Put your API key here\")\n",
    "\n",
    "# Load Mixtral 8x7b model\n",
    "llm_mixtral = MistralAI(api_key=api_key, \n",
    "                        model=\"open-mixtral-8x7b\", \n",
    "                        temperature=0.0,\n",
    "                        max_tokens=1024)\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "semantic_splitter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:40<00:00,  1.22it/s]\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.97 ms /   146 runs   (    0.10 ms per token,  9754.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14047.73 ms /   505 tokens (   27.82 ms per token,    35.95 tokens per second)\n",
      "llama_print_timings:        eval time =   13843.06 ms /   145 runs   (   95.47 ms per token,    10.47 tokens per second)\n",
      "llama_print_timings:       total time =   28089.91 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.65 ms /   340 runs   (    0.09 ms per token, 11094.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22957.39 ms /  1340 tokens (   17.13 ms per token,    58.37 tokens per second)\n",
      "llama_print_timings:        eval time =   26499.09 ms /   339 runs   (   78.17 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =   49937.43 ms /  1679 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.91 ms /   234 runs   (    0.10 ms per token,  9786.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5516.69 ms /   374 tokens (   14.75 ms per token,    67.79 tokens per second)\n",
      "llama_print_timings:        eval time =   20040.08 ms /   233 runs   (   86.01 ms per token,    11.63 tokens per second)\n",
      "llama_print_timings:       total time =   25889.64 ms /   607 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    34 runs   (    0.16 ms per token,  6256.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5823.39 ms /   182 tokens (   32.00 ms per token,    31.25 tokens per second)\n",
      "llama_print_timings:        eval time =   10957.98 ms /    33 runs   (  332.06 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   16857.22 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    40 runs   (    0.13 ms per token,  7642.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7781.91 ms /   373 tokens (   20.86 ms per token,    47.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5550.57 ms /    39 runs   (  142.32 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   13398.35 ms /   412 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.29 ms /    85 runs   (    0.10 ms per token, 10247.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2908.86 ms /    47 tokens (   61.89 ms per token,    16.16 tokens per second)\n",
      "llama_print_timings:        eval time =    6948.90 ms /    84 runs   (   82.73 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =    9978.49 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    64 runs   (    0.10 ms per token,  9560.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4948.30 ms /   203 tokens (   24.38 ms per token,    41.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5578.10 ms /    63 runs   (   88.54 ms per token,    11.29 tokens per second)\n",
      "llama_print_timings:       total time =   10616.73 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.81 ms /   228 runs   (    0.09 ms per token, 10954.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4901.48 ms /   250 tokens (   19.61 ms per token,    51.01 tokens per second)\n",
      "llama_print_timings:        eval time =   17146.87 ms /   227 runs   (   75.54 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   22369.17 ms /   477 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.35 ms /   144 runs   (    0.09 ms per token, 11660.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12043.82 ms /   798 tokens (   15.09 ms per token,    66.26 tokens per second)\n",
      "llama_print_timings:        eval time =   10969.21 ms /   143 runs   (   76.71 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:       total time =   23211.85 ms /   941 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.76 ms /   101 runs   (    0.10 ms per token, 10349.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5966.18 ms /   386 tokens (   15.46 ms per token,    64.70 tokens per second)\n",
      "llama_print_timings:        eval time =    7335.35 ms /   100 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   13436.25 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.83 ms /   126 runs   (    0.11 ms per token,  9108.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14495.74 ms /   925 tokens (   15.67 ms per token,    63.81 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.03 ms /   125 runs   (   90.35 ms per token,    11.07 tokens per second)\n",
      "llama_print_timings:       total time =   25970.29 ms /  1050 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.47 ms /    94 runs   (    0.10 ms per token,  9920.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12194.96 ms /   720 tokens (   16.94 ms per token,    59.04 tokens per second)\n",
      "llama_print_timings:        eval time =    7145.97 ms /    93 runs   (   76.84 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:       total time =   19480.68 ms /   813 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.41 ms /   323 runs   (    0.11 ms per token,  9386.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19761.44 ms /  1012 tokens (   19.53 ms per token,    51.21 tokens per second)\n",
      "llama_print_timings:        eval time =   44144.34 ms /   322 runs   (  137.09 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:       total time =   64394.76 ms /  1334 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.54 ms /   101 runs   (    0.10 ms per token,  9578.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5839.69 ms /   386 tokens (   15.13 ms per token,    66.10 tokens per second)\n",
      "llama_print_timings:        eval time =    8205.20 ms /   100 runs   (   82.05 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   14193.00 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.92 ms /   314 runs   (    0.11 ms per token,  8990.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6283.40 ms /   445 tokens (   14.12 ms per token,    70.82 tokens per second)\n",
      "llama_print_timings:        eval time =   23718.34 ms /   313 runs   (   75.78 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:       total time =   30590.17 ms /   758 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      40.07 ms /   429 runs   (    0.09 ms per token, 10705.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14174.66 ms /  1009 tokens (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_print_timings:        eval time =   35045.25 ms /   428 runs   (   81.88 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   49937.21 ms /  1437 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.43 ms /   146 runs   (    0.10 ms per token, 10119.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4915.75 ms /   278 tokens (   17.68 ms per token,    56.55 tokens per second)\n",
      "llama_print_timings:        eval time =   10700.50 ms /   145 runs   (   73.80 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   15815.38 ms /   423 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.14 ms /   441 runs   (    0.10 ms per token, 10466.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11444.00 ms /   674 tokens (   16.98 ms per token,    58.90 tokens per second)\n",
      "llama_print_timings:        eval time =   43075.74 ms /   440 runs   (   97.90 ms per token,    10.21 tokens per second)\n",
      "llama_print_timings:       total time =   55192.49 ms /  1114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.58 ms /    66 runs   (    0.10 ms per token, 10025.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2744.33 ms /    37 tokens (   74.17 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6412.20 ms /    65 runs   (   98.65 ms per token,    10.14 tokens per second)\n",
      "llama_print_timings:       total time =    9251.98 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.55 ms /   116 runs   (    0.11 ms per token,  9241.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5659.19 ms /   384 tokens (   14.74 ms per token,    67.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9242.18 ms /   115 runs   (   80.37 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   15066.36 ms /   499 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.51 ms /   241 runs   (    0.09 ms per token, 11205.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20450.41 ms /  1409 tokens (   14.51 ms per token,    68.90 tokens per second)\n",
      "llama_print_timings:        eval time =   17605.17 ms /   240 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   38596.93 ms /  1649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.39 ms /   151 runs   (    0.10 ms per token,  9811.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5973.08 ms /   385 tokens (   15.51 ms per token,    64.46 tokens per second)\n",
      "llama_print_timings:        eval time =   11977.28 ms /   150 runs   (   79.85 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   18162.54 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.80 ms /   135 runs   (    0.11 ms per token,  9120.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15256.85 ms /   988 tokens (   15.44 ms per token,    64.76 tokens per second)\n",
      "llama_print_timings:        eval time =   16684.11 ms /   134 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   32168.66 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      47.90 ms /   504 runs   (    0.10 ms per token, 10521.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4529.14 ms /   213 tokens (   21.26 ms per token,    47.03 tokens per second)\n",
      "llama_print_timings:        eval time =   39086.72 ms /   503 runs   (   77.71 ms per token,    12.87 tokens per second)\n",
      "llama_print_timings:       total time =   44371.20 ms /   716 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.04 ms /   116 runs   (    0.10 ms per token,  9632.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5199.85 ms /   278 tokens (   18.70 ms per token,    53.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9415.15 ms /   115 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   14781.38 ms /   393 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.99 ms /   241 runs   (    0.10 ms per token,  9644.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24475.21 ms /  1409 tokens (   17.37 ms per token,    57.57 tokens per second)\n",
      "llama_print_timings:        eval time =   36059.86 ms /   240 runs   (  150.25 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   60937.31 ms /  1649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.25 ms /   151 runs   (    0.10 ms per token,  9904.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6160.06 ms /   385 tokens (   16.00 ms per token,    62.50 tokens per second)\n",
      "llama_print_timings:        eval time =   10856.96 ms /   150 runs   (   72.38 ms per token,    13.82 tokens per second)\n",
      "llama_print_timings:       total time =   17220.99 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.21 ms /   135 runs   (    0.09 ms per token, 11052.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15065.70 ms /   988 tokens (   15.25 ms per token,    65.58 tokens per second)\n",
      "llama_print_timings:        eval time =   10578.72 ms /   134 runs   (   78.95 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:       total time =   25836.18 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      46.74 ms /   504 runs   (    0.09 ms per token, 10782.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4122.86 ms /   213 tokens (   19.36 ms per token,    51.66 tokens per second)\n",
      "llama_print_timings:        eval time =   36254.49 ms /   503 runs   (   72.08 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =   41128.69 ms /   716 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.49 ms /   349 runs   (    0.10 ms per token, 10422.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39643.02 ms /  2114 tokens (   18.75 ms per token,    53.33 tokens per second)\n",
      "llama_print_timings:        eval time =   39369.62 ms /   348 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:       total time =   79571.31 ms /  2462 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.68 ms /   219 runs   (    0.09 ms per token, 10590.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27341.79 ms /  1208 tokens (   22.63 ms per token,    44.18 tokens per second)\n",
      "llama_print_timings:        eval time =   27043.96 ms /   218 runs   (  124.05 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   54738.88 ms /  1426 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.74 ms /   134 runs   (    0.10 ms per token, 10517.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22223.92 ms /  1242 tokens (   17.89 ms per token,    55.89 tokens per second)\n",
      "llama_print_timings:        eval time =   10216.38 ms /   133 runs   (   76.81 ms per token,    13.02 tokens per second)\n",
      "llama_print_timings:       total time =   32638.06 ms /  1375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.80 ms /   162 runs   (    0.10 ms per token, 10253.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13122.73 ms /   825 tokens (   15.91 ms per token,    62.87 tokens per second)\n",
      "llama_print_timings:        eval time =   15895.93 ms /   161 runs   (   98.73 ms per token,    10.13 tokens per second)\n",
      "llama_print_timings:       total time =   29258.20 ms /   986 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.41 ms /   271 runs   (    0.09 ms per token, 10663.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22687.16 ms /  1200 tokens (   18.91 ms per token,    52.89 tokens per second)\n",
      "llama_print_timings:        eval time =   28940.83 ms /   270 runs   (  107.19 ms per token,     9.33 tokens per second)\n",
      "llama_print_timings:       total time =   52052.78 ms /  1470 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      35.22 ms /   395 runs   (    0.09 ms per token, 11215.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18808.09 ms /  1078 tokens (   17.45 ms per token,    57.32 tokens per second)\n",
      "llama_print_timings:        eval time =   41262.81 ms /   394 runs   (  104.73 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =   60688.52 ms /  1472 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.03 ms /   317 runs   (    0.09 ms per token, 10557.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14495.65 ms /   911 tokens (   15.91 ms per token,    62.85 tokens per second)\n",
      "llama_print_timings:        eval time =   26373.21 ms /   316 runs   (   83.46 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   41343.83 ms /  1227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.56 ms /   335 runs   (    0.10 ms per token,  9981.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11789.76 ms /   669 tokens (   17.62 ms per token,    56.74 tokens per second)\n",
      "llama_print_timings:        eval time =   30679.22 ms /   334 runs   (   91.85 ms per token,    10.89 tokens per second)\n",
      "llama_print_timings:       total time =   42990.15 ms /  1003 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.28 ms /   146 runs   (    0.10 ms per token,  9555.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6267.23 ms /   384 tokens (   16.32 ms per token,    61.27 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.32 ms /   145 runs   (   77.89 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =   17771.87 ms /   529 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.77 ms /   119 runs   (    0.10 ms per token, 10111.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23219.22 ms /  1310 tokens (   17.72 ms per token,    56.42 tokens per second)\n",
      "llama_print_timings:        eval time =   10699.97 ms /   118 runs   (   90.68 ms per token,    11.03 tokens per second)\n",
      "llama_print_timings:       total time =   34099.18 ms /  1428 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.76 ms /    81 runs   (    0.10 ms per token, 10435.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19165.32 ms /  1088 tokens (   17.62 ms per token,    56.77 tokens per second)\n",
      "llama_print_timings:        eval time =    8694.89 ms /    80 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_print_timings:       total time =   28035.66 ms /  1168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    66 runs   (    0.11 ms per token,  9443.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3120.70 ms /    68 tokens (   45.89 ms per token,    21.79 tokens per second)\n",
      "llama_print_timings:        eval time =    8490.47 ms /    65 runs   (  130.62 ms per token,     7.66 tokens per second)\n",
      "llama_print_timings:       total time =   11713.28 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.38 ms /   116 runs   (    0.10 ms per token, 10195.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6785.09 ms /    79 tokens (   85.89 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:        eval time =   14473.22 ms /   115 runs   (  125.85 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =   21431.28 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.84 ms /   101 runs   (    0.10 ms per token, 10260.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3234.03 ms /    68 tokens (   47.56 ms per token,    21.03 tokens per second)\n",
      "llama_print_timings:        eval time =  593595.01 ms /   100 runs   ( 5935.95 ms per token,     0.17 tokens per second)\n",
      "llama_print_timings:       total time =  597000.17 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.55 ms /   175 runs   (    0.09 ms per token, 10577.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11550.12 ms /   726 tokens (   15.91 ms per token,    62.86 tokens per second)\n",
      "llama_print_timings:        eval time =   13797.06 ms /   174 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =   25605.43 ms /   900 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.63 ms /   101 runs   (    0.11 ms per token,  9501.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5721.26 ms /   386 tokens (   14.82 ms per token,    67.47 tokens per second)\n",
      "llama_print_timings:        eval time =    7424.12 ms /   100 runs   (   74.24 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   13291.15 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.09 ms /   279 runs   (    0.09 ms per token, 10691.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52152.84 ms /  2914 tokens (   17.90 ms per token,    55.87 tokens per second)\n",
      "llama_print_timings:        eval time =   35452.09 ms /   278 runs   (  127.53 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:       total time =   88068.75 ms /  3192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      37.17 ms /   414 runs   (    0.09 ms per token, 11137.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25651.54 ms /  1544 tokens (   16.61 ms per token,    60.19 tokens per second)\n",
      "llama_print_timings:        eval time =   33905.22 ms /   413 runs   (   82.09 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =   60229.41 ms /  1957 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.06 ms /   412 runs   (    0.09 ms per token, 10548.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23854.21 ms /  1583 tokens (   15.07 ms per token,    66.36 tokens per second)\n",
      "llama_print_timings:        eval time =   51021.88 ms /   411 runs   (  124.14 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   75554.30 ms /  1994 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.70 ms /    35 runs   (    0.11 ms per token,  9469.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14121.80 ms /   702 tokens (   20.12 ms per token,    49.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3479.20 ms /    34 runs   (  102.33 ms per token,     9.77 tokens per second)\n",
      "llama_print_timings:       total time =   17662.80 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.50 ms /    53 runs   (    0.12 ms per token,  8150.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10474.47 ms /   456 tokens (   22.97 ms per token,    43.53 tokens per second)\n",
      "llama_print_timings:        eval time =   40910.65 ms /    52 runs   (  786.74 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time =   51490.49 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.43 ms /    35 runs   (    0.10 ms per token, 10195.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3087.07 ms /    29 tokens (  106.45 ms per token,     9.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4880.75 ms /    34 runs   (  143.55 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =    8032.21 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.26 ms /    69 runs   (    0.12 ms per token,  8353.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5169.09 ms /    97 tokens (   53.29 ms per token,    18.77 tokens per second)\n",
      "llama_print_timings:        eval time =   10608.91 ms /    68 runs   (  156.01 ms per token,     6.41 tokens per second)\n",
      "llama_print_timings:       total time =   15917.72 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.93 ms /   376 runs   (    0.10 ms per token,  9659.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17453.05 ms /   710 tokens (   24.58 ms per token,    40.68 tokens per second)\n",
      "llama_print_timings:        eval time =   53811.94 ms /   375 runs   (  143.50 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =   72110.95 ms /  1085 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.33 ms /   331 runs   (    0.10 ms per token, 10237.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33457.28 ms /  1740 tokens (   19.23 ms per token,    52.01 tokens per second)\n",
      "llama_print_timings:        eval time =   39476.11 ms /   330 runs   (  119.62 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:       total time =   73625.52 ms /  2070 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       2.60 ms /    29 runs   (    0.09 ms per token, 11140.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9985.17 ms /   634 tokens (   15.75 ms per token,    63.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1935.67 ms /    28 runs   (   69.13 ms per token,    14.47 tokens per second)\n",
      "llama_print_timings:       total time =   11998.21 ms /   662 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       2.06 ms /    20 runs   (    0.10 ms per token,  9699.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4420.61 ms /   203 tokens (   21.78 ms per token,    45.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1543.65 ms /    19 runs   (   81.24 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    6015.73 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.79 ms /   146 runs   (    0.10 ms per token,  9870.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4506.74 ms /   278 tokens (   16.21 ms per token,    61.69 tokens per second)\n",
      "llama_print_timings:        eval time =   11723.48 ms /   145 runs   (   80.85 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   16572.87 ms /   423 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      47.22 ms /   512 runs   (    0.09 ms per token, 10841.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18094.20 ms /  1228 tokens (   14.73 ms per token,    67.87 tokens per second)\n",
      "llama_print_timings:        eval time =   45509.67 ms /   511 runs   (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   64769.63 ms /  1739 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.68 ms /   248 runs   (    0.10 ms per token, 10471.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27445.70 ms /  1482 tokens (   18.52 ms per token,    54.00 tokens per second)\n",
      "llama_print_timings:        eval time =   32116.52 ms /   247 runs   (  130.03 ms per token,     7.69 tokens per second)\n",
      "llama_print_timings:       total time =   59961.75 ms /  1729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.88 ms /   440 runs   (    0.09 ms per token, 11031.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19151.06 ms /  1367 tokens (   14.01 ms per token,    71.38 tokens per second)\n",
      "llama_print_timings:        eval time =   33409.01 ms /   439 runs   (   76.10 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:       total time =   53231.34 ms /  1806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.38 ms /   334 runs   (    0.09 ms per token, 10644.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10019.55 ms /   576 tokens (   17.40 ms per token,    57.49 tokens per second)\n",
      "llama_print_timings:        eval time =   24047.53 ms /   333 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =   34684.69 ms /   909 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.93 ms /   130 runs   (    0.08 ms per token, 11892.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9275.15 ms /   630 tokens (   14.72 ms per token,    67.92 tokens per second)\n",
      "llama_print_timings:        eval time =    9968.51 ms /   129 runs   (   77.28 ms per token,    12.94 tokens per second)\n",
      "llama_print_timings:       total time =   19435.32 ms /   759 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.12 ms /   213 runs   (    0.09 ms per token, 11139.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18523.35 ms /  1328 tokens (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_print_timings:        eval time =   17702.53 ms /   212 runs   (   83.50 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   36555.41 ms /  1540 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.84 ms /   219 runs   (    0.10 ms per token, 10506.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19610.76 ms /  1370 tokens (   14.31 ms per token,    69.86 tokens per second)\n",
      "llama_print_timings:        eval time =   16668.17 ms /   218 runs   (   76.46 ms per token,    13.08 tokens per second)\n",
      "llama_print_timings:       total time =   36601.46 ms /  1588 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.31 ms /   176 runs   (    0.09 ms per token, 11495.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14592.90 ms /  1035 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_print_timings:        eval time =   12427.72 ms /   175 runs   (   71.02 ms per token,    14.08 tokens per second)\n",
      "llama_print_timings:       total time =   27263.95 ms /  1210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.55 ms /   179 runs   (    0.09 ms per token, 11513.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41556.85 ms /  2961 tokens (   14.03 ms per token,    71.25 tokens per second)\n",
      "llama_print_timings:        eval time =   14130.35 ms /   178 runs   (   79.38 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   56133.36 ms /  3139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.98 ms /   182 runs   (    0.09 ms per token, 10715.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12444.76 ms /   939 tokens (   13.25 ms per token,    75.45 tokens per second)\n",
      "llama_print_timings:        eval time =   12757.23 ms /   181 runs   (   70.48 ms per token,    14.19 tokens per second)\n",
      "llama_print_timings:       total time =   25456.07 ms /  1120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.10 ms /   234 runs   (    0.10 ms per token, 10131.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5057.93 ms /   384 tokens (   13.17 ms per token,    75.92 tokens per second)\n",
      "llama_print_timings:        eval time =   15862.23 ms /   233 runs   (   68.08 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =   21340.17 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    73 runs   (    0.10 ms per token, 10446.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38053.07 ms /  2598 tokens (   14.65 ms per token,    68.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5600.01 ms /    72 runs   (   77.78 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   43758.60 ms /  2670 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.79 ms /   234 runs   (    0.10 ms per token, 10267.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5075.33 ms /   384 tokens (   13.22 ms per token,    75.66 tokens per second)\n",
      "llama_print_timings:        eval time =   15849.54 ms /   233 runs   (   68.02 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =   21248.82 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   149 runs   (    0.09 ms per token, 11049.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30518.17 ms /  2208 tokens (   13.82 ms per token,    72.35 tokens per second)\n",
      "llama_print_timings:        eval time =   11481.92 ms /   148 runs   (   77.58 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:       total time =   42413.93 ms /  2356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.04 ms /   141 runs   (    0.09 ms per token, 10817.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5368.09 ms /   376 tokens (   14.28 ms per token,    70.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9563.29 ms /   140 runs   (   68.31 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   15123.16 ms /   516 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.78 ms /   424 runs   (    0.09 ms per token, 10657.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17017.74 ms /  1241 tokens (   13.71 ms per token,    72.92 tokens per second)\n",
      "llama_print_timings:        eval time =   30623.61 ms /   423 runs   (   72.40 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:       total time =   48268.33 ms /  1664 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.66 ms /    62 runs   (    0.09 ms per token, 10957.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1750.03 ms /    25 tokens (   70.00 ms per token,    14.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4417.07 ms /    61 runs   (   72.41 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:       total time =    6250.94 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.32 ms /   105 runs   (    0.09 ms per token, 11268.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3526.85 ms /   203 tokens (   17.37 ms per token,    57.56 tokens per second)\n",
      "llama_print_timings:        eval time =    7323.60 ms /   104 runs   (   70.42 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:       total time =   10995.12 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.33 ms /   368 runs   (    0.09 ms per token, 11381.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31141.25 ms /  2152 tokens (   14.47 ms per token,    69.10 tokens per second)\n",
      "llama_print_timings:        eval time =   28133.31 ms /   367 runs   (   76.66 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:       total time =   59946.90 ms /  2519 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.02 ms /    91 runs   (    0.09 ms per token, 11348.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9346.53 ms /   625 tokens (   14.95 ms per token,    66.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6225.33 ms /    90 runs   (   69.17 ms per token,    14.46 tokens per second)\n",
      "llama_print_timings:       total time =   15696.79 ms /   715 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.47 ms /   268 runs   (    0.08 ms per token, 11929.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38868.73 ms /  2753 tokens (   14.12 ms per token,    70.83 tokens per second)\n",
      "llama_print_timings:        eval time =   21053.89 ms /   267 runs   (   78.85 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =   60438.22 ms /  3020 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.45 ms /   512 runs   (    0.09 ms per token, 11265.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16063.33 ms /  1079 tokens (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:        eval time =   37008.38 ms /   511 runs   (   72.42 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:       total time =   53832.08 ms /  1590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.82 ms /   175 runs   (    0.08 ms per token, 11806.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39345.88 ms /  2747 tokens (   14.32 ms per token,    69.82 tokens per second)\n",
      "llama_print_timings:        eval time =   13685.77 ms /   174 runs   (   78.65 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =   53285.73 ms /  2921 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.62 ms /   234 runs   (    0.10 ms per token,  9906.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5377.69 ms /   384 tokens (   14.00 ms per token,    71.41 tokens per second)\n",
      "llama_print_timings:        eval time =   15973.03 ms /   233 runs   (   68.55 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =   21681.55 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.66 ms /   284 runs   (    0.09 ms per token, 11516.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1083130.80 ms /  2531 tokens (  427.95 ms per token,     2.34 tokens per second)\n",
      "llama_print_timings:        eval time =   22418.41 ms /   283 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time = 1105965.35 ms /  2814 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.30 ms /   351 runs   (    0.09 ms per token, 10867.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4747.13 ms /   318 tokens (   14.93 ms per token,    66.99 tokens per second)\n",
      "llama_print_timings:        eval time =  247305.14 ms /   350 runs   (  706.59 ms per token,     1.42 tokens per second)\n",
      "llama_print_timings:       total time =  252591.29 ms /   668 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.89 ms /   106 runs   (    0.09 ms per token, 10721.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11223.92 ms /   714 tokens (   15.72 ms per token,    63.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7248.59 ms /   105 runs   (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   18619.29 ms /   819 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.83 ms /   241 runs   (    0.14 ms per token,  6920.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11254.22 ms /   646 tokens (   17.42 ms per token,    57.40 tokens per second)\n",
      "llama_print_timings:        eval time =   23776.91 ms /   240 runs   (   99.07 ms per token,    10.09 tokens per second)\n",
      "llama_print_timings:       total time =   35536.86 ms /   886 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.46 ms /   114 runs   (    0.09 ms per token, 10901.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12698.28 ms /   725 tokens (   17.51 ms per token,    57.09 tokens per second)\n",
      "llama_print_timings:        eval time =    7918.89 ms /   113 runs   (   70.08 ms per token,    14.27 tokens per second)\n",
      "llama_print_timings:       total time =   20772.43 ms /   838 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    56 runs   (    0.10 ms per token,  9569.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5541.90 ms /   424 tokens (   13.07 ms per token,    76.51 tokens per second)\n",
      "llama_print_timings:        eval time =  356274.88 ms /    55 runs   ( 6477.73 ms per token,     0.15 tokens per second)\n",
      "llama_print_timings:       total time =  361917.92 ms /   479 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.96 ms /   470 runs   (    0.10 ms per token, 10225.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10912.18 ms /   517 tokens (   21.11 ms per token,    47.38 tokens per second)\n",
      "llama_print_timings:        eval time =   54903.32 ms /   469 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_print_timings:       total time =   66605.51 ms /   986 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.91 ms /   124 runs   (    0.12 ms per token,  8316.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20333.42 ms /   872 tokens (   23.32 ms per token,    42.89 tokens per second)\n",
      "llama_print_timings:        eval time =   41196.53 ms /   123 runs   (  334.93 ms per token,     2.99 tokens per second)\n",
      "llama_print_timings:       total time =   61770.20 ms /   995 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      29.27 ms /   259 runs   (    0.11 ms per token,  8848.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15456.51 ms /   799 tokens (   19.34 ms per token,    51.69 tokens per second)\n",
      "llama_print_timings:        eval time =   68261.00 ms /   258 runs   (  264.58 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:       total time =   84179.07 ms /  1057 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.76 ms /    67 runs   (    0.10 ms per token,  9905.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5062.74 ms /   314 tokens (   16.12 ms per token,    62.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5145.88 ms /    66 runs   (   77.97 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   10313.39 ms /   380 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.01 ms /   254 runs   (    0.09 ms per token, 11040.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14165.59 ms /   937 tokens (   15.12 ms per token,    66.15 tokens per second)\n",
      "llama_print_timings:        eval time =   17990.52 ms /   253 runs   (   71.11 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =   32529.12 ms /  1190 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "token_splitter_512\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:28<00:00,  1.38it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.33 ms /   273 runs   (    0.11 ms per token,  8713.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39878.79 ms /   780 tokens (   51.13 ms per token,    19.56 tokens per second)\n",
      "llama_print_timings:        eval time =   51463.24 ms /   272 runs   (  189.20 ms per token,     5.29 tokens per second)\n",
      "llama_print_timings:       total time =   92134.24 ms /  1052 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.64 ms /   439 runs   (    0.10 ms per token, 10296.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12590.05 ms /   498 tokens (   25.28 ms per token,    39.56 tokens per second)\n",
      "llama_print_timings:        eval time =  105837.68 ms /   438 runs   (  241.64 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:       total time =  119491.41 ms /   936 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    38 runs   (    0.11 ms per token,  9019.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10783.66 ms /   378 tokens (   28.53 ms per token,    35.05 tokens per second)\n",
      "llama_print_timings:        eval time =    9888.74 ms /    37 runs   (  267.26 ms per token,     3.74 tokens per second)\n",
      "llama_print_timings:       total time =   20773.24 ms /   415 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    65 runs   (    0.11 ms per token,  9188.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8504.89 ms /   501 tokens (   16.98 ms per token,    58.91 tokens per second)\n",
      "llama_print_timings:        eval time =   15122.42 ms /    64 runs   (  236.29 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:       total time =   23781.70 ms /   565 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.48 ms /   336 runs   (    0.09 ms per token, 10673.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16918.70 ms /   868 tokens (   19.49 ms per token,    51.30 tokens per second)\n",
      "llama_print_timings:        eval time =   54163.59 ms /   335 runs   (  161.68 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:       total time =   71941.11 ms /  1203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.93 ms /   151 runs   (    0.10 ms per token, 10117.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17076.76 ms /   843 tokens (   20.26 ms per token,    49.37 tokens per second)\n",
      "llama_print_timings:        eval time =   29804.28 ms /   150 runs   (  198.70 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   47184.26 ms /   993 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.09 ms /   161 runs   (    0.10 ms per token, 10003.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22843.83 ms /   729 tokens (   31.34 ms per token,    31.91 tokens per second)\n",
      "llama_print_timings:        eval time =   24236.57 ms /   160 runs   (  151.48 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:       total time =   47466.87 ms /   889 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.61 ms /   135 runs   (    0.11 ms per token,  9240.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9590.85 ms /   373 tokens (   25.71 ms per token,    38.89 tokens per second)\n",
      "llama_print_timings:        eval time =   17048.72 ms /   134 runs   (  127.23 ms per token,     7.86 tokens per second)\n",
      "llama_print_timings:       total time =   26914.97 ms /   507 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.00 ms /   220 runs   (    0.12 ms per token,  8149.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16312.24 ms /   704 tokens (   23.17 ms per token,    43.16 tokens per second)\n",
      "llama_print_timings:        eval time =   58216.55 ms /   219 runs   (  265.83 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:       total time =   75008.29 ms /   923 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.12 ms /   190 runs   (    0.10 ms per token,  9937.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6990.33 ms /   373 tokens (   18.74 ms per token,    53.36 tokens per second)\n",
      "llama_print_timings:        eval time =   21884.56 ms /   189 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:       total time =   29182.20 ms /   562 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      53.13 ms /   512 runs   (    0.10 ms per token,  9636.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24421.80 ms /  1209 tokens (   20.20 ms per token,    49.50 tokens per second)\n",
      "llama_print_timings:        eval time =   91491.80 ms /   511 runs   (  179.04 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =  117134.44 ms /  1720 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.11 ms /   135 runs   (    0.13 ms per token,  7891.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12920.66 ms /   559 tokens (   23.11 ms per token,    43.26 tokens per second)\n",
      "llama_print_timings:        eval time =   22726.80 ms /   134 runs   (  169.60 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =   35985.09 ms /   693 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.68 ms /   452 runs   (    0.10 ms per token,  9895.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18421.96 ms /   591 tokens (   31.17 ms per token,    32.08 tokens per second)\n",
      "llama_print_timings:        eval time =   64053.63 ms /   451 runs   (  142.03 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   83326.03 ms /  1042 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   135 runs   (    0.10 ms per token,  9691.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6837.99 ms /   373 tokens (   18.33 ms per token,    54.55 tokens per second)\n",
      "llama_print_timings:        eval time =   16244.77 ms /   134 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   23317.90 ms /   507 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.70 ms /   235 runs   (    0.11 ms per token,  8802.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16063.16 ms /   972 tokens (   16.53 ms per token,    60.51 tokens per second)\n",
      "llama_print_timings:        eval time =   45101.01 ms /   234 runs   (  192.74 ms per token,     5.19 tokens per second)\n",
      "llama_print_timings:       total time =   61595.23 ms /  1206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.07 ms /   135 runs   (    0.10 ms per token,  9592.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12737.32 ms /   550 tokens (   23.16 ms per token,    43.18 tokens per second)\n",
      "llama_print_timings:        eval time =   16942.62 ms /   134 runs   (  126.44 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:       total time =   29911.66 ms /   684 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      46.27 ms /   452 runs   (    0.10 ms per token,  9767.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14063.36 ms /   591 tokens (   23.80 ms per token,    42.02 tokens per second)\n",
      "llama_print_timings:        eval time =   59566.33 ms /   451 runs   (  132.08 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:       total time =   74449.26 ms /  1042 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.46 ms /   135 runs   (    0.11 ms per token,  9338.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7842.96 ms /   373 tokens (   21.03 ms per token,    47.56 tokens per second)\n",
      "llama_print_timings:        eval time =   16525.51 ms /   134 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =   24590.36 ms /   507 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.91 ms /   235 runs   (    0.10 ms per token,  9829.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16654.75 ms /   972 tokens (   17.13 ms per token,    58.36 tokens per second)\n",
      "llama_print_timings:        eval time =   28629.54 ms /   234 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   45682.47 ms /  1206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      35.26 ms /   359 runs   (    0.10 ms per token, 10182.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22582.65 ms /  1209 tokens (   18.68 ms per token,    53.54 tokens per second)\n",
      "llama_print_timings:        eval time =   52240.97 ms /   358 runs   (  145.92 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:       total time =   75484.38 ms /  1567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    47 runs   (    0.12 ms per token,  8462.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16111.99 ms /   722 tokens (   22.32 ms per token,    44.81 tokens per second)\n",
      "llama_print_timings:        eval time =    6079.49 ms /    46 runs   (  132.16 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:       total time =   22277.42 ms /   768 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.97 ms /   250 runs   (    0.10 ms per token,  9625.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7966.88 ms /   392 tokens (   20.32 ms per token,    49.20 tokens per second)\n",
      "llama_print_timings:        eval time =   33248.35 ms /   249 runs   (  133.53 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:       total time =   41651.81 ms /   641 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.11 ms /   181 runs   (    0.11 ms per token,  9002.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11573.26 ms /   487 tokens (   23.76 ms per token,    42.08 tokens per second)\n",
      "llama_print_timings:        eval time =   29783.10 ms /   180 runs   (  165.46 ms per token,     6.04 tokens per second)\n",
      "llama_print_timings:       total time =   41693.32 ms /   667 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.04 ms /   121 runs   (    0.11 ms per token,  9281.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8923.90 ms /   373 tokens (   23.92 ms per token,    41.80 tokens per second)\n",
      "llama_print_timings:        eval time =   14854.48 ms /   120 runs   (  123.79 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   23986.77 ms /   493 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.64 ms /   136 runs   (    0.11 ms per token,  9288.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6318.29 ms /   202 tokens (   31.28 ms per token,    31.97 tokens per second)\n",
      "llama_print_timings:        eval time =   17255.18 ms /   135 runs   (  127.82 ms per token,     7.82 tokens per second)\n",
      "llama_print_timings:       total time =   23808.78 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.22 ms /   229 runs   (    0.12 ms per token,  8412.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.08 ms /   372 tokens (   23.33 ms per token,    42.87 tokens per second)\n",
      "llama_print_timings:        eval time =   55345.24 ms /   228 runs   (  242.74 ms per token,     4.12 tokens per second)\n",
      "llama_print_timings:       total time =   64495.29 ms /   600 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.63 ms /   102 runs   (    0.15 ms per token,  6526.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8083.56 ms /   377 tokens (   21.44 ms per token,    46.64 tokens per second)\n",
      "llama_print_timings:        eval time =   53035.51 ms /   101 runs   (  525.10 ms per token,     1.90 tokens per second)\n",
      "llama_print_timings:       total time =   61344.76 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    47 runs   (    0.16 ms per token,  6267.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8009.94 ms /   370 tokens (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:        eval time =   37378.27 ms /    46 runs   (  812.57 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time =   45497.53 ms /   416 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.23 ms /   373 runs   (    0.11 ms per token,  8832.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10997.44 ms /   357 tokens (   30.81 ms per token,    32.46 tokens per second)\n",
      "llama_print_timings:        eval time =   66381.70 ms /   372 runs   (  178.45 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   78202.87 ms /   729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    58 runs   (    0.11 ms per token,  8906.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8816.48 ms /   387 tokens (   22.78 ms per token,    43.90 tokens per second)\n",
      "llama_print_timings:        eval time =    7228.26 ms /    57 runs   (  126.81 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:       total time =   16143.62 ms /   444 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.54 ms /    55 runs   (    0.16 ms per token,  6443.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20492.52 ms /   710 tokens (   28.86 ms per token,    34.65 tokens per second)\n",
      "llama_print_timings:        eval time =   13292.24 ms /    54 runs   (  246.15 ms per token,     4.06 tokens per second)\n",
      "llama_print_timings:       total time =   33928.09 ms /   764 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.00 ms /   189 runs   (    0.14 ms per token,  6998.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10450.82 ms /   364 tokens (   28.71 ms per token,    34.83 tokens per second)\n",
      "llama_print_timings:        eval time =   48035.20 ms /   188 runs   (  255.51 ms per token,     3.91 tokens per second)\n",
      "llama_print_timings:       total time =   58960.70 ms /   552 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.73 ms /   144 runs   (    0.10 ms per token,  9775.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8944.22 ms /   398 tokens (   22.47 ms per token,    44.50 tokens per second)\n",
      "llama_print_timings:        eval time =   16563.60 ms /   143 runs   (  115.83 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:       total time =   25745.20 ms /   541 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    33 runs   (    0.12 ms per token,  8196.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8430.96 ms /   359 tokens (   23.48 ms per token,    42.58 tokens per second)\n",
      "llama_print_timings:        eval time =    6509.16 ms /    32 runs   (  203.41 ms per token,     4.92 tokens per second)\n",
      "llama_print_timings:       total time =   15010.42 ms /   391 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.82 ms /   382 runs   (    0.11 ms per token,  8921.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8105.71 ms /   386 tokens (   21.00 ms per token,    47.62 tokens per second)\n",
      "llama_print_timings:        eval time =   75078.69 ms /   381 runs   (  197.06 ms per token,     5.07 tokens per second)\n",
      "llama_print_timings:       total time =   83936.27 ms /   767 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.32 ms /   373 runs   (    0.12 ms per token,  8229.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8597.90 ms /   357 tokens (   24.08 ms per token,    41.52 tokens per second)\n",
      "llama_print_timings:        eval time =   71014.43 ms /   372 runs   (  190.90 ms per token,     5.24 tokens per second)\n",
      "llama_print_timings:       total time =   80366.93 ms /   729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    34 runs   (    0.15 ms per token,  6838.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10290.93 ms /   392 tokens (   26.25 ms per token,    38.09 tokens per second)\n",
      "llama_print_timings:        eval time =    7131.05 ms /    33 runs   (  216.09 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =   17491.80 ms /   425 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.30 ms /   118 runs   (    0.11 ms per token,  8873.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8228.72 ms /   361 tokens (   22.79 ms per token,    43.87 tokens per second)\n",
      "llama_print_timings:        eval time =   16421.51 ms /   117 runs   (  140.35 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:       total time =   24859.45 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.81 ms /    99 runs   (    0.12 ms per token,  8380.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7420.02 ms /   373 tokens (   19.89 ms per token,    50.27 tokens per second)\n",
      "llama_print_timings:        eval time =   13962.76 ms /    98 runs   (  142.48 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   21553.14 ms /   471 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.71 ms /   300 runs   (    0.13 ms per token,  7750.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12069.05 ms /   368 tokens (   32.80 ms per token,    30.49 tokens per second)\n",
      "llama_print_timings:        eval time =   57724.16 ms /   299 runs   (  193.06 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time =   70421.66 ms /   667 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.99 ms /    34 runs   (    0.12 ms per token,  8527.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7882.68 ms /   384 tokens (   20.53 ms per token,    48.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3756.66 ms /    33 runs   (  113.84 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:       total time =   11700.43 ms /   417 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    44 runs   (    0.12 ms per token,  8479.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7757.48 ms /   367 tokens (   21.14 ms per token,    47.31 tokens per second)\n",
      "llama_print_timings:        eval time =    6508.62 ms /    43 runs   (  151.36 ms per token,     6.61 tokens per second)\n",
      "llama_print_timings:       total time =   14353.13 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.84 ms /    90 runs   (    0.13 ms per token,  7601.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9140.79 ms /   384 tokens (   23.80 ms per token,    42.01 tokens per second)\n",
      "llama_print_timings:        eval time =   21173.45 ms /    89 runs   (  237.90 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:       total time =   30501.03 ms /   473 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    54 runs   (    0.11 ms per token,  8730.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7998.94 ms /   350 tokens (   22.85 ms per token,    43.76 tokens per second)\n",
      "llama_print_timings:        eval time =    7956.21 ms /    53 runs   (  150.12 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   16053.49 ms /   403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.13 ms /   102 runs   (    0.14 ms per token,  7218.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7784.49 ms /   177 tokens (   43.98 ms per token,    22.74 tokens per second)\n",
      "llama_print_timings:        eval time =   24170.20 ms /   101 runs   (  239.31 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =   32178.31 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.23 ms /   175 runs   (    0.12 ms per token,  8651.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6272.16 ms /   155 tokens (   40.47 ms per token,    24.71 tokens per second)\n",
      "llama_print_timings:        eval time =   27784.08 ms /   174 runs   (  159.68 ms per token,     6.26 tokens per second)\n",
      "llama_print_timings:       total time =   34400.08 ms /   329 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      36.80 ms /   311 runs   (    0.12 ms per token,  8450.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.79 ms /   387 tokens (   22.43 ms per token,    44.59 tokens per second)\n",
      "llama_print_timings:        eval time =   50118.38 ms /   310 runs   (  161.67 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =   59386.61 ms /   697 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.62 ms /    44 runs   (    0.13 ms per token,  7827.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5889.09 ms /   173 tokens (   34.04 ms per token,    29.38 tokens per second)\n",
      "llama_print_timings:        eval time =    7584.28 ms /    43 runs   (  176.38 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   13551.33 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.03 ms /   102 runs   (    0.12 ms per token,  8478.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8611.01 ms /   356 tokens (   24.19 ms per token,    41.34 tokens per second)\n",
      "llama_print_timings:        eval time =   13597.93 ms /   101 runs   (  134.63 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:       total time =   22375.53 ms /   457 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.26 ms /   120 runs   (    0.13 ms per token,  7864.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8514.85 ms /   361 tokens (   23.59 ms per token,    42.40 tokens per second)\n",
      "llama_print_timings:        eval time =   19744.68 ms /   119 runs   (  165.92 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:       total time =   28487.65 ms /   480 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.45 ms /    46 runs   (    0.16 ms per token,  6175.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7749.68 ms /   344 tokens (   22.53 ms per token,    44.39 tokens per second)\n",
      "llama_print_timings:        eval time =   10512.04 ms /    45 runs   (  233.60 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:       total time =   18353.02 ms /   389 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /    60 runs   (    0.13 ms per token,  7476.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6260.36 ms /   180 tokens (   34.78 ms per token,    28.75 tokens per second)\n",
      "llama_print_timings:        eval time =   10216.12 ms /    59 runs   (  173.15 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   16588.29 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.31 ms /   181 runs   (    0.13 ms per token,  7764.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8809.44 ms /   348 tokens (   25.31 ms per token,    39.50 tokens per second)\n",
      "llama_print_timings:        eval time =   32426.82 ms /   180 runs   (  180.15 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   41582.69 ms /   528 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    44 runs   (    0.13 ms per token,  7844.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7769.01 ms /   366 tokens (   21.23 ms per token,    47.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9229.60 ms /    43 runs   (  214.64 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:       total time =   17085.55 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    57 runs   (    0.10 ms per token,  9868.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8832.49 ms /   413 tokens (   21.39 ms per token,    46.76 tokens per second)\n",
      "llama_print_timings:        eval time =    8128.91 ms /    56 runs   (  145.16 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:       total time =   17060.90 ms /   469 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.37 ms /   347 runs   (    0.11 ms per token,  9043.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6654.05 ms /   193 tokens (   34.48 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:        eval time =   55663.05 ms /   346 runs   (  160.88 ms per token,     6.22 tokens per second)\n",
      "llama_print_timings:       total time =   62997.42 ms /   539 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.26 ms /   147 runs   (    0.12 ms per token,  8515.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9045.80 ms /   360 tokens (   25.13 ms per token,    39.80 tokens per second)\n",
      "llama_print_timings:        eval time =   21861.41 ms /   146 runs   (  149.74 ms per token,     6.68 tokens per second)\n",
      "llama_print_timings:       total time =   31168.79 ms /   506 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.56 ms /   138 runs   (    0.12 ms per token,  8333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8594.91 ms /   397 tokens (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:        eval time =   23384.92 ms /   137 runs   (  170.69 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   32225.76 ms /   534 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.18 ms /   177 runs   (    0.12 ms per token,  8355.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7888.22 ms /   371 tokens (   21.26 ms per token,    47.03 tokens per second)\n",
      "llama_print_timings:        eval time =   27203.53 ms /   176 runs   (  154.57 ms per token,     6.47 tokens per second)\n",
      "llama_print_timings:       total time =   35407.88 ms /   547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.57 ms /    52 runs   (    0.15 ms per token,  6872.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8285.77 ms /   367 tokens (   22.58 ms per token,    44.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9955.26 ms /    51 runs   (  195.20 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:       total time =   18338.64 ms /   418 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.10 ms /   199 runs   (    0.12 ms per token,  8257.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11327.78 ms /   420 tokens (   26.97 ms per token,    37.08 tokens per second)\n",
      "llama_print_timings:        eval time =   33813.96 ms /   198 runs   (  170.78 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   45511.23 ms /   618 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   112 runs   (    0.12 ms per token,  8311.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14801.90 ms /   730 tokens (   20.28 ms per token,    49.32 tokens per second)\n",
      "llama_print_timings:        eval time =   19991.27 ms /   111 runs   (  180.10 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   35053.92 ms /   841 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    44 runs   (    0.15 ms per token,  6864.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8028.61 ms /   367 tokens (   21.88 ms per token,    45.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9428.81 ms /    43 runs   (  219.27 ms per token,     4.56 tokens per second)\n",
      "llama_print_timings:       total time =   17556.01 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.16 ms /   130 runs   (    0.12 ms per token,  8045.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8794.62 ms /   397 tokens (   22.15 ms per token,    45.14 tokens per second)\n",
      "llama_print_timings:        eval time =   30969.39 ms /   129 runs   (  240.07 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:       total time =   40030.97 ms /   526 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.81 ms /    45 runs   (    0.15 ms per token,  6607.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8198.25 ms /   366 tokens (   22.40 ms per token,    44.64 tokens per second)\n",
      "llama_print_timings:        eval time =    8835.33 ms /    44 runs   (  200.80 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:       total time =   17127.83 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.86 ms /    96 runs   (    0.11 ms per token,  8843.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9701.70 ms /   368 tokens (   26.36 ms per token,    37.93 tokens per second)\n",
      "llama_print_timings:        eval time =   16252.32 ms /    95 runs   (  171.08 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   26145.03 ms /   463 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.49 ms /   136 runs   (    0.11 ms per token,  8777.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9023.40 ms /   355 tokens (   25.42 ms per token,    39.34 tokens per second)\n",
      "llama_print_timings:        eval time =   24007.90 ms /   135 runs   (  177.84 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   33288.25 ms /   490 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.26 ms /   256 runs   (    0.13 ms per token,  7697.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7819.83 ms /   382 tokens (   20.47 ms per token,    48.85 tokens per second)\n",
      "llama_print_timings:        eval time =   53218.38 ms /   255 runs   (  208.70 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   61593.24 ms /   637 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.02 ms /    96 runs   (    0.11 ms per token,  8710.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5251.71 ms /   174 tokens (   30.18 ms per token,    33.13 tokens per second)\n",
      "llama_print_timings:        eval time =   22217.34 ms /    95 runs   (  233.87 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:       total time =   27658.56 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.51 ms /   167 runs   (    0.11 ms per token,  9023.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9141.63 ms /   388 tokens (   23.56 ms per token,    42.44 tokens per second)\n",
      "llama_print_timings:        eval time =   43325.48 ms /   166 runs   (  261.00 ms per token,     3.83 tokens per second)\n",
      "llama_print_timings:       total time =   52765.22 ms /   554 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.92 ms /   134 runs   (    0.10 ms per token, 10373.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5453.44 ms /   356 tokens (   15.32 ms per token,    65.28 tokens per second)\n",
      "llama_print_timings:        eval time =   10060.66 ms /   133 runs   (   75.64 ms per token,    13.22 tokens per second)\n",
      "llama_print_timings:       total time =   15730.26 ms /   489 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.99 ms /   105 runs   (    0.10 ms per token,  9557.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5694.64 ms /   356 tokens (   16.00 ms per token,    62.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9309.81 ms /   104 runs   (   89.52 ms per token,    11.17 tokens per second)\n",
      "llama_print_timings:       total time =   15163.88 ms /   460 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.80 ms /   134 runs   (    0.10 ms per token, 10470.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5361.12 ms /   339 tokens (   15.81 ms per token,    63.23 tokens per second)\n",
      "llama_print_timings:        eval time =    9586.83 ms /   133 runs   (   72.08 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =   15162.53 ms /   472 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.01 ms /   125 runs   (    0.09 ms per token, 11355.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5259.14 ms /   343 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_print_timings:        eval time =    8454.62 ms /   124 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =   13893.10 ms /   467 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.47 ms /   171 runs   (    0.08 ms per token, 11820.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5872.74 ms /   397 tokens (   14.79 ms per token,    67.60 tokens per second)\n",
      "llama_print_timings:        eval time =   11710.73 ms /   170 runs   (   68.89 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =   17824.86 ms /   567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.58 ms /   115 runs   (    0.08 ms per token, 12005.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5323.02 ms /   384 tokens (   13.86 ms per token,    72.14 tokens per second)\n",
      "llama_print_timings:        eval time =    8467.54 ms /   114 runs   (   74.28 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =   13957.07 ms /   498 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.08 ms /   120 runs   (    0.09 ms per token, 10829.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6122.77 ms /   424 tokens (   14.44 ms per token,    69.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9302.22 ms /   119 runs   (   78.17 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =   15604.24 ms /   543 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.49 ms /    96 runs   (    0.09 ms per token, 11308.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5470.27 ms /   368 tokens (   14.86 ms per token,    67.27 tokens per second)\n",
      "llama_print_timings:        eval time =    6957.16 ms /    95 runs   (   73.23 ms per token,    13.65 tokens per second)\n",
      "llama_print_timings:       total time =   12564.87 ms /   463 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    49 runs   (    0.09 ms per token, 10539.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5549.44 ms /   380 tokens (   14.60 ms per token,    68.48 tokens per second)\n",
      "llama_print_timings:        eval time =    3699.76 ms /    48 runs   (   77.08 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:       total time =    9320.84 ms /   428 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.68 ms /   102 runs   (    0.09 ms per token, 10533.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3539.76 ms /   169 tokens (   20.95 ms per token,    47.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6913.53 ms /   101 runs   (   68.45 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =   10592.81 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.94 ms /   125 runs   (    0.10 ms per token, 10467.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5027.17 ms /   359 tokens (   14.00 ms per token,    71.41 tokens per second)\n",
      "llama_print_timings:        eval time =   13262.62 ms /   124 runs   (  106.96 ms per token,     9.35 tokens per second)\n",
      "llama_print_timings:       total time =   18490.18 ms /   483 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.63 ms /    83 runs   (    0.10 ms per token,  9614.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8046.71 ms /   358 tokens (   22.48 ms per token,    44.49 tokens per second)\n",
      "llama_print_timings:        eval time =    7219.77 ms /    82 runs   (   88.05 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   15404.93 ms /   440 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.84 ms /    41 runs   (    0.09 ms per token, 10668.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4485.84 ms /   170 tokens (   26.39 ms per token,    37.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3153.61 ms /    40 runs   (   78.84 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =    7701.71 ms /   210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.04 ms /   230 runs   (    0.10 ms per token, 10437.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5254.29 ms /   180 tokens (   29.19 ms per token,    34.26 tokens per second)\n",
      "llama_print_timings:        eval time =   25194.78 ms /   229 runs   (  110.02 ms per token,     9.09 tokens per second)\n",
      "llama_print_timings:       total time =   30832.14 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.91 ms /    44 runs   (    0.09 ms per token, 11258.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8490.84 ms /   367 tokens (   23.14 ms per token,    43.22 tokens per second)\n",
      "llama_print_timings:        eval time =    3205.86 ms /    43 runs   (   74.55 ms per token,    13.41 tokens per second)\n",
      "llama_print_timings:       total time =   11762.89 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.34 ms /    92 runs   (    0.09 ms per token, 11027.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6727.35 ms /   404 tokens (   16.65 ms per token,    60.05 tokens per second)\n",
      "llama_print_timings:        eval time =    7725.23 ms /    91 runs   (   84.89 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =   14593.49 ms /   495 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.47 ms /    94 runs   (    0.09 ms per token, 11100.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10232.82 ms /   694 tokens (   14.74 ms per token,    67.82 tokens per second)\n",
      "llama_print_timings:        eval time =    6541.97 ms /    93 runs   (   70.34 ms per token,    14.22 tokens per second)\n",
      "llama_print_timings:       total time =   16909.56 ms /   787 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.39 ms /   109 runs   (    0.09 ms per token, 11609.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13492.43 ms /   983 tokens (   13.73 ms per token,    72.86 tokens per second)\n",
      "llama_print_timings:        eval time =    7744.35 ms /   108 runs   (   71.71 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =   21510.86 ms /  1091 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.84 ms /   101 runs   (    0.09 ms per token, 11422.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6269.76 ms /   359 tokens (   17.46 ms per token,    57.26 tokens per second)\n",
      "llama_print_timings:        eval time =    7079.37 ms /   100 runs   (   70.79 ms per token,    14.13 tokens per second)\n",
      "llama_print_timings:       total time =   13497.58 ms /   459 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.95 ms /    64 runs   (    0.09 ms per token, 10761.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3756.92 ms /   150 tokens (   25.05 ms per token,    39.93 tokens per second)\n",
      "llama_print_timings:        eval time =    4434.26 ms /    63 runs   (   70.39 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =    8292.17 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    81 runs   (    0.08 ms per token, 11766.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12908.30 ms /   984 tokens (   13.12 ms per token,    76.23 tokens per second)\n",
      "llama_print_timings:        eval time =    5938.11 ms /    80 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   18970.46 ms /  1064 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    65 runs   (    0.09 ms per token, 10781.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5209.72 ms /   363 tokens (   14.35 ms per token,    69.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4337.64 ms /    64 runs   (   67.78 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =    9640.98 ms /   427 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.77 ms /   271 runs   (    0.09 ms per token, 11399.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5013.80 ms /   368 tokens (   13.62 ms per token,    73.40 tokens per second)\n",
      "llama_print_timings:        eval time =   18813.51 ms /   270 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =   24257.36 ms /   638 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.26 ms /    33 runs   (    0.10 ms per token, 10119.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10796.59 ms /   691 tokens (   15.62 ms per token,    64.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2238.14 ms /    32 runs   (   69.94 ms per token,    14.30 tokens per second)\n",
      "llama_print_timings:       total time =   13086.80 ms /   723 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      29.03 ms /   318 runs   (    0.09 ms per token, 10954.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6009.82 ms /   361 tokens (   16.65 ms per token,    60.07 tokens per second)\n",
      "llama_print_timings:        eval time =   23840.44 ms /   317 runs   (   75.21 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:       total time =   30347.91 ms /   678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.47 ms /   199 runs   (    0.09 ms per token, 10773.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5702.36 ms /   380 tokens (   15.01 ms per token,    66.64 tokens per second)\n",
      "llama_print_timings:        eval time =   15042.88 ms /   198 runs   (   75.97 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time =   21056.40 ms /   578 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.78 ms /   214 runs   (    0.10 ms per token, 10300.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6587.23 ms /   382 tokens (   17.24 ms per token,    57.99 tokens per second)\n",
      "llama_print_timings:        eval time =   15992.49 ms /   213 runs   (   75.08 ms per token,    13.32 tokens per second)\n",
      "llama_print_timings:       total time =   22905.23 ms /   595 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.10 ms /   132 runs   (    0.09 ms per token, 10912.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6959.10 ms /   378 tokens (   18.41 ms per token,    54.32 tokens per second)\n",
      "llama_print_timings:        eval time =    9188.43 ms /   131 runs   (   70.14 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   16345.80 ms /   509 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.22 ms /   105 runs   (    0.09 ms per token, 11390.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5838.18 ms /   366 tokens (   15.95 ms per token,    62.69 tokens per second)\n",
      "llama_print_timings:        eval time =    7372.44 ms /   104 runs   (   70.89 ms per token,    14.11 tokens per second)\n",
      "llama_print_timings:       total time =   13365.73 ms /   470 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.07 ms /    39 runs   (    0.10 ms per token,  9579.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6841.14 ms /   375 tokens (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:        eval time =    2974.83 ms /    38 runs   (   78.28 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =    9876.03 ms /   413 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.05 ms /   222 runs   (    0.10 ms per token, 10068.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6960.99 ms /   356 tokens (   19.55 ms per token,    51.14 tokens per second)\n",
      "llama_print_timings:        eval time =   20320.60 ms /   221 runs   (   91.95 ms per token,    10.88 tokens per second)\n",
      "llama_print_timings:       total time =   27651.80 ms /   577 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.14 ms /   279 runs   (    0.09 ms per token, 10673.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7779.41 ms /   384 tokens (   20.26 ms per token,    49.36 tokens per second)\n",
      "llama_print_timings:        eval time =   23675.94 ms /   278 runs   (   85.17 ms per token,    11.74 tokens per second)\n",
      "llama_print_timings:       total time =   31889.54 ms /   662 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.96 ms /   228 runs   (    0.10 ms per token, 10382.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6083.86 ms /   369 tokens (   16.49 ms per token,    60.65 tokens per second)\n",
      "llama_print_timings:        eval time =   17451.54 ms /   227 runs   (   76.88 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:       total time =   23874.50 ms /   596 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.49 ms /   144 runs   (    0.11 ms per token,  9296.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5704.41 ms /   375 tokens (   15.21 ms per token,    65.74 tokens per second)\n",
      "llama_print_timings:        eval time =   24063.79 ms /   143 runs   (  168.28 ms per token,     5.94 tokens per second)\n",
      "llama_print_timings:       total time =   30015.91 ms /   518 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    35 runs   (    0.12 ms per token,  8488.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7462.22 ms /   356 tokens (   20.96 ms per token,    47.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4878.49 ms /    34 runs   (  143.49 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =   12405.53 ms /   390 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.72 ms /   220 runs   (    0.10 ms per token, 10130.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8043.27 ms /   372 tokens (   21.62 ms per token,    46.25 tokens per second)\n",
      "llama_print_timings:        eval time =   19953.20 ms /   219 runs   (   91.11 ms per token,    10.98 tokens per second)\n",
      "llama_print_timings:       total time =   28336.06 ms /   591 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.50 ms /   111 runs   (    0.09 ms per token, 10571.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5608.00 ms /   359 tokens (   15.62 ms per token,    64.02 tokens per second)\n",
      "llama_print_timings:        eval time =    7835.88 ms /   110 runs   (   71.24 ms per token,    14.04 tokens per second)\n",
      "llama_print_timings:       total time =   13605.71 ms /   469 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.78 ms /   131 runs   (    0.10 ms per token, 10250.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11216.30 ms /   648 tokens (   17.31 ms per token,    57.77 tokens per second)\n",
      "llama_print_timings:        eval time =   11854.34 ms /   130 runs   (   91.19 ms per token,    10.97 tokens per second)\n",
      "llama_print_timings:       total time =   23278.01 ms /   778 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.31 ms /   196 runs   (    0.09 ms per token, 10702.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6831.30 ms /   386 tokens (   17.70 ms per token,    56.50 tokens per second)\n",
      "llama_print_timings:        eval time =   15016.66 ms /   195 runs   (   77.01 ms per token,    12.99 tokens per second)\n",
      "llama_print_timings:       total time =   22152.46 ms /   581 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.00 ms /   117 runs   (    0.09 ms per token, 10639.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8275.31 ms /   404 tokens (   20.48 ms per token,    48.82 tokens per second)\n",
      "llama_print_timings:        eval time =    8895.02 ms /   116 runs   (   76.68 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:       total time =   17339.22 ms /   520 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.26 ms /   125 runs   (    0.10 ms per token, 10199.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5651.03 ms /   363 tokens (   15.57 ms per token,    64.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9100.43 ms /   124 runs   (   73.39 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   14939.10 ms /   487 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.56 ms /   157 runs   (    0.10 ms per token, 10092.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8250.00 ms /   366 tokens (   22.54 ms per token,    44.36 tokens per second)\n",
      "llama_print_timings:        eval time =   12878.35 ms /   156 runs   (   82.55 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   21381.86 ms /   522 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.09 ms /    44 runs   (    0.12 ms per token,  8651.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8421.46 ms /   367 tokens (   22.95 ms per token,    43.58 tokens per second)\n",
      "llama_print_timings:        eval time =   14273.57 ms /    43 runs   (  331.94 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   22770.32 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.91 ms /    60 runs   (    0.12 ms per token,  8681.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9307.12 ms /   363 tokens (   25.64 ms per token,    39.00 tokens per second)\n",
      "llama_print_timings:        eval time =    8784.72 ms /    59 runs   (  148.89 ms per token,     6.72 tokens per second)\n",
      "llama_print_timings:       total time =   18195.49 ms /   422 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.39 ms /   167 runs   (    0.10 ms per token, 10191.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6859.48 ms /   356 tokens (   19.27 ms per token,    51.90 tokens per second)\n",
      "llama_print_timings:        eval time =   17699.00 ms /   166 runs   (  106.62 ms per token,     9.38 tokens per second)\n",
      "llama_print_timings:       total time =   24818.28 ms /   522 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.28 ms /   145 runs   (    0.09 ms per token, 10921.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5919.07 ms /   339 tokens (   17.46 ms per token,    57.27 tokens per second)\n",
      "llama_print_timings:        eval time =   10359.34 ms /   144 runs   (   71.94 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =   16495.37 ms /   483 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.45 ms /    97 runs   (    0.10 ms per token, 10260.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7544.19 ms /   337 tokens (   22.39 ms per token,    44.67 tokens per second)\n",
      "llama_print_timings:        eval time =    6559.57 ms /    96 runs   (   68.33 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   14249.91 ms /   433 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       1.46 ms /    15 runs   (    0.10 ms per token, 10259.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6263.93 ms /   335 tokens (   18.70 ms per token,    53.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1234.12 ms /    14 runs   (   88.15 ms per token,    11.34 tokens per second)\n",
      "llama_print_timings:       total time =    7523.79 ms /   349 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.12 ms /   282 runs   (    0.10 ms per token, 10397.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7476.74 ms /   325 tokens (   23.01 ms per token,    43.47 tokens per second)\n",
      "llama_print_timings:        eval time =   22088.97 ms /   281 runs   (   78.61 ms per token,    12.72 tokens per second)\n",
      "llama_print_timings:       total time =   30006.73 ms /   606 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.05 ms /   115 runs   (    0.10 ms per token, 10404.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7428.82 ms /   425 tokens (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8882.38 ms /   114 runs   (   77.92 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   16484.89 ms /   539 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.08 ms /   207 runs   (    0.09 ms per token, 10848.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5826.57 ms /   381 tokens (   15.29 ms per token,    65.39 tokens per second)\n",
      "llama_print_timings:        eval time =   14692.41 ms /   206 runs   (   71.32 ms per token,    14.02 tokens per second)\n",
      "llama_print_timings:       total time =   20822.32 ms /   587 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.59 ms /   175 runs   (    0.09 ms per token, 11224.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6192.05 ms /   384 tokens (   16.13 ms per token,    62.02 tokens per second)\n",
      "llama_print_timings:        eval time =   12026.18 ms /   174 runs   (   69.12 ms per token,    14.47 tokens per second)\n",
      "llama_print_timings:       total time =   18469.75 ms /   558 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.52 ms /   267 runs   (    0.09 ms per token, 10887.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6695.44 ms /   357 tokens (   18.75 ms per token,    53.32 tokens per second)\n",
      "llama_print_timings:        eval time =   21213.24 ms /   266 runs   (   79.75 ms per token,    12.54 tokens per second)\n",
      "llama_print_timings:       total time =   28305.47 ms /   623 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    61 runs   (    0.11 ms per token,  8862.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11051.45 ms /   709 tokens (   15.59 ms per token,    64.15 tokens per second)\n",
      "llama_print_timings:        eval time =    6389.01 ms /    60 runs   (  106.48 ms per token,     9.39 tokens per second)\n",
      "llama_print_timings:       total time =   17544.72 ms /   769 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.24 ms /   294 runs   (    0.11 ms per token,  9118.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6184.58 ms /   376 tokens (   16.45 ms per token,    60.80 tokens per second)\n",
      "llama_print_timings:        eval time =   30932.74 ms /   293 runs   (  105.57 ms per token,     9.47 tokens per second)\n",
      "llama_print_timings:       total time =   37586.86 ms /   669 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.08 ms /   234 runs   (    0.09 ms per token, 10599.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6302.06 ms /   381 tokens (   16.54 ms per token,    60.46 tokens per second)\n",
      "llama_print_timings:        eval time =   16592.26 ms /   233 runs   (   71.21 ms per token,    14.04 tokens per second)\n",
      "llama_print_timings:       total time =   23239.28 ms /   614 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.92 ms /   239 runs   (    0.10 ms per token,  9589.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6964.45 ms /   377 tokens (   18.47 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:        eval time =   20821.54 ms /   238 runs   (   87.49 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:       total time =   28156.16 ms /   615 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.13 ms /   197 runs   (    0.11 ms per token,  9324.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6729.13 ms /   371 tokens (   18.14 ms per token,    55.13 tokens per second)\n",
      "llama_print_timings:        eval time =   22037.06 ms /   196 runs   (  112.43 ms per token,     8.89 tokens per second)\n",
      "llama_print_timings:       total time =   29091.32 ms /   567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    58 runs   (    0.10 ms per token, 10390.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11174.49 ms /   647 tokens (   17.27 ms per token,    57.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3926.40 ms /    57 runs   (   68.88 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =   15188.02 ms /   704 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.23 ms /    81 runs   (    0.09 ms per token, 11211.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6162.31 ms /   218 tokens (   28.27 ms per token,    35.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5543.01 ms /    80 runs   (   69.29 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =   11826.08 ms /   298 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    67 runs   (    0.10 ms per token, 10449.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6707.02 ms /   386 tokens (   17.38 ms per token,    57.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4934.08 ms /    66 runs   (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =   11738.36 ms /   452 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.76 ms /    70 runs   (    0.10 ms per token, 10361.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4589.08 ms /   206 tokens (   22.28 ms per token,    44.89 tokens per second)\n",
      "llama_print_timings:        eval time =    4704.89 ms /    69 runs   (   68.19 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    9398.24 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.26 ms /    69 runs   (    0.12 ms per token,  8354.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6616.76 ms /   373 tokens (   17.74 ms per token,    56.37 tokens per second)\n",
      "llama_print_timings:        eval time =    8821.11 ms /    68 runs   (  129.72 ms per token,     7.71 tokens per second)\n",
      "llama_print_timings:       total time =   15542.75 ms /   441 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.87 ms /    25 runs   (    0.19 ms per token,  5138.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9862.73 ms /   361 tokens (   27.32 ms per token,    36.60 tokens per second)\n",
      "llama_print_timings:        eval time =   15721.90 ms /    24 runs   (  655.08 ms per token,     1.53 tokens per second)\n",
      "llama_print_timings:       total time =   25645.21 ms /   385 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      36.93 ms /   409 runs   (    0.09 ms per token, 11076.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6251.22 ms /   397 tokens (   15.75 ms per token,    63.51 tokens per second)\n",
      "llama_print_timings:        eval time =   29671.28 ms /   408 runs   (   72.72 ms per token,    13.75 tokens per second)\n",
      "llama_print_timings:       total time =   36576.62 ms /   805 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.11 ms /   282 runs   (    0.09 ms per token, 10799.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5992.47 ms /   382 tokens (   15.69 ms per token,    63.75 tokens per second)\n",
      "llama_print_timings:        eval time =   22440.24 ms /   281 runs   (   79.86 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   28873.79 ms /   663 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.33 ms /   173 runs   (    0.10 ms per token,  9982.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7076.91 ms /   376 tokens (   18.82 ms per token,    53.13 tokens per second)\n",
      "llama_print_timings:        eval time =   33833.59 ms /   172 runs   (  196.71 ms per token,     5.08 tokens per second)\n",
      "llama_print_timings:       total time =   41197.34 ms /   548 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.09 ms /   183 runs   (    0.12 ms per token,  8678.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7964.57 ms /   399 tokens (   19.96 ms per token,    50.10 tokens per second)\n",
      "llama_print_timings:        eval time =   32620.98 ms /   182 runs   (  179.24 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   40924.02 ms /   581 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.00 ms /   196 runs   (    0.10 ms per token,  9798.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8560.42 ms /   385 tokens (   22.23 ms per token,    44.97 tokens per second)\n",
      "llama_print_timings:        eval time =   16547.78 ms /   195 runs   (   84.86 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =   25412.52 ms /   580 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.36 ms /   140 runs   (    0.13 ms per token,  7626.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16957.48 ms /   660 tokens (   25.69 ms per token,    38.92 tokens per second)\n",
      "llama_print_timings:        eval time =   21189.91 ms /   139 runs   (  152.45 ms per token,     6.56 tokens per second)\n",
      "llama_print_timings:       total time =   38400.92 ms /   799 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.40 ms /   165 runs   (    0.09 ms per token, 10717.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12148.38 ms /   647 tokens (   18.78 ms per token,    53.26 tokens per second)\n",
      "llama_print_timings:        eval time =   12581.52 ms /   164 runs   (   76.72 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =   25017.41 ms /   811 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.60 ms /   108 runs   (    0.12 ms per token,  8570.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6096.57 ms /   362 tokens (   16.84 ms per token,    59.38 tokens per second)\n",
      "llama_print_timings:        eval time =    8224.75 ms /   107 runs   (   76.87 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:       total time =   14507.34 ms /   469 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.08 ms /   187 runs   (    0.10 ms per token, 10340.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7424.37 ms /   356 tokens (   20.85 ms per token,    47.95 tokens per second)\n",
      "llama_print_timings:        eval time =   14961.15 ms /   186 runs   (   80.44 ms per token,    12.43 tokens per second)\n",
      "llama_print_timings:       total time =   22672.37 ms /   542 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.26 ms /   120 runs   (    0.10 ms per token,  9787.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9260.44 ms /   387 tokens (   23.93 ms per token,    41.79 tokens per second)\n",
      "llama_print_timings:        eval time =   12681.23 ms /   119 runs   (  106.56 ms per token,     9.38 tokens per second)\n",
      "llama_print_timings:       total time =   22133.60 ms /   506 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.96 ms /    94 runs   (    0.11 ms per token,  9433.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5205.17 ms /   218 tokens (   23.88 ms per token,    41.88 tokens per second)\n",
      "llama_print_timings:        eval time =    8626.19 ms /    93 runs   (   92.75 ms per token,    10.78 tokens per second)\n",
      "llama_print_timings:       total time =   13979.89 ms /   311 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.11 ms /    88 runs   (    0.10 ms per token,  9663.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6186.35 ms /   315 tokens (   19.64 ms per token,    50.92 tokens per second)\n",
      "llama_print_timings:        eval time =    8002.55 ms /    87 runs   (   91.98 ms per token,    10.87 tokens per second)\n",
      "llama_print_timings:       total time =   14339.20 ms /   402 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      35.91 ms /   344 runs   (    0.10 ms per token,  9578.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12028.97 ms /   648 tokens (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:        eval time =   27181.81 ms /   343 runs   (   79.25 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =   39885.36 ms /   991 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.06 ms /   188 runs   (    0.12 ms per token,  8522.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7531.84 ms /   362 tokens (   20.81 ms per token,    48.06 tokens per second)\n",
      "llama_print_timings:        eval time =   38176.50 ms /   187 runs   (  204.15 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:       total time =   46035.88 ms /   549 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.12 ms /   351 runs   (    0.10 ms per token, 10286.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4720.48 ms /   187 tokens (   25.24 ms per token,    39.61 tokens per second)\n",
      "llama_print_timings:        eval time =   25980.93 ms /   350 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   31249.80 ms /   537 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.47 ms /   139 runs   (    0.10 ms per token, 10316.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8635.88 ms /   397 tokens (   21.75 ms per token,    45.97 tokens per second)\n",
      "llama_print_timings:        eval time =   11240.11 ms /   138 runs   (   81.45 ms per token,    12.28 tokens per second)\n",
      "llama_print_timings:       total time =   20084.27 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.94 ms /   261 runs   (    0.10 ms per token, 10466.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7532.58 ms /   388 tokens (   19.41 ms per token,    51.51 tokens per second)\n",
      "llama_print_timings:        eval time =   18208.69 ms /   260 runs   (   70.03 ms per token,    14.28 tokens per second)\n",
      "llama_print_timings:       total time =   26132.20 ms /   648 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.24 ms /   114 runs   (    0.09 ms per token, 11128.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11791.34 ms /   763 tokens (   15.45 ms per token,    64.71 tokens per second)\n",
      "llama_print_timings:        eval time =    8162.52 ms /   113 runs   (   72.23 ms per token,    13.84 tokens per second)\n",
      "llama_print_timings:       total time =   20129.11 ms /   876 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.87 ms /   347 runs   (    0.09 ms per token, 11239.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6158.43 ms /   425 tokens (   14.49 ms per token,    69.01 tokens per second)\n",
      "llama_print_timings:        eval time =   24802.32 ms /   346 runs   (   71.68 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =   31473.61 ms /   771 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.13 ms /   222 runs   (    0.10 ms per token, 10033.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7457.22 ms /   372 tokens (   20.05 ms per token,    49.88 tokens per second)\n",
      "llama_print_timings:        eval time =   18939.51 ms /   221 runs   (   85.70 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =   26755.92 ms /   593 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.30 ms /   206 runs   (    0.10 ms per token,  9670.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6151.84 ms /   375 tokens (   16.40 ms per token,    60.96 tokens per second)\n",
      "llama_print_timings:        eval time =   19763.95 ms /   205 runs   (   96.41 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =   26232.95 ms /   580 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      37.52 ms /   373 runs   (    0.10 ms per token,  9940.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5965.62 ms /   357 tokens (   16.71 ms per token,    59.84 tokens per second)\n",
      "llama_print_timings:        eval time =   34199.07 ms /   372 runs   (   91.93 ms per token,    10.88 tokens per second)\n",
      "llama_print_timings:       total time =   40759.96 ms /   729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    64 runs   (    0.12 ms per token,  8423.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7442.30 ms /   381 tokens (   19.53 ms per token,    51.19 tokens per second)\n",
      "llama_print_timings:        eval time =    6177.91 ms /    63 runs   (   98.06 ms per token,    10.20 tokens per second)\n",
      "llama_print_timings:       total time =   13721.89 ms /   444 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.44 ms /    89 runs   (    0.12 ms per token,  8528.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6707.80 ms /   385 tokens (   17.42 ms per token,    57.40 tokens per second)\n",
      "llama_print_timings:        eval time =    8803.73 ms /    88 runs   (  100.04 ms per token,    10.00 tokens per second)\n",
      "llama_print_timings:       total time =   15657.06 ms /   473 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /   154 runs   (    0.10 ms per token, 10292.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7110.86 ms /   402 tokens (   17.69 ms per token,    56.53 tokens per second)\n",
      "llama_print_timings:        eval time =   13784.23 ms /   153 runs   (   90.09 ms per token,    11.10 tokens per second)\n",
      "llama_print_timings:       total time =   21133.01 ms /   555 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.00 ms /   136 runs   (    0.10 ms per token,  9713.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5786.68 ms /   374 tokens (   15.47 ms per token,    64.63 tokens per second)\n",
      "llama_print_timings:        eval time =   12006.04 ms /   135 runs   (   88.93 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =   18001.64 ms /   509 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.77 ms /   187 runs   (    0.10 ms per token,  9964.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6157.81 ms /   377 tokens (   16.33 ms per token,    61.22 tokens per second)\n",
      "llama_print_timings:        eval time =   16314.96 ms /   186 runs   (   87.71 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =   22754.64 ms /   563 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.16 ms /   103 runs   (    0.10 ms per token, 10133.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8031.65 ms /   380 tokens (   21.14 ms per token,    47.31 tokens per second)\n",
      "llama_print_timings:        eval time =   12297.60 ms /   102 runs   (  120.56 ms per token,     8.29 tokens per second)\n",
      "llama_print_timings:       total time =   20479.89 ms /   482 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.80 ms /   190 runs   (    0.10 ms per token,  9595.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7654.59 ms /   357 tokens (   21.44 ms per token,    46.64 tokens per second)\n",
      "llama_print_timings:        eval time =   28668.39 ms /   189 runs   (  151.68 ms per token,     6.59 tokens per second)\n",
      "llama_print_timings:       total time =   36631.49 ms /   546 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.45 ms /   255 runs   (    0.10 ms per token, 10021.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16710.33 ms /  1043 tokens (   16.02 ms per token,    62.42 tokens per second)\n",
      "llama_print_timings:        eval time =   27559.13 ms /   254 runs   (  108.50 ms per token,     9.22 tokens per second)\n",
      "llama_print_timings:       total time =   44887.75 ms /  1297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.02 ms /    73 runs   (    0.15 ms per token,  6625.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16454.03 ms /   746 tokens (   22.06 ms per token,    45.34 tokens per second)\n",
      "llama_print_timings:        eval time =   30552.50 ms /    72 runs   (  424.34 ms per token,     2.36 tokens per second)\n",
      "llama_print_timings:       total time =   47161.89 ms /   818 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.93 ms /   151 runs   (    0.11 ms per token,  9481.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11480.84 ms /   613 tokens (   18.73 ms per token,    53.39 tokens per second)\n",
      "llama_print_timings:        eval time =   15600.66 ms /   150 runs   (  104.00 ms per token,     9.61 tokens per second)\n",
      "llama_print_timings:       total time =   27333.88 ms /   763 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.44 ms /   341 runs   (    0.09 ms per token, 10846.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13212.93 ms /   786 tokens (   16.81 ms per token,    59.49 tokens per second)\n",
      "llama_print_timings:        eval time =   29329.17 ms /   340 runs   (   86.26 ms per token,    11.59 tokens per second)\n",
      "llama_print_timings:       total time =   43116.65 ms /  1126 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.36 ms /   202 runs   (    0.09 ms per token, 11005.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17603.74 ms /  1051 tokens (   16.75 ms per token,    59.70 tokens per second)\n",
      "llama_print_timings:        eval time =   15516.14 ms /   201 runs   (   77.19 ms per token,    12.95 tokens per second)\n",
      "llama_print_timings:       total time =   33428.05 ms /  1252 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.15 ms /   153 runs   (    0.10 ms per token, 10099.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10856.55 ms /   696 tokens (   15.60 ms per token,    64.11 tokens per second)\n",
      "llama_print_timings:        eval time =   11568.68 ms /   152 runs   (   76.11 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:       total time =   22657.17 ms /   848 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.27 ms /   217 runs   (    0.11 ms per token,  8942.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6635.79 ms /   357 tokens (   18.59 ms per token,    53.80 tokens per second)\n",
      "llama_print_timings:        eval time =   45109.38 ms /   216 runs   (  208.84 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   52129.77 ms /   573 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    55 runs   (    0.10 ms per token,  9942.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10680.01 ms /   603 tokens (   17.71 ms per token,    56.46 tokens per second)\n",
      "llama_print_timings:        eval time =    3915.50 ms /    54 runs   (   72.51 ms per token,    13.79 tokens per second)\n",
      "llama_print_timings:       total time =   14679.87 ms /   657 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.68 ms /   228 runs   (    0.10 ms per token, 10516.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12224.87 ms /   773 tokens (   15.81 ms per token,    63.23 tokens per second)\n",
      "llama_print_timings:        eval time =   17696.78 ms /   227 runs   (   77.96 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   30271.66 ms /  1000 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.64 ms /   172 runs   (    0.09 ms per token, 10993.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12975.48 ms /   797 tokens (   16.28 ms per token,    61.42 tokens per second)\n",
      "llama_print_timings:        eval time =   12350.13 ms /   171 runs   (   72.22 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =   25585.29 ms /   968 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.75 ms /   217 runs   (    0.09 ms per token, 11575.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15853.47 ms /  1120 tokens (   14.15 ms per token,    70.65 tokens per second)\n",
      "llama_print_timings:        eval time =   15830.56 ms /   216 runs   (   73.29 ms per token,    13.64 tokens per second)\n",
      "llama_print_timings:       total time =   32050.30 ms /  1336 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.96 ms /   373 runs   (    0.09 ms per token, 11317.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5224.47 ms /   373 tokens (   14.01 ms per token,    71.39 tokens per second)\n",
      "llama_print_timings:        eval time =   28475.67 ms /   372 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   34268.12 ms /   745 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.01 ms /   117 runs   (    0.09 ms per token, 10625.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15777.59 ms /  1061 tokens (   14.87 ms per token,    67.25 tokens per second)\n",
      "llama_print_timings:        eval time =    8326.50 ms /   116 runs   (   71.78 ms per token,    13.93 tokens per second)\n",
      "llama_print_timings:       total time =   24276.42 ms /  1177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.76 ms /   190 runs   (    0.09 ms per token, 10698.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5794.18 ms /   373 tokens (   15.53 ms per token,    64.37 tokens per second)\n",
      "llama_print_timings:        eval time =   13447.58 ms /   189 runs   (   71.15 ms per token,    14.05 tokens per second)\n",
      "llama_print_timings:       total time =   19519.75 ms /   562 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.23 ms /   374 runs   (    0.09 ms per token, 11253.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14543.29 ms /  1038 tokens (   14.01 ms per token,    71.37 tokens per second)\n",
      "llama_print_timings:        eval time =   27021.72 ms /   373 runs   (   72.44 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   42198.99 ms /  1411 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.48 ms /   216 runs   (    0.09 ms per token, 11087.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10433.96 ms /   730 tokens (   14.29 ms per token,    69.96 tokens per second)\n",
      "llama_print_timings:        eval time =   15111.46 ms /   215 runs   (   70.29 ms per token,    14.23 tokens per second)\n",
      "llama_print_timings:       total time =   25859.05 ms /   945 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.01 ms /   267 runs   (    0.09 ms per token, 11119.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5595.10 ms /   373 tokens (   15.00 ms per token,    66.67 tokens per second)\n",
      "llama_print_timings:        eval time =   19794.45 ms /   266 runs   (   74.42 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   25783.43 ms /   639 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      41.57 ms /   472 runs   (    0.09 ms per token, 11355.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13322.62 ms /   773 tokens (   17.23 ms per token,    58.02 tokens per second)\n",
      "llama_print_timings:        eval time =   34521.47 ms /   471 runs   (   73.29 ms per token,    13.64 tokens per second)\n",
      "llama_print_timings:       total time =   48560.27 ms /  1244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      44.66 ms /   511 runs   (    0.09 ms per token, 11442.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16494.58 ms /  1097 tokens (   15.04 ms per token,    66.51 tokens per second)\n",
      "llama_print_timings:        eval time =   38249.85 ms /   510 runs   (   75.00 ms per token,    13.33 tokens per second)\n",
      "llama_print_timings:       total time =   55841.36 ms /  1607 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.33 ms /   190 runs   (    0.09 ms per token, 10962.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5271.85 ms /   373 tokens (   14.13 ms per token,    70.75 tokens per second)\n",
      "llama_print_timings:        eval time =   13638.63 ms /   189 runs   (   72.16 ms per token,    13.86 tokens per second)\n",
      "llama_print_timings:       total time =   19193.73 ms /   562 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      37.25 ms /   411 runs   (    0.09 ms per token, 11034.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17735.68 ms /  1185 tokens (   14.97 ms per token,    66.81 tokens per second)\n",
      "llama_print_timings:        eval time =   30018.73 ms /   410 runs   (   73.22 ms per token,    13.66 tokens per second)\n",
      "llama_print_timings:       total time =   48395.69 ms /  1595 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.85 ms /   249 runs   (    0.09 ms per token, 11397.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13644.93 ms /   997 tokens (   13.69 ms per token,    73.07 tokens per second)\n",
      "llama_print_timings:        eval time =   17732.50 ms /   248 runs   (   71.50 ms per token,    13.99 tokens per second)\n",
      "llama_print_timings:       total time =   31754.34 ms /  1245 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.70 ms /    86 runs   (    0.09 ms per token, 11176.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13268.65 ms /   996 tokens (   13.32 ms per token,    75.06 tokens per second)\n",
      "llama_print_timings:        eval time =    6032.18 ms /    85 runs   (   70.97 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =   19427.23 ms /  1081 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.82 ms /   302 runs   (    0.09 ms per token, 11258.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16317.85 ms /  1044 tokens (   15.63 ms per token,    63.98 tokens per second)\n",
      "llama_print_timings:        eval time =   21628.95 ms /   301 runs   (   71.86 ms per token,    13.92 tokens per second)\n",
      "llama_print_timings:       total time =   38395.78 ms /  1345 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.99 ms /   196 runs   (    0.09 ms per token, 11536.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16298.11 ms /  1046 tokens (   15.58 ms per token,    64.18 tokens per second)\n",
      "llama_print_timings:        eval time =   14434.07 ms /   195 runs   (   74.02 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   31032.43 ms /  1241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.24 ms /   440 runs   (    0.09 ms per token, 11212.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21587.30 ms /  1038 tokens (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:        eval time =   31747.71 ms /   439 runs   (   72.32 ms per token,    13.83 tokens per second)\n",
      "llama_print_timings:       total time =   54008.77 ms /  1477 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.57 ms /   173 runs   (    0.10 ms per token, 10438.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12408.18 ms /   817 tokens (   15.19 ms per token,    65.84 tokens per second)\n",
      "llama_print_timings:        eval time =   12029.62 ms /   172 runs   (   69.94 ms per token,    14.30 tokens per second)\n",
      "llama_print_timings:       total time =   24711.38 ms /   989 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "token_splitter_1024\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:41<00:00,  1.26it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.41 ms /    90 runs   (    0.10 ms per token,  9567.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17785.35 ms /   635 tokens (   28.01 ms per token,    35.70 tokens per second)\n",
      "llama_print_timings:        eval time =    8332.64 ms /    89 runs   (   93.63 ms per token,    10.68 tokens per second)\n",
      "llama_print_timings:       total time =   26304.46 ms /   724 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.70 ms /   433 runs   (    0.09 ms per token, 11189.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32842.42 ms /  1984 tokens (   16.55 ms per token,    60.41 tokens per second)\n",
      "llama_print_timings:        eval time =   44582.05 ms /   432 runs   (  103.20 ms per token,     9.69 tokens per second)\n",
      "llama_print_timings:       total time =   78339.18 ms /  2416 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.68 ms /    90 runs   (    0.10 ms per token, 10371.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11302.90 ms /   635 tokens (   17.80 ms per token,    56.18 tokens per second)\n",
      "llama_print_timings:        eval time =    8120.61 ms /    89 runs   (   91.24 ms per token,    10.96 tokens per second)\n",
      "llama_print_timings:       total time =   19563.51 ms /   724 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      44.51 ms /   484 runs   (    0.09 ms per token, 10874.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31532.83 ms /  2048 tokens (   15.40 ms per token,    64.95 tokens per second)\n",
      "llama_print_timings:        eval time =   47870.15 ms /   484 runs   (   98.91 ms per token,    10.11 tokens per second)\n",
      "llama_print_timings:       total time =   80250.04 ms /  2532 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.15 ms /   104 runs   (    0.10 ms per token, 10249.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12613.86 ms /   633 tokens (   19.93 ms per token,    50.18 tokens per second)\n",
      "llama_print_timings:        eval time =    8851.32 ms /   103 runs   (   85.94 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:       total time =   21629.69 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.30 ms /   211 runs   (    0.10 ms per token, 10392.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17293.51 ms /  1111 tokens (   15.57 ms per token,    64.24 tokens per second)\n",
      "llama_print_timings:        eval time =   18671.17 ms /   210 runs   (   88.91 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =   36387.09 ms /  1321 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.32 ms /    75 runs   (    0.10 ms per token, 10248.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10993.43 ms /   633 tokens (   17.37 ms per token,    57.58 tokens per second)\n",
      "llama_print_timings:        eval time =    6367.21 ms /    74 runs   (   86.04 ms per token,    11.62 tokens per second)\n",
      "llama_print_timings:       total time =   17479.37 ms /   707 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.10 ms /   422 runs   (    0.09 ms per token, 10792.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20072.75 ms /  1229 tokens (   16.33 ms per token,    61.23 tokens per second)\n",
      "llama_print_timings:        eval time =   48644.48 ms /   421 runs   (  115.55 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:       total time =   69573.91 ms /  1650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.58 ms /    78 runs   (    0.11 ms per token,  9089.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11693.57 ms /   633 tokens (   18.47 ms per token,    54.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10994.36 ms /    77 runs   (  142.78 ms per token,     7.00 tokens per second)\n",
      "llama_print_timings:       total time =   22829.05 ms /   710 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      44.86 ms /   478 runs   (    0.09 ms per token, 10656.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28681.92 ms /  1401 tokens (   20.47 ms per token,    48.85 tokens per second)\n",
      "llama_print_timings:        eval time =   46406.57 ms /   477 runs   (   97.29 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =   75997.66 ms /  1878 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.56 ms /    78 runs   (    0.10 ms per token, 10314.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10390.81 ms /   633 tokens (   16.42 ms per token,    60.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5251.97 ms /    77 runs   (   68.21 ms per token,    14.66 tokens per second)\n",
      "llama_print_timings:       total time =   15754.76 ms /   710 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      43.11 ms /   478 runs   (    0.09 ms per token, 11087.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19550.79 ms /  1401 tokens (   13.95 ms per token,    71.66 tokens per second)\n",
      "llama_print_timings:        eval time =   38197.39 ms /   477 runs   (   80.08 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =   58485.57 ms /  1878 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      28.70 ms /   312 runs   (    0.09 ms per token, 10869.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18411.80 ms /  1232 tokens (   14.94 ms per token,    66.91 tokens per second)\n",
      "llama_print_timings:        eval time =  981125.63 ms /   311 runs   ( 3154.74 ms per token,     0.32 tokens per second)\n",
      "llama_print_timings:       total time = 1000005.33 ms /  1543 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.22 ms /    90 runs   (    0.10 ms per token,  9761.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10353.92 ms /   635 tokens (   16.31 ms per token,    61.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6290.96 ms /    89 runs   (   70.68 ms per token,    14.15 tokens per second)\n",
      "llama_print_timings:       total time =   16781.17 ms /   724 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.14 ms /   279 runs   (    0.11 ms per token,  9257.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17353.51 ms /  1111 tokens (   15.62 ms per token,    64.02 tokens per second)\n",
      "llama_print_timings:        eval time =  926573.45 ms /   278 runs   ( 3333.00 ms per token,     0.30 tokens per second)\n",
      "llama_print_timings:       total time =  944409.67 ms /  1389 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.77 ms /   133 runs   (    0.10 ms per token,  9662.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13939.55 ms /   633 tokens (   22.02 ms per token,    45.41 tokens per second)\n",
      "llama_print_timings:        eval time =   13459.75 ms /   132 runs   (  101.97 ms per token,     9.81 tokens per second)\n",
      "llama_print_timings:       total time =   27630.66 ms /   765 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.08 ms /   163 runs   (    0.10 ms per token,  9542.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12509.06 ms /   608 tokens (   20.57 ms per token,    48.60 tokens per second)\n",
      "llama_print_timings:        eval time =   16486.95 ms /   162 runs   (  101.77 ms per token,     9.83 tokens per second)\n",
      "llama_print_timings:       total time =   29259.45 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.07 ms /    42 runs   (    0.10 ms per token, 10314.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12766.32 ms /   625 tokens (   20.43 ms per token,    48.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3125.66 ms /    41 runs   (   76.24 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:       total time =   15960.35 ms /   666 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.53 ms /   173 runs   (    0.09 ms per token, 11141.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25415.76 ms /  1090 tokens (   23.32 ms per token,    42.89 tokens per second)\n",
      "llama_print_timings:        eval time =   12509.68 ms /   172 runs   (   72.73 ms per token,    13.75 tokens per second)\n",
      "llama_print_timings:       total time =   38192.51 ms /  1262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      28.14 ms /   311 runs   (    0.09 ms per token, 11051.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12115.41 ms /   633 tokens (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:        eval time =   24096.50 ms /   310 runs   (   77.73 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   36837.80 ms /   943 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.10 ms /   149 runs   (    0.09 ms per token, 10566.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11238.74 ms /   622 tokens (   18.07 ms per token,    55.34 tokens per second)\n",
      "llama_print_timings:        eval time =   10107.35 ms /   148 runs   (   68.29 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   21568.96 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.88 ms /   212 runs   (    0.10 ms per token, 10152.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10741.88 ms /   653 tokens (   16.45 ms per token,    60.79 tokens per second)\n",
      "llama_print_timings:        eval time =   18073.90 ms /   211 runs   (   85.66 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =   29147.62 ms /   864 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.91 ms /    90 runs   (    0.10 ms per token, 10098.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11467.84 ms /   619 tokens (   18.53 ms per token,    53.98 tokens per second)\n",
      "llama_print_timings:        eval time =    6066.82 ms /    89 runs   (   68.17 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =   17665.94 ms /   708 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.12 ms /   512 runs   (    0.09 ms per token, 11348.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10637.34 ms /   643 tokens (   16.54 ms per token,    60.45 tokens per second)\n",
      "llama_print_timings:        eval time =  519332.74 ms /   511 runs   ( 1016.31 ms per token,     0.98 tokens per second)\n",
      "llama_print_timings:       total time =  530916.62 ms /  1154 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.45 ms /    90 runs   (    0.11 ms per token,  9521.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11625.10 ms /   619 tokens (   18.78 ms per token,    53.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7465.66 ms /    89 runs   (   83.88 ms per token,    11.92 tokens per second)\n",
      "llama_print_timings:       total time =   19234.45 ms /   708 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.12 ms /   213 runs   (    0.12 ms per token,  8480.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6124.64 ms /   320 tokens (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:        eval time =   17508.58 ms /   212 runs   (   82.59 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   24016.10 ms /   532 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      37.41 ms /   410 runs   (    0.09 ms per token, 10959.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10697.59 ms /   642 tokens (   16.66 ms per token,    60.01 tokens per second)\n",
      "llama_print_timings:        eval time =   33097.46 ms /   409 runs   (   80.92 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =   44432.15 ms /  1051 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.97 ms /   220 runs   (    0.10 ms per token, 10012.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10766.78 ms /   614 tokens (   17.54 ms per token,    57.03 tokens per second)\n",
      "llama_print_timings:        eval time = 1045691.26 ms /   219 runs   ( 4774.85 ms per token,     0.21 tokens per second)\n",
      "llama_print_timings:       total time = 1056808.87 ms /   833 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.66 ms /   296 runs   (    0.09 ms per token, 11103.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10455.48 ms /   642 tokens (   16.29 ms per token,    61.40 tokens per second)\n",
      "llama_print_timings:        eval time =  991791.69 ms /   295 runs   ( 3362.01 ms per token,     0.30 tokens per second)\n",
      "llama_print_timings:       total time = 1002692.25 ms /   937 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.19 ms /   213 runs   (    0.09 ms per token, 11097.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12644.59 ms /   664 tokens (   19.04 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:        eval time =   14943.50 ms /   212 runs   (   70.49 ms per token,    14.19 tokens per second)\n",
      "llama_print_timings:       total time =   27891.91 ms /   876 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.42 ms /   354 runs   (    0.09 ms per token, 10591.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3333.06 ms /    63 tokens (   52.91 ms per token,    18.90 tokens per second)\n",
      "llama_print_timings:        eval time =   25231.29 ms /   353 runs   (   71.48 ms per token,    13.99 tokens per second)\n",
      "llama_print_timings:       total time =   29089.64 ms /   416 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      48.45 ms /   348 runs   (    0.14 ms per token,  7182.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12888.25 ms /   649 tokens (   19.86 ms per token,    50.36 tokens per second)\n",
      "llama_print_timings:        eval time =   34112.67 ms /   347 runs   (   98.31 ms per token,    10.17 tokens per second)\n",
      "llama_print_timings:       total time =   47801.22 ms /   996 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      36.93 ms /   260 runs   (    0.14 ms per token,  7040.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12646.85 ms /   653 tokens (   19.37 ms per token,    51.63 tokens per second)\n",
      "llama_print_timings:        eval time =   24189.06 ms /   259 runs   (   93.39 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:       total time =   37422.81 ms /   912 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.04 ms /   148 runs   (    0.14 ms per token,  7035.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11651.74 ms /   649 tokens (   17.95 ms per token,    55.70 tokens per second)\n",
      "llama_print_timings:        eval time =   10837.41 ms /   147 runs   (   73.72 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   22803.06 ms /   796 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.29 ms /    90 runs   (    0.13 ms per token,  7972.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11612.17 ms /   619 tokens (   18.76 ms per token,    53.31 tokens per second)\n",
      "llama_print_timings:        eval time =    8277.12 ms /    89 runs   (   93.00 ms per token,    10.75 tokens per second)\n",
      "llama_print_timings:       total time =   20095.96 ms /   708 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.20 ms /   105 runs   (    0.14 ms per token,  6908.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12343.78 ms /   624 tokens (   19.78 ms per token,    50.55 tokens per second)\n",
      "llama_print_timings:        eval time =    8260.59 ms /   104 runs   (   79.43 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:       total time =   20864.98 ms /   728 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.64 ms /   167 runs   (    0.14 ms per token,  7374.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11314.77 ms /   614 tokens (   18.43 ms per token,    54.27 tokens per second)\n",
      "llama_print_timings:        eval time =  193967.59 ms /   166 runs   ( 1168.48 ms per token,     0.86 tokens per second)\n",
      "llama_print_timings:       total time =  205645.51 ms /   780 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.40 ms /    94 runs   (    0.11 ms per token,  9035.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10323.26 ms /   598 tokens (   17.26 ms per token,    57.93 tokens per second)\n",
      "llama_print_timings:        eval time =    6414.92 ms /    93 runs   (   68.98 ms per token,    14.50 tokens per second)\n",
      "llama_print_timings:       total time =   16919.18 ms /   691 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.98 ms /    70 runs   (    0.09 ms per token, 11715.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11117.25 ms /   605 tokens (   18.38 ms per token,    54.42 tokens per second)\n",
      "llama_print_timings:        eval time =    4701.07 ms /    69 runs   (   68.13 ms per token,    14.68 tokens per second)\n",
      "llama_print_timings:       total time =   15916.43 ms /   674 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.79 ms /    70 runs   (    0.10 ms per token, 10316.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11788.33 ms /   615 tokens (   19.17 ms per token,    52.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5018.72 ms /    69 runs   (   72.74 ms per token,    13.75 tokens per second)\n",
      "llama_print_timings:       total time =   16911.58 ms /   684 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.23 ms /   290 runs   (    0.10 ms per token,  9594.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13737.35 ms /   627 tokens (   21.91 ms per token,    45.64 tokens per second)\n",
      "llama_print_timings:        eval time =   25544.43 ms /   289 runs   (   88.39 ms per token,    11.31 tokens per second)\n",
      "llama_print_timings:       total time =   39789.89 ms /   916 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.75 ms /   136 runs   (    0.09 ms per token, 11576.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10269.08 ms /   621 tokens (   16.54 ms per token,    60.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9298.37 ms /   135 runs   (   68.88 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =   19752.88 ms /   756 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.12 ms /   199 runs   (    0.09 ms per token, 11624.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10551.31 ms /   620 tokens (   17.02 ms per token,    58.76 tokens per second)\n",
      "llama_print_timings:        eval time =   15607.94 ms /   198 runs   (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =   26451.14 ms /   818 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.94 ms /   123 runs   (    0.09 ms per token, 11244.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10861.23 ms /   613 tokens (   17.72 ms per token,    56.44 tokens per second)\n",
      "llama_print_timings:        eval time =    8338.55 ms /   122 runs   (   68.35 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =   19383.34 ms /   735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.12 ms /   157 runs   (    0.09 ms per token, 11122.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10320.85 ms /   646 tokens (   15.98 ms per token,    62.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10940.72 ms /   156 runs   (   70.13 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   21481.54 ms /   802 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      35.30 ms /   376 runs   (    0.09 ms per token, 10651.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13626.39 ms /   618 tokens (   22.05 ms per token,    45.35 tokens per second)\n",
      "llama_print_timings:        eval time =   25796.35 ms /   375 runs   (   68.79 ms per token,    14.54 tokens per second)\n",
      "llama_print_timings:       total time =   39987.62 ms /   993 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.72 ms /   163 runs   (    0.09 ms per token, 11074.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10587.93 ms /   629 tokens (   16.83 ms per token,    59.41 tokens per second)\n",
      "llama_print_timings:        eval time =   12824.05 ms /   162 runs   (   79.16 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   23644.31 ms /   791 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.94 ms /   123 runs   (    0.10 ms per token, 10299.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11105.34 ms /   646 tokens (   17.19 ms per token,    58.17 tokens per second)\n",
      "llama_print_timings:        eval time =    8400.38 ms /   122 runs   (   68.86 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =   19679.73 ms /   768 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.47 ms /   275 runs   (    0.09 ms per token, 11236.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10483.44 ms /   645 tokens (   16.25 ms per token,    61.53 tokens per second)\n",
      "llama_print_timings:        eval time =   20584.86 ms /   274 runs   (   75.13 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   31482.47 ms /   919 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.14 ms /   172 runs   (    0.09 ms per token, 11356.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10609.48 ms /   627 tokens (   16.92 ms per token,    59.10 tokens per second)\n",
      "llama_print_timings:        eval time =   11712.03 ms /   171 runs   (   68.49 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =   22569.31 ms /   798 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.76 ms /   272 runs   (    0.09 ms per token, 11447.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12108.17 ms /   614 tokens (   19.72 ms per token,    50.71 tokens per second)\n",
      "llama_print_timings:        eval time =   18631.70 ms /   271 runs   (   68.75 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   31125.73 ms /   885 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.79 ms /   173 runs   (    0.08 ms per token, 12542.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21188.88 ms /  1447 tokens (   14.64 ms per token,    68.29 tokens per second)\n",
      "llama_print_timings:        eval time =  458312.41 ms /   172 runs   ( 2664.61 ms per token,     0.38 tokens per second)\n",
      "llama_print_timings:       total time =  479745.06 ms /  1619 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.05 ms /   243 runs   (    0.09 ms per token, 11541.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10207.01 ms /   622 tokens (   16.41 ms per token,    60.94 tokens per second)\n",
      "llama_print_timings:        eval time =   16636.86 ms /   242 runs   (   68.75 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   27188.72 ms /   864 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.55 ms /   108 runs   (    0.09 ms per token, 11312.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25785.30 ms /  1444 tokens (   17.86 ms per token,    56.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7802.22 ms /   107 runs   (   72.92 ms per token,    13.71 tokens per second)\n",
      "llama_print_timings:       total time =   33746.54 ms /  1551 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.27 ms /   223 runs   (    0.09 ms per token, 11004.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10006.05 ms /   617 tokens (   16.22 ms per token,    61.66 tokens per second)\n",
      "llama_print_timings:        eval time =  947276.64 ms /   222 runs   ( 4267.01 ms per token,     0.23 tokens per second)\n",
      "llama_print_timings:       total time =  957616.57 ms /   839 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.68 ms /   234 runs   (    0.09 ms per token, 10791.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13371.52 ms /   910 tokens (   14.69 ms per token,    68.06 tokens per second)\n",
      "llama_print_timings:        eval time =   17098.48 ms /   233 runs   (   73.38 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   30808.30 ms /  1143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /   157 runs   (    0.09 ms per token, 11414.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5703.79 ms /   340 tokens (   16.78 ms per token,    59.61 tokens per second)\n",
      "llama_print_timings:        eval time =   11929.46 ms /   156 runs   (   76.47 ms per token,    13.08 tokens per second)\n",
      "llama_print_timings:       total time =   17860.11 ms /   496 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.70 ms /   231 runs   (    0.11 ms per token,  8986.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12096.13 ms /   670 tokens (   18.05 ms per token,    55.39 tokens per second)\n",
      "llama_print_timings:        eval time =   16130.86 ms /   230 runs   (   70.13 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   28632.34 ms /   900 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.17 ms /    60 runs   (    0.12 ms per token,  8363.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12477.15 ms /   609 tokens (   20.49 ms per token,    48.81 tokens per second)\n",
      "llama_print_timings:        eval time =    4116.09 ms /    59 runs   (   69.76 ms per token,    14.33 tokens per second)\n",
      "llama_print_timings:       total time =   16710.18 ms /   668 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.88 ms /   113 runs   (    0.13 ms per token,  7596.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11178.23 ms /   679 tokens (   16.46 ms per token,    60.74 tokens per second)\n",
      "llama_print_timings:        eval time =    8252.18 ms /   112 runs   (   73.68 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   19663.37 ms /   791 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.29 ms /   111 runs   (    0.15 ms per token,  6814.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8550.72 ms /   327 tokens (   26.15 ms per token,    38.24 tokens per second)\n",
      "llama_print_timings:        eval time =    9087.00 ms /   110 runs   (   82.61 ms per token,    12.11 tokens per second)\n",
      "llama_print_timings:       total time =   17863.77 ms /   437 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.93 ms /   110 runs   (    0.14 ms per token,  6903.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11940.80 ms /   661 tokens (   18.06 ms per token,    55.36 tokens per second)\n",
      "llama_print_timings:        eval time =    8662.91 ms /   109 runs   (   79.48 ms per token,    12.58 tokens per second)\n",
      "llama_print_timings:       total time =   20848.07 ms /   770 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      47.94 ms /   309 runs   (    0.16 ms per token,  6445.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19517.60 ms /   672 tokens (   29.04 ms per token,    34.43 tokens per second)\n",
      "llama_print_timings:        eval time =   25812.57 ms /   308 runs   (   83.81 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   46080.08 ms /   980 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.38 ms /   155 runs   (    0.14 ms per token,  7248.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6660.43 ms /   317 tokens (   21.01 ms per token,    47.59 tokens per second)\n",
      "llama_print_timings:        eval time =   13636.97 ms /   154 runs   (   88.55 ms per token,    11.29 tokens per second)\n",
      "llama_print_timings:       total time =   20626.13 ms /   471 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.26 ms /   222 runs   (    0.14 ms per token,  7335.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11836.13 ms /   641 tokens (   18.47 ms per token,    54.16 tokens per second)\n",
      "llama_print_timings:        eval time =   16371.26 ms /   221 runs   (   74.08 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =   28688.35 ms /   862 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.60 ms /    75 runs   (    0.11 ms per token,  8721.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =  254617.47 ms /   617 tokens (  412.67 ms per token,     2.42 tokens per second)\n",
      "llama_print_timings:        eval time =   17695.59 ms /    74 runs   (  239.13 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =  272448.73 ms /   691 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    48 runs   (    0.10 ms per token,  9722.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13154.74 ms /   631 tokens (   20.85 ms per token,    47.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3301.43 ms /    47 runs   (   70.24 ms per token,    14.24 tokens per second)\n",
      "llama_print_timings:       total time =   16528.01 ms /   678 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    58 runs   (    0.09 ms per token, 10939.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11534.71 ms /   652 tokens (   17.69 ms per token,    56.53 tokens per second)\n",
      "llama_print_timings:        eval time =    4142.73 ms /    57 runs   (   72.68 ms per token,    13.76 tokens per second)\n",
      "llama_print_timings:       total time =   15766.00 ms /   709 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    65 runs   (    0.10 ms per token, 10276.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7210.35 ms /   319 tokens (   22.60 ms per token,    44.24 tokens per second)\n",
      "llama_print_timings:        eval time =    4674.50 ms /    64 runs   (   73.04 ms per token,    13.69 tokens per second)\n",
      "llama_print_timings:       total time =   11983.43 ms /   383 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.63 ms /    62 runs   (    0.09 ms per token, 11008.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4578.36 ms /   112 tokens (   40.88 ms per token,    24.46 tokens per second)\n",
      "llama_print_timings:        eval time =    4239.98 ms /    61 runs   (   69.51 ms per token,    14.39 tokens per second)\n",
      "llama_print_timings:       total time =    8980.20 ms /   173 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.98 ms /    65 runs   (    0.09 ms per token, 10878.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10189.50 ms /   611 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    6252.07 ms /    64 runs   (   97.69 ms per token,    10.24 tokens per second)\n",
      "llama_print_timings:       total time =   16543.20 ms /   675 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    74 runs   (    0.08 ms per token, 11796.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11134.78 ms /   620 tokens (   17.96 ms per token,    55.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5393.48 ms /    73 runs   (   73.88 ms per token,    13.53 tokens per second)\n",
      "llama_print_timings:       total time =   16638.18 ms /   693 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.46 ms /   289 runs   (    0.09 ms per token, 11351.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8588.71 ms /   457 tokens (   18.79 ms per token,    53.21 tokens per second)\n",
      "llama_print_timings:        eval time =   20056.00 ms /   288 runs   (   69.64 ms per token,    14.36 tokens per second)\n",
      "llama_print_timings:       total time =   29202.93 ms /   745 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.18 ms /   195 runs   (    0.09 ms per token, 11350.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27564.13 ms /  1835 tokens (   15.02 ms per token,    66.57 tokens per second)\n",
      "llama_print_timings:        eval time =   14635.03 ms /   194 runs   (   75.44 ms per token,    13.26 tokens per second)\n",
      "llama_print_timings:       total time =   42606.95 ms /  2029 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      44.19 ms /   512 runs   (    0.09 ms per token, 11585.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32109.14 ms /  2224 tokens (   14.44 ms per token,    69.26 tokens per second)\n",
      "llama_print_timings:        eval time =   39646.86 ms /   511 runs   (   77.59 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:       total time =   72788.62 ms /  2735 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.34 ms /    81 runs   (    0.09 ms per token, 11035.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11577.95 ms /   632 tokens (   18.32 ms per token,    54.59 tokens per second)\n",
      "llama_print_timings:        eval time =    5554.92 ms /    80 runs   (   69.44 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   17254.86 ms /   712 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      41.04 ms /   469 runs   (    0.09 ms per token, 11426.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33433.22 ms /  2287 tokens (   14.62 ms per token,    68.41 tokens per second)\n",
      "llama_print_timings:        eval time =   36890.22 ms /   468 runs   (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =   71063.98 ms /  2755 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.43 ms /   284 runs   (    0.09 ms per token, 11625.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18199.67 ms /  1250 tokens (   14.56 ms per token,    68.68 tokens per second)\n",
      "llama_print_timings:        eval time =   20557.81 ms /   283 runs   (   72.64 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =   39178.53 ms /  1533 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.06 ms /    58 runs   (    0.09 ms per token, 11460.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14323.50 ms /   633 tokens (   22.63 ms per token,    44.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4700.95 ms /    57 runs   (   82.47 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:       total time =   19110.26 ms /   690 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.52 ms /   256 runs   (    0.08 ms per token, 11898.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33517.38 ms /  2229 tokens (   15.04 ms per token,    66.50 tokens per second)\n",
      "llama_print_timings:        eval time =   19781.35 ms /   255 runs   (   77.57 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:       total time =   53692.01 ms /  2484 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    75 runs   (    0.08 ms per token, 11935.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11436.63 ms /   633 tokens (   18.07 ms per token,    55.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5323.34 ms /    74 runs   (   71.94 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =   16869.32 ms /   707 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      43.33 ms /   512 runs   (    0.08 ms per token, 11817.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32971.68 ms /  2215 tokens (   14.89 ms per token,    67.18 tokens per second)\n",
      "llama_print_timings:        eval time =   41283.54 ms /   511 runs   (   80.79 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =   75074.84 ms /  2726 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.85 ms /   208 runs   (    0.09 ms per token, 11650.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   29522.77 ms /  2036 tokens (   14.50 ms per token,    68.96 tokens per second)\n",
      "llama_print_timings:        eval time =   16579.72 ms /   207 runs   (   80.10 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =   46426.68 ms /  2243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.53 ms /   146 runs   (    0.09 ms per token, 11653.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25602.39 ms /  1753 tokens (   14.60 ms per token,    68.47 tokens per second)\n",
      "llama_print_timings:        eval time =   22452.25 ms /   145 runs   (  154.84 ms per token,     6.46 tokens per second)\n",
      "llama_print_timings:       total time =   48315.78 ms /  1898 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.02 ms /    81 runs   (    0.09 ms per token, 11531.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11838.26 ms /   632 tokens (   18.73 ms per token,    53.39 tokens per second)\n",
      "llama_print_timings:        eval time =    5719.91 ms /    80 runs   (   71.50 ms per token,    13.99 tokens per second)\n",
      "llama_print_timings:       total time =   17678.57 ms /   712 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.57 ms /   355 runs   (    0.09 ms per token, 11610.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   26259.52 ms /  1755 tokens (   14.96 ms per token,    66.83 tokens per second)\n",
      "llama_print_timings:        eval time =   26387.42 ms /   354 runs   (   74.54 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   53194.01 ms /  2109 tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"0. The model's response is not related to the context provided, which discusses a scenario in a fantasy world with various magical items, enemies, and choices for the character. The model's response focuses on the combination of two specific magical items and their potential uses, but this information is not supported by the context.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Format metrics and save it\u001b[39;00m\n\u001b[1;32m    114\u001b[0m df_hrr_mrr \u001b[38;5;241m=\u001b[39m compute_hit_hrr_results(eval_results_dict)\n\u001b[0;32m--> 115\u001b[0m df_other_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_correctness_relevancy_answer_relevancy_faithfulness_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_results_dict_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df_hrr_mrr, df_other_metrics, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretriever_name\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    117\u001b[0m df_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes_nbr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(nodes)\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mcompute_correctness_relevancy_answer_relevancy_faithfulness_results\u001b[0;34m(eval_results_dict)\u001b[0m\n\u001b[1;32m      3\u001b[0m full_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, eval_results \u001b[38;5;129;01min\u001b[39;00m eval_results_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 5\u001b[0m     faithfulness_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMy score = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43meval_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfaithfulness\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     relevancy_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(result\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m     correctness_score \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28msum\u001b[39m(result\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Max = 5 points\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m full_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, eval_results \u001b[38;5;129;01min\u001b[39;00m eval_results_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m----> 5\u001b[0m     faithfulness_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeedback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMy score = \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfaithfulness\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m     relevancy_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(result\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelevancy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m     correctness_score \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28msum\u001b[39m(result\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(eval_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcorrectness\u001b[39m\u001b[38;5;124m'\u001b[39m])) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Max = 5 points\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: \"0. The model's response is not related to the context provided, which discusses a scenario in a fantasy world with various magical items, enemies, and choices for the character. The model's response focuses on the combination of two specific magical items and their potential uses, but this information is not supported by the context.\""
     ]
    }
   ],
   "source": [
    "eval_results_dict = {}\n",
    "eval_results_dict_2 = {}\n",
    "questions_per_chunk = 1\n",
    "output_name = \"MISQ4KM_bge_small_re2\"\n",
    "\n",
    "for parser_name, parser in parsers.items():\n",
    "    print(f\"\\n{parser_name}\\n\")\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    Settings.llm = llm_mixtral\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    qa_dataset = generate_question_context_pairs(\n",
    "        nodes,\n",
    "        llm=llm_mixtral,\n",
    "        num_questions_per_chunk=questions_per_chunk\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "    # Create retriever evaluator\n",
    "    # It evaluates a retriever using a set of metrics. (here : hit rate, is the correct context among the retrieved ones\n",
    "    # and MRR, how well the correct context is positioned among retrieved contexts\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names([\"mrr\", \"hit_rate\"], retriever=retriever)\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)  # Can't put this line in a function otherwise it raises an error\n",
    "    eval_results_dict[parser_name] = eval_results\n",
    "\n",
    "    # Create query engine\n",
    "    query_engine = get_query_engine(sentence_index=vector_index, similarity_top_k=3, rerank_top_n=2)\n",
    "\n",
    "    # Get queries created from Mixtral\n",
    "    queries = list(qa_dataset.queries.values())\n",
    "\n",
    "    # Compute faithfulness evaluation\n",
    "    # Evaluates whether a response is faithful to the contexts\n",
    "    # (i.e. whether the response is supported by the contexts or hallucinated.)\n",
    "    # For faithfulness score the base prompt had to be changed since it did not respect the instructions given and so the scores were 90% of the time at None or something with apple\n",
    "    # because of the base preprompt\n",
    "    faithfulness_custom_template = PromptTemplate(\n",
    "    \"\"\"In this task, you will act as a faithfulness evaluator for a language model's responses. Your job is to determine whether the model's response is faithful to the given context, i.e. \n",
    "    whether the response is supported by the context or if it is hallucinated.\n",
    "\n",
    "    Here are the criteria for your evaluation:\n",
    "\n",
    "    * If the response is not supported by the context, you should give it a score of 0.\n",
    "    * If the response is fully supported by the context, you should give it a score of 1.\n",
    "    * If the response is partially supported by the context, you may give it a score between 0 and 1, where a score closer to 1 indicates greater faithfulness to the context.\n",
    "\n",
    "    Here are two examples to help you understand the task:\n",
    "\n",
    "    Example 1:\n",
    "    Context: \"The capital of France is Paris.\"\n",
    "    Model response: \"The capital of France is Rome.\"\n",
    "    Faithfulness score: 0 (The response is not supported by the context, as the capital of France is Paris, not Rome.)\n",
    "\n",
    "    Example 2:\n",
    "    Context: \"The capital of France is Paris. The Eiffel Tower is a famous landmark in Paris.\"\n",
    "    Model response: \"The Eiffel Tower is a famous landmark in the capital of France.\"\n",
    "    Faithfulness score: 1 (The response is fully supported by the context, as the Eiffel Tower is indeed a famous landmark in Paris, which is the capital of France.)\n",
    "\n",
    "    Based on given context:\n",
    "    \\n\"{context_str}\"\\n\n",
    "\n",
    "    And the model answer :\n",
    "    \\n\"{query_str}\"\\n\n",
    "\n",
    "    Evaluate the faithfulness of the following model response. Put your score as \"My score = \\n\"\"\")\n",
    "    faithfulness_mixtral = FaithfulnessEvaluator(llm=llm_mixtral, eval_template=faithfulness_custom_template)\n",
    "\n",
    "    # Compute relevancy evaluation\n",
    "    # Evaluates the relevancy of retrieved contexts and response to a query.\n",
    "    # This evaluator considers the query string, retrieved contexts, and response string.\n",
    "    relevancy_mixtral = RelevancyEvaluator(llm=llm_mixtral)\n",
    "\n",
    "    # Compute answer relevancy evaluation\n",
    "    # Evaluates the relevancy of response to a query.\n",
    "    # This evaluator considers the query string and response string.\n",
    "    # Focuses on assessing how pertinent the generated answer is to the given prompt\n",
    "    # For answer relevancy score the base prompt had to be changed since it did not respect the instructions given and so the scores were 90% of the time at None\n",
    "    answer_relevancy_custom_template = PromptTemplate(\n",
    "    \"\"\"Your goal is to evaluate the answer relevancy of an other model to a question. You have to score the model's answer between 0 and 1. 1 meaning the \n",
    "    model perfectly answered the question, 0 meaning it does not at all answer the question. For example, to the question \"What is the capital of France?\" \n",
    "    The answer \"The capital of France is Paris.\" will have an answer relevancy score of 1, whereas the answer \"France is a country in Europe with many famous cities like Paris and Lyon.\" \n",
    "    will have an answer relevancy score of 0.5 and \"The capital of Spain is Madrid.\" will have an answer relevancy score of 0.\\n\\n\n",
    "\n",
    "    Based on this query :\n",
    "    \\n\"{query}\"\\n\n",
    "\n",
    "    And the model answer :\n",
    "    \\n\"{response}\"\\n\n",
    "\n",
    "    How would you rate the model answer relevancy to the question ? Put your score as \"My score = \\n\"\"\"\n",
    "    )\n",
    "    answer_relevancy_mixtral = AnswerRelevancyEvaluator(llm=llm_mixtral, eval_template=answer_relevancy_custom_template)\n",
    "\n",
    "    # Compute correctness evaluation\n",
    "    # Evaluate the relevance and correctness of a generated answer against a reference answer.\n",
    "    correctness_mixtral = CorrectnessEvaluator(llm=llm_mixtral)\n",
    "\n",
    "    # Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
    "    runner = BatchEvalRunner({\"faithfulness\": faithfulness_mixtral, \"relevancy\": relevancy_mixtral,\n",
    "                              \"answer_relevancy\": answer_relevancy_mixtral, \"correctness\": correctness_mixtral},\n",
    "                              workers=8)\n",
    "\n",
    "    # Compute evaluation\n",
    "    eval_results_2 = await runner.aevaluate_queries(query_engine, queries=queries)\n",
    "    eval_results_dict_2[parser_name] = eval_results_2\n",
    "\n",
    "    # Add number of nodes\n",
    "    eval_results_dict_2[parser_name][\"nodes_nbr\"] = len(nodes)\n",
    "    \n",
    "# Format metrics and save it\n",
    "df_hrr_mrr = compute_hit_hrr_results(eval_results_dict)\n",
    "df_other_metrics = compute_correctness_relevancy_answer_relevancy_faithfulness_results(eval_results_dict_2)\n",
    "df_results = pd.merge(df_hrr_mrr, df_other_metrics, on=\"retriever_name\")\n",
    "df_results[\"retriever_name\"] = df_results[\"retriever_name\"].apply(lambda x: x + \"_\" + output_name)\n",
    "df_results.to_csv(os.path.join(results_folder, output_name + \".csv\"), index=False)\n",
    "\n",
    "# Save full results\n",
    "with open(os.path.join(results_folder, output_name + \"_hrr_mrr_full_results.txt\"), \"w\") as f:\n",
    "    f.write(jsonpickle.encode(eval_results_dict))\n",
    "\n",
    "with open(os.path.join(results_folder, output_name + \"_other_metrics_full_results.txt\"), \"w\") as f:\n",
    "    f.write(jsonpickle.encode(eval_results_dict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results\n",
    "# Convert it to dataframe\n",
    "# Plot graphs\n",
    "# Test various embeddings, rerankings, model, parsers\n",
    "# Add testing time : make 10 queries and compute time spent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(folder_path: str, output_file: str):\n",
    "    # Get a list of files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # Filter only CSV files if needed\n",
    "    csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Iterate over each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Construct the full path to the CSV file\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Combine dataframes\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df[\"retriever_name\"] = combined_df[\"retriever_name\"].apply(lambda x: x.replace(\"semantic_splitter\", \"ssp\"))\n",
    "    combined_df[\"retriever_name\"] = combined_df[\"retriever_name\"].apply(lambda x: x.replace(\"token_splitter\", \"tsp\"))\n",
    "\n",
    "    # Convert metrics to percentage\n",
    "    metric_columns = ['hit_rate', 'mrr', 'faithfulness', 'relevancy', 'correctness', 'answer_relevancy', 'mean_score']\n",
    "    combined_df[metric_columns] *= 100\n",
    "\n",
    "    # Create figure with subplots for each metric\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Create subplots for each metric in a 2-column layout\n",
    "    fig = make_subplots(rows=math.ceil(len(metric_columns) / 2), cols=2, subplot_titles=[col.capitalize() for col in metric_columns])\n",
    "\n",
    "    for i, col in enumerate(metric_columns, start=1):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=combined_df['retriever_name'],\n",
    "                y=combined_df[col],\n",
    "                text=combined_df[col].round(2),  # Display values rounded to 2 decimal places as text on bars\n",
    "                textposition='auto',  # Automatically place text on bars\n",
    "                marker_color=px.colors.qualitative.Plotly[i-1],  # Use Plotly qualitative color palette\n",
    "                showlegend=False,  # Hide legend for individual plots\n",
    "            ),\n",
    "            row=(i + 1) // 2, col=(i % 2) + 1  # Place subplots in 2 columns per row\n",
    "        )\n",
    "\n",
    "    # Update layout for the entire figure\n",
    "    fig.update_layout(\n",
    "        height=1500,  # Adjust height as needed\n",
    "        width=1000,  # Adjust width as needed\n",
    "        title='Comparison of Metrics by Retriever Name',\n",
    "        xaxis_title='Retriever Name',\n",
    "        yaxis_title='Percentage',\n",
    "        yaxis_tickformat='.0f',  # Format y-axis ticks as integer\n",
    "    )\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    fig.write_html(output_file)\n",
    "\n",
    "    print(f\"Figure saved as HTML: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved as HTML: data_evaluation/batch_2/results/test.html\n"
     ]
    }
   ],
   "source": [
    "plot_results(folder_path=results_folder, output_file=os.path.join(results_folder, \"test.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mistralai.client import MistralClient\n",
    "# from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# api_key = input(\"Enter API key\")\n",
    "# model = \"open-mixtral-8x7b\"\n",
    "\n",
    "# client = MistralClient(api_key=api_key)\n",
    "\n",
    "# chat_response = client.chat(\n",
    "#     model=model,\n",
    "#     messages=[ChatMessage(role=\"user\", content=\"\"\"I want to evaluate an LLM's faithfulness. A faithfulness evaluator is defined as such : \"Evaluates whether a response is faithful to the \n",
    "#                           contexts (i.e. whether the response is supported by the contexts or hallucinated.). This evaluator only considers the response string and the list of context \n",
    "#                           strings. The score has to be between 0 and 1, 0 meaning the answer is not supported by context and so the modle hallucinated, 1 meaning the answer is fully supported\n",
    "#                           by the given context.\n",
    "#                           Based on given information, write me a preprompt, with at least two examples, to give to my model ?\"\"\")]\n",
    "# )\n",
    "\n",
    "# print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
