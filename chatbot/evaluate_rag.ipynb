{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import glob\n",
    "import base64\n",
    "import pandas as pd\n",
    "import jsonpickle\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter, SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, RetrieverEvaluator\n",
    "from llama_index.core.prompts import BasePromptTemplate, PromptTemplate\n",
    "from transformers import AutoTokenizer\n",
    "import nest_asyncio\n",
    "\n",
    "# To allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define constants\n",
    "results_folder = os.path.join(\"data_evaluation\", \"full_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correctness_relevancy_answer_relevancy_faithfulness_results(eval_results_dict: dict):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    full_df = pd.DataFrame()\n",
    "    for name, eval_results in eval_results_dict.items():\n",
    "        faithfulness_score = sum(float(result.feedback.split(\"My score = \")[1].split(\"\\n\")[0]) for result in eval_results['faithfulness']) / len(eval_results['faithfulness'])\n",
    "        relevancy_score = sum(result.score for result in eval_results['relevancy']) / len(eval_results['relevancy'])\n",
    "        correctness_score = (sum(result.score for result in eval_results['correctness']) / len(eval_results['correctness'])) / 5  # Max = 5 points\n",
    "        answer_relevancy_score = sum(float(result.feedback.split(\"My score = \")[1].split(\"\\n\")[0]) for result in eval_results['answer_relevancy']) / len(eval_results['answer_relevancy'])\n",
    "\n",
    "        metric_df = pd.DataFrame(\n",
    "            {\"retriever_name\": [name], \"faithfulness\": [faithfulness_score], \"relevancy\": [relevancy_score],\n",
    "             \"correctness\": [correctness_score], \"answer_relevancy\": [answer_relevancy_score] }\n",
    "        )\n",
    "\n",
    "        full_df = pd.concat([full_df, metric_df])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hit_hrr_results(eval_results_dict: dict):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    full_df = pd.DataFrame()\n",
    "    for name, eval_results in eval_results_dict.items():\n",
    "        metric_dicts = []\n",
    "        for eval_result in eval_results:\n",
    "            metric_dict = eval_result.metric_vals_dict\n",
    "            metric_dicts.append(metric_dict)\n",
    "\n",
    "        df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "        hit_rate = df[\"hit_rate\"].mean()\n",
    "        mrr = df[\"mrr\"].mean()\n",
    "\n",
    "        metric_df = pd.DataFrame(\n",
    "            {\"retriever_name\": [name], \"hit_rate\": [hit_rate], \"mrr\": [mrr]}\n",
    "        )\n",
    "\n",
    "        full_df = pd.concat([full_df, metric_df])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, ( 5258.59 / 10922.67)\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, ( 5579.59 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, ( 6139.61 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 3\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "input_folder = \"./data_evaluation/batch_1\"\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic_splitter \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.02 ms /   154 runs   (    0.08 ms per token, 12810.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18053.94 ms /   756 tokens (   23.88 ms per token,    41.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11712.03 ms /   153 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   29966.20 ms /   909 tokens\n",
      "  1%|          | 1/101 [00:29<49:57, 29.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    62 runs   (    0.08 ms per token, 12889.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2715.31 ms /   117 tokens (   23.21 ms per token,    43.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.40 ms /    61 runs   (   71.14 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    7136.51 ms /   178 tokens\n",
      "  2%|▏         | 2/101 [00:37<27:17, 16.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.21 ms /    99 runs   (    0.07 ms per token, 13736.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2374.18 ms /   117 tokens (   20.29 ms per token,    49.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6801.93 ms /    98 runs   (   69.41 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    9300.17 ms /   215 tokens\n",
      "  3%|▎         | 3/101 [00:46<21:37, 13.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.63 ms /    49 runs   (    0.07 ms per token, 13506.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2216.13 ms /    92 tokens (   24.09 ms per token,    41.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3145.71 ms /    48 runs   (   65.54 ms per token,    15.26 tokens per second)\n",
      "llama_print_timings:       total time =    5423.16 ms /   140 tokens\n",
      "  4%|▍         | 4/101 [00:51<16:25, 10.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.45 ms /    48 runs   (    0.07 ms per token, 13913.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2822.10 ms /   153 tokens (   18.45 ms per token,    54.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3074.91 ms /    47 runs   (   65.42 ms per token,    15.29 tokens per second)\n",
      "llama_print_timings:       total time =    5956.77 ms /   200 tokens\n",
      "  5%|▍         | 5/101 [00:57<13:49,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.06 ms /    75 runs   (    0.08 ms per token, 12374.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6111.74 ms /   452 tokens (   13.52 ms per token,    73.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5896.77 ms /    74 runs   (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =   12110.01 ms /   526 tokens\n",
      "  6%|▌         | 6/101 [01:09<15:33,  9.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.59 ms /    91 runs   (    0.08 ms per token, 11986.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4366.06 ms /   192 tokens (   22.74 ms per token,    43.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9057.28 ms /    90 runs   (  100.64 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time =   13555.55 ms /   282 tokens\n",
      "  7%|▋         | 7/101 [01:23<17:18, 11.05s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.03 ms /   170 runs   (    0.08 ms per token, 13045.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3868.34 ms /   269 tokens (   14.38 ms per token,    69.54 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.56 ms /   169 runs   (   66.83 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =   15380.27 ms /   438 tokens\n",
      "  8%|▊         | 8/101 [01:38<19:15, 12.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.73 ms /    64 runs   (    0.07 ms per token, 13536.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8644.96 ms /   624 tokens (   13.85 ms per token,    72.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4350.16 ms /    63 runs   (   69.05 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time =   13076.46 ms /   687 tokens\n",
      "  9%|▉         | 9/101 [01:51<19:22, 12.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.09 ms /    68 runs   (    0.07 ms per token, 13351.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2853.48 ms /   146 tokens (   19.54 ms per token,    51.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4613.45 ms /    67 runs   (   68.86 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    7553.73 ms /   213 tokens\n",
      " 10%|▉         | 10/101 [01:59<16:47, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.57 ms /    68 runs   (    0.08 ms per token, 12210.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3304.26 ms /   178 tokens (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6464.45 ms /    67 runs   (   96.48 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =    9861.08 ms /   245 tokens\n",
      " 11%|█         | 11/101 [02:09<16:03, 10.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.03 ms /    87 runs   (    0.08 ms per token, 12380.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.52 ms /   300 tokens (   13.97 ms per token,    71.57 tokens per second)\n",
      "llama_print_timings:        eval time =    6623.54 ms /    86 runs   (   77.02 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =   10934.55 ms /   386 tokens\n",
      " 12%|█▏        | 12/101 [02:20<15:58, 10.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /   102 runs   (    0.08 ms per token, 13058.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8880.03 ms /   618 tokens (   14.37 ms per token,    69.59 tokens per second)\n",
      "llama_print_timings:        eval time =    7994.84 ms /   101 runs   (   79.16 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   17014.98 ms /   719 tokens\n",
      " 13%|█▎        | 13/101 [02:37<18:34, 12.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    78 runs   (    0.07 ms per token, 13605.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4438.62 ms /   333 tokens (   13.33 ms per token,    75.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5137.80 ms /    77 runs   (   66.72 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =    9675.69 ms /   410 tokens\n",
      " 14%|█▍        | 14/101 [02:47<17:03, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      17.00 ms /   229 runs   (    0.07 ms per token, 13466.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8395.27 ms /   599 tokens (   14.02 ms per token,    71.35 tokens per second)\n",
      "llama_print_timings:        eval time =   15630.23 ms /   228 runs   (   68.55 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =   24327.05 ms /   827 tokens\n",
      " 15%|█▍        | 15/101 [03:11<22:17, 15.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    88 runs   (    0.07 ms per token, 13645.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9384.82 ms /   611 tokens (   15.36 ms per token,    65.11 tokens per second)\n",
      "llama_print_timings:        eval time =    6306.25 ms /    87 runs   (   72.49 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   15809.17 ms /   698 tokens\n",
      " 16%|█▌        | 16/101 [03:27<22:08, 15.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.11 ms /    81 runs   (    0.08 ms per token, 13250.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4627.50 ms /   339 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5908.62 ms /    80 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =   10644.06 ms /   419 tokens\n",
      " 17%|█▋        | 17/101 [03:37<19:47, 14.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    70 runs   (    0.08 ms per token, 12544.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9340.30 ms /   590 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5598.02 ms /    69 runs   (   81.13 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   15038.52 ms /   659 tokens\n",
      " 18%|█▊        | 18/101 [03:52<19:55, 14.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.06 ms /    63 runs   (    0.08 ms per token, 12457.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6364.06 ms /   493 tokens (   12.91 ms per token,    77.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5290.84 ms /    62 runs   (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_print_timings:       total time =   11742.35 ms /   555 tokens\n",
      " 19%|█▉        | 19/101 [04:04<18:36, 13.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    69 runs   (    0.08 ms per token, 12062.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3179.33 ms /   174 tokens (   18.27 ms per token,    54.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.85 ms /    68 runs   (   85.48 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =    9091.19 ms /   242 tokens\n",
      " 20%|█▉        | 20/101 [04:13<16:32, 12.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    90 runs   (    0.08 ms per token, 12710.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3654.85 ms /   195 tokens (   18.74 ms per token,    53.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9051.43 ms /    89 runs   (  101.70 ms per token,     9.83 tokens per second)\n",
      "llama_print_timings:       total time =   12829.68 ms /   284 tokens\n",
      " 21%|██        | 21/101 [04:26<16:34, 12.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    68 runs   (    0.07 ms per token, 13570.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12968.38 ms /   907 tokens (   14.30 ms per token,    69.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.62 ms /    67 runs   (   74.31 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =   18040.43 ms /   974 tokens\n",
      " 22%|██▏       | 22/101 [04:44<18:35, 14.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.82 ms /    77 runs   (    0.08 ms per token, 13239.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.10 ms /   182 tokens (   16.52 ms per token,    60.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5091.00 ms /    76 runs   (   66.99 ms per token,    14.93 tokens per second)\n",
      "llama_print_timings:       total time =    8197.42 ms /   258 tokens\n",
      " 23%|██▎       | 23/101 [04:52<16:02, 12.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.13 ms /    78 runs   (    0.08 ms per token, 12718.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3497.20 ms /   218 tokens (   16.04 ms per token,    62.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5649.99 ms /    77 runs   (   73.38 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =    9253.89 ms /   295 tokens\n",
      " 24%|██▍       | 24/101 [05:02<14:39, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.26 ms /    94 runs   (    0.08 ms per token, 12945.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11297.81 ms /   836 tokens (   13.51 ms per token,    74.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7583.21 ms /    93 runs   (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =   19017.22 ms /   929 tokens\n",
      " 25%|██▍       | 25/101 [05:21<17:21, 13.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.36 ms /    92 runs   (    0.08 ms per token, 12505.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5946.19 ms /   456 tokens (   13.04 ms per token,    76.69 tokens per second)\n",
      "llama_print_timings:        eval time =    7150.57 ms /    91 runs   (   78.58 ms per token,    12.73 tokens per second)\n",
      "llama_print_timings:       total time =   13232.47 ms /   547 tokens\n",
      " 26%|██▌       | 26/101 [05:34<16:57, 13.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    61 runs   (    0.09 ms per token, 11344.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.48 ms /   267 tokens (   15.24 ms per token,    65.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4890.34 ms /    60 runs   (   81.51 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    9049.37 ms /   327 tokens\n",
      " 27%|██▋       | 27/101 [05:43<15:03, 12.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    71 runs   (    0.08 ms per token, 12384.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2823.14 ms /   142 tokens (   19.88 ms per token,    50.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5084.73 ms /    70 runs   (   72.64 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    8004.18 ms /   212 tokens\n",
      " 28%|██▊       | 28/101 [05:51<13:19, 10.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    76 runs   (    0.07 ms per token, 13615.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6649.52 ms /   516 tokens (   12.89 ms per token,    77.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5149.99 ms /    75 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =   11897.63 ms /   591 tokens\n",
      " 29%|██▊       | 29/101 [06:03<13:29, 11.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    72 runs   (    0.07 ms per token, 13748.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3582.96 ms /   254 tokens (   14.11 ms per token,    70.89 tokens per second)\n",
      "llama_print_timings:        eval time =    4683.86 ms /    71 runs   (   65.97 ms per token,    15.16 tokens per second)\n",
      "llama_print_timings:       total time =    8357.64 ms /   325 tokens\n",
      " 30%|██▉       | 30/101 [06:11<12:16, 10.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    55 runs   (    0.07 ms per token, 13657.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5613.61 ms /   454 tokens (   12.36 ms per token,    80.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3629.55 ms /    54 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    9312.68 ms /   508 tokens\n",
      " 31%|███       | 31/101 [06:21<11:43, 10.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    54 runs   (    0.07 ms per token, 13650.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2506.56 ms /   118 tokens (   21.24 ms per token,    47.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3451.56 ms /    53 runs   (   65.12 ms per token,    15.36 tokens per second)\n",
      "llama_print_timings:       total time =    6024.10 ms /   171 tokens\n",
      " 32%|███▏      | 32/101 [06:27<10:10,  8.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    84 runs   (    0.07 ms per token, 14098.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10013.06 ms /   766 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5747.01 ms /    83 runs   (   69.24 ms per token,    14.44 tokens per second)\n",
      "llama_print_timings:       total time =   15868.22 ms /   849 tokens\n",
      " 33%|███▎      | 33/101 [06:42<12:24, 10.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /   108 runs   (    0.07 ms per token, 13966.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4652.02 ms /   353 tokens (   13.18 ms per token,    75.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7134.48 ms /   107 runs   (   66.68 ms per token,    15.00 tokens per second)\n",
      "llama_print_timings:       total time =   11923.06 ms /   460 tokens\n",
      " 34%|███▎      | 34/101 [06:54<12:33, 11.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    63 runs   (    0.08 ms per token, 13174.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2683.18 ms /   129 tokens (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4030.98 ms /    62 runs   (   65.02 ms per token,    15.38 tokens per second)\n",
      "llama_print_timings:       total time =    6792.14 ms /   191 tokens\n",
      " 35%|███▍      | 35/101 [07:01<10:54,  9.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    64 runs   (    0.07 ms per token, 13468.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3034.21 ms /   188 tokens (   16.14 ms per token,    61.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4119.95 ms /    63 runs   (   65.40 ms per token,    15.29 tokens per second)\n",
      "llama_print_timings:       total time =    7235.23 ms /   251 tokens\n",
      " 36%|███▌      | 36/101 [07:08<09:52,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    55 runs   (    0.07 ms per token, 13681.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15335.77 ms /  1152 tokens (   13.31 ms per token,    75.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3812.76 ms /    54 runs   (   70.61 ms per token,    14.16 tokens per second)\n",
      "llama_print_timings:       total time =   19221.02 ms /  1206 tokens\n",
      " 37%|███▋      | 37/101 [07:28<12:57, 12.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    85 runs   (    0.08 ms per token, 13032.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27712.60 ms /  2065 tokens (   13.42 ms per token,    74.51 tokens per second)\n",
      "llama_print_timings:        eval time =    7479.48 ms /    84 runs   (   89.04 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   35309.09 ms /  2149 tokens\n",
      " 38%|███▊      | 38/101 [08:03<20:03, 19.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    95 runs   (    0.08 ms per token, 13036.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11346.44 ms /   716 tokens (   15.85 ms per token,    63.10 tokens per second)\n",
      "llama_print_timings:        eval time =    6569.49 ms /    94 runs   (   69.89 ms per token,    14.31 tokens per second)\n",
      "llama_print_timings:       total time =   18041.89 ms /   810 tokens\n",
      " 39%|███▊      | 39/101 [08:21<19:24, 18.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.60 ms /    62 runs   (    0.07 ms per token, 13478.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2126.61 ms /    85 tokens (   25.02 ms per token,    39.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3956.54 ms /    61 runs   (   64.86 ms per token,    15.42 tokens per second)\n",
      "llama_print_timings:       total time =    6161.03 ms /   146 tokens\n",
      " 40%|███▉      | 40/101 [08:27<15:14, 15.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    52 runs   (    0.08 ms per token, 13147.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2756.61 ms /   133 tokens (   20.73 ms per token,    48.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3330.74 ms /    51 runs   (   65.31 ms per token,    15.31 tokens per second)\n",
      "llama_print_timings:       total time =    6152.98 ms /   184 tokens\n",
      " 41%|████      | 41/101 [08:33<12:20, 12.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    68 runs   (    0.08 ms per token, 12705.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2720.64 ms /   137 tokens (   19.86 ms per token,    50.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4464.39 ms /    67 runs   (   66.63 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7271.58 ms /   204 tokens\n",
      " 42%|████▏     | 42/101 [08:41<10:38, 10.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    72 runs   (    0.07 ms per token, 14299.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10358.97 ms /   759 tokens (   13.65 ms per token,    73.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4931.52 ms /    71 runs   (   69.46 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   15384.65 ms /   830 tokens\n",
      " 43%|████▎     | 43/101 [08:56<11:47, 12.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.71 ms /   101 runs   (    0.08 ms per token, 13101.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9635.84 ms /   697 tokens (   13.82 ms per token,    72.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6873.34 ms /   100 runs   (   68.73 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   16639.64 ms /   797 tokens\n",
      " 44%|████▎     | 44/101 [09:13<12:51, 13.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    69 runs   (    0.07 ms per token, 13487.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2337.28 ms /    79 tokens (   29.59 ms per token,    33.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4428.89 ms /    68 runs   (   65.13 ms per token,    15.35 tokens per second)\n",
      "llama_print_timings:       total time =    6854.89 ms /   147 tokens\n",
      " 45%|████▍     | 45/101 [09:19<10:45, 11.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.87 ms /    76 runs   (    0.08 ms per token, 12958.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2680.85 ms /   145 tokens (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4884.47 ms /    75 runs   (   65.13 ms per token,    15.35 tokens per second)\n",
      "llama_print_timings:       total time =    7661.71 ms /   220 tokens\n",
      " 46%|████▌     | 46/101 [09:27<09:30, 10.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    85 runs   (    0.07 ms per token, 13571.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4267.50 ms /   304 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_print_timings:        eval time =    5564.57 ms /    84 runs   (   66.24 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    9940.29 ms /   388 tokens\n",
      " 47%|████▋     | 47/101 [09:37<09:13, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    90 runs   (    0.07 ms per token, 13499.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4733.17 ms /   358 tokens (   13.22 ms per token,    75.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5987.77 ms /    89 runs   (   67.28 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =   10834.85 ms /   447 tokens\n",
      " 48%|████▊     | 48/101 [09:48<09:12, 10.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    68 runs   (    0.07 ms per token, 13798.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4540.02 ms /   347 tokens (   13.08 ms per token,    76.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4442.06 ms /    67 runs   (   66.30 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:       total time =    9068.50 ms /   414 tokens\n",
      " 49%|████▊     | 49/101 [09:57<08:40, 10.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.39 ms /   132 runs   (    0.07 ms per token, 14056.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9243.09 ms /   662 tokens (   13.96 ms per token,    71.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9025.65 ms /   131 runs   (   68.90 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =   18440.69 ms /   793 tokens\n",
      " 50%|████▉     | 50/101 [10:15<10:39, 12.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.03 ms /    40 runs   (    0.08 ms per token, 13214.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2379.99 ms /    80 tokens (   29.75 ms per token,    33.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2531.40 ms /    39 runs   (   64.91 ms per token,    15.41 tokens per second)\n",
      "llama_print_timings:       total time =    4962.16 ms /   119 tokens\n",
      " 50%|█████     | 51/101 [10:20<08:33, 10.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    66 runs   (    0.07 ms per token, 13678.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2347.44 ms /   107 tokens (   21.94 ms per token,    45.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4224.37 ms /    65 runs   (   64.99 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:       total time =    6655.26 ms /   172 tokens\n",
      " 51%|█████▏    | 52/101 [10:27<07:30,  9.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.95 ms /    97 runs   (    0.07 ms per token, 13952.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3592.46 ms /   229 tokens (   15.69 ms per token,    63.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6353.25 ms /    96 runs   (   66.18 ms per token,    15.11 tokens per second)\n",
      "llama_print_timings:       total time =   10068.10 ms /   325 tokens\n",
      " 52%|█████▏    | 53/101 [10:37<07:33,  9.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.16 ms /    71 runs   (    0.07 ms per token, 13759.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15779.51 ms /  1142 tokens (   13.82 ms per token,    72.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4944.96 ms /    70 runs   (   70.64 ms per token,    14.16 tokens per second)\n",
      "llama_print_timings:       total time =   20817.61 ms /  1212 tokens\n",
      " 53%|█████▎    | 54/101 [10:58<10:04, 12.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    66 runs   (    0.07 ms per token, 13496.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3347.24 ms /   206 tokens (   16.25 ms per token,    61.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4393.87 ms /    65 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    7825.99 ms /   271 tokens\n",
      " 54%|█████▍    | 55/101 [11:06<08:42, 11.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.88 ms /    79 runs   (    0.07 ms per token, 13430.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4533.76 ms /   334 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_print_timings:        eval time =    5193.70 ms /    78 runs   (   66.59 ms per token,    15.02 tokens per second)\n",
      "llama_print_timings:       total time =    9828.69 ms /   412 tokens\n",
      " 55%|█████▌    | 56/101 [11:16<08:10, 10.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    73 runs   (    0.07 ms per token, 13729.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6682.82 ms /   516 tokens (   12.95 ms per token,    77.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4880.73 ms /    72 runs   (   67.79 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =   11658.46 ms /   588 tokens\n",
      " 56%|█████▋    | 57/101 [11:27<08:09, 11.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    68 runs   (    0.07 ms per token, 13500.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4284.06 ms /   311 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4445.35 ms /    67 runs   (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    8814.72 ms /   378 tokens\n",
      " 57%|█████▋    | 58/101 [11:36<07:28, 10.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    45 runs   (    0.07 ms per token, 13521.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2425.11 ms /   100 tokens (   24.25 ms per token,    41.24 tokens per second)\n",
      "llama_print_timings:        eval time =    2858.54 ms /    44 runs   (   64.97 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:       total time =    5340.26 ms /   144 tokens\n",
      " 58%|█████▊    | 59/101 [11:41<06:14,  8.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    63 runs   (    0.07 ms per token, 13840.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10063.49 ms /   762 tokens (   13.21 ms per token,    75.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4284.13 ms /    62 runs   (   69.10 ms per token,    14.47 tokens per second)\n",
      "llama_print_timings:       total time =   14429.31 ms /   824 tokens\n",
      " 59%|█████▉    | 60/101 [11:56<07:13, 10.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.83 ms /    75 runs   (    0.08 ms per token, 12862.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3070.44 ms /   166 tokens (   18.50 ms per token,    54.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4842.31 ms /    74 runs   (   65.44 ms per token,    15.28 tokens per second)\n",
      "llama_print_timings:       total time =    8008.11 ms /   240 tokens\n",
      " 60%|██████    | 61/101 [12:04<06:31,  9.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    63 runs   (    0.07 ms per token, 13639.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6585.83 ms /   515 tokens (   12.79 ms per token,    78.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4194.18 ms /    62 runs   (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time =   10861.43 ms /   577 tokens\n",
      " 61%|██████▏   | 62/101 [12:15<06:34, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    72 runs   (    0.08 ms per token, 12598.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8551.43 ms /   573 tokens (   14.92 ms per token,    67.01 tokens per second)\n",
      "llama_print_timings:        eval time =   70224.48 ms /    71 runs   (  989.08 ms per token,     1.01 tokens per second)\n",
      "llama_print_timings:       total time =   78887.62 ms /   644 tokens\n",
      " 62%|██████▏   | 63/101 [13:34<19:28, 30.75s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.56 ms /    88 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9738.12 ms /   729 tokens (   13.36 ms per token,    74.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5999.55 ms /    87 runs   (   68.96 ms per token,    14.50 tokens per second)\n",
      "llama_print_timings:       total time =   15852.73 ms /   816 tokens\n",
      " 63%|██████▎   | 64/101 [13:49<16:12, 26.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    54 runs   (    0.07 ms per token, 13636.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5696.74 ms /   453 tokens (   12.58 ms per token,    79.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3678.43 ms /    53 runs   (   69.40 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    9444.78 ms /   506 tokens\n",
      " 64%|██████▍   | 65/101 [13:59<12:44, 21.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.69 ms /   107 runs   (    0.07 ms per token, 13923.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14913.51 ms /  1076 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =    7521.71 ms /   106 runs   (   70.96 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =   22574.97 ms /  1182 tokens\n",
      " 65%|██████▌   | 66/101 [14:21<12:37, 21.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.64 ms /    61 runs   (    0.08 ms per token, 13140.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3448.49 ms /   218 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =    3962.92 ms /    60 runs   (   66.05 ms per token,    15.14 tokens per second)\n",
      "llama_print_timings:       total time =    7488.64 ms /   278 tokens\n",
      " 66%|██████▋   | 67/101 [14:29<09:51, 17.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    54 runs   (    0.07 ms per token, 13892.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2358.95 ms /   124 tokens (   19.02 ms per token,    52.57 tokens per second)\n",
      "llama_print_timings:        eval time =    3441.22 ms /    53 runs   (   64.93 ms per token,    15.40 tokens per second)\n",
      "llama_print_timings:       total time =    5868.16 ms /   177 tokens\n",
      " 67%|██████▋   | 68/101 [14:35<07:39, 13.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.70 ms /    64 runs   (    0.07 ms per token, 13611.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2971.13 ms /   167 tokens (   17.79 ms per token,    56.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4106.99 ms /    63 runs   (   65.19 ms per token,    15.34 tokens per second)\n",
      "llama_print_timings:       total time =    7158.34 ms /   230 tokens\n",
      " 68%|██████▊   | 69/101 [14:42<06:20, 11.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.44 ms /   158 runs   (    0.07 ms per token, 13812.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10063.91 ms /   739 tokens (   13.62 ms per token,    73.43 tokens per second)\n",
      "llama_print_timings:        eval time =   10837.10 ms /   157 runs   (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   21108.87 ms /   896 tokens\n",
      " 69%|██████▉   | 70/101 [15:03<07:34, 14.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    80 runs   (    0.07 ms per token, 13665.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8523.91 ms /   593 tokens (   14.37 ms per token,    69.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5372.03 ms /    79 runs   (   68.00 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =   13998.87 ms /   672 tokens\n",
      " 70%|███████   | 71/101 [15:17<07:14, 14.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    87 runs   (    0.07 ms per token, 13644.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11457.66 ms /   897 tokens (   12.77 ms per token,    78.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5998.21 ms /    86 runs   (   69.75 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =   17568.60 ms /   983 tokens\n",
      " 71%|███████▏  | 72/101 [15:35<07:26, 15.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    74 runs   (    0.07 ms per token, 13915.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5639.66 ms /   473 tokens (   11.92 ms per token,    83.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5956.81 ms /    73 runs   (   81.60 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =   11692.81 ms /   546 tokens\n",
      " 72%|███████▏  | 73/101 [15:46<06:40, 14.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /   132 runs   (    0.07 ms per token, 14369.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  356557.04 ms /   244 tokens ( 1461.30 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:        eval time =    8954.80 ms /   131 runs   (   68.36 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =  365678.39 ms /   375 tokens\n",
      " 73%|███████▎  | 74/101 [21:52<53:52, 119.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.75 ms /   149 runs   (    0.08 ms per token, 12676.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11508.01 ms /   859 tokens (   13.40 ms per token,    74.64 tokens per second)\n",
      "llama_print_timings:        eval time =   11060.66 ms /   148 runs   (   74.73 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =   22766.81 ms /  1007 tokens\n",
      " 74%|███████▍  | 75/101 [22:15<39:16, 90.63s/it] Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.42 ms /    61 runs   (    0.07 ms per token, 13816.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4074.67 ms /   303 tokens (   13.45 ms per token,    74.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4168.39 ms /    60 runs   (   69.47 ms per token,    14.39 tokens per second)\n",
      "llama_print_timings:       total time =    8319.99 ms /   363 tokens\n",
      " 75%|███████▌  | 76/101 [22:23<27:28, 65.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    61 runs   (    0.09 ms per token, 11570.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24307.61 ms /  1584 tokens (   15.35 ms per token,    65.16 tokens per second)\n",
      "llama_print_timings:        eval time =   16621.23 ms /    60 runs   (  277.02 ms per token,     3.61 tokens per second)\n",
      "llama_print_timings:       total time =   41044.39 ms /  1644 tokens\n",
      " 76%|███████▌  | 77/101 [23:06<23:34, 58.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.61 ms /   179 runs   (    0.09 ms per token, 11468.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7013.49 ms /   506 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =   18646.15 ms /   178 runs   (  104.75 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =   25931.07 ms /   684 tokens\n",
      " 77%|███████▋  | 78/101 [23:32<18:48, 49.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /    38 runs   (    0.09 ms per token, 11695.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2525.99 ms /    99 tokens (   25.52 ms per token,    39.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3814.23 ms /    37 runs   (  103.09 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =    6401.95 ms /   136 tokens\n",
      " 78%|███████▊  | 79/101 [23:38<13:17, 36.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    71 runs   (    0.08 ms per token, 12434.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15360.18 ms /  1019 tokens (   15.07 ms per token,    66.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5798.19 ms /    70 runs   (   82.83 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   21262.04 ms /  1089 tokens\n",
      " 79%|███████▉  | 80/101 [23:59<11:06, 31.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.72 ms /    83 runs   (    0.08 ms per token, 12356.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4700.67 ms /   335 tokens (   14.03 ms per token,    71.27 tokens per second)\n",
      "llama_print_timings:        eval time =    6717.41 ms /    82 runs   (   81.92 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   11533.44 ms /   417 tokens\n",
      " 80%|████████  | 81/101 [24:11<08:33, 25.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.15 ms /    89 runs   (    0.08 ms per token, 12452.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17058.64 ms /  1161 tokens (   14.69 ms per token,    68.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7136.93 ms /    88 runs   (   81.10 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   24328.10 ms /  1249 tokens\n",
      " 81%|████████  | 82/101 [24:35<08:00, 25.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    82 runs   (    0.08 ms per token, 13022.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25919.98 ms /  1758 tokens (   14.74 ms per token,    67.82 tokens per second)\n",
      "llama_print_timings:        eval time =    6725.02 ms /    81 runs   (   83.02 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   32769.70 ms /  1839 tokens\n",
      " 82%|████████▏ | 83/101 [25:08<08:15, 27.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    50 runs   (    0.08 ms per token, 12623.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2859.20 ms /   121 tokens (   23.63 ms per token,    42.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4765.58 ms /    49 runs   (   97.26 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =    7714.10 ms /   170 tokens\n",
      " 83%|████████▎ | 84/101 [25:16<06:07, 21.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.73 ms /    86 runs   (    0.08 ms per token, 12782.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12521.34 ms /   844 tokens (   14.84 ms per token,    67.40 tokens per second)\n",
      "llama_print_timings:        eval time =    6325.04 ms /    85 runs   (   74.41 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   18967.12 ms /   929 tokens\n",
      " 84%|████████▍ | 85/101 [25:35<05:32, 20.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      18.30 ms /   231 runs   (    0.08 ms per token, 12623.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5423.32 ms /   442 tokens (   12.27 ms per token,    81.50 tokens per second)\n",
      "llama_print_timings:        eval time =   17240.24 ms /   230 runs   (   74.96 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =   22977.78 ms /   672 tokens\n",
      " 85%|████████▌ | 86/101 [25:58<05:21, 21.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.55 ms /   120 runs   (    0.08 ms per token, 12560.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3063.80 ms /   188 tokens (   16.30 ms per token,    61.36 tokens per second)\n",
      "llama_print_timings:        eval time =    8128.14 ms /   119 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   11345.97 ms /   307 tokens\n",
      " 86%|████████▌ | 87/101 [26:09<04:17, 18.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    82 runs   (    0.07 ms per token, 14238.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8081.68 ms /   566 tokens (   14.28 ms per token,    70.03 tokens per second)\n",
      "llama_print_timings:        eval time =    5899.44 ms /    81 runs   (   72.83 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time =   14082.55 ms /   647 tokens\n",
      " 87%|████████▋ | 88/101 [26:23<03:42, 17.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    83 runs   (    0.07 ms per token, 13828.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5635.79 ms /   417 tokens (   13.52 ms per token,    73.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6015.82 ms /    82 runs   (   73.36 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   11757.59 ms /   499 tokens\n",
      " 88%|████████▊ | 89/101 [26:35<03:06, 15.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    86 runs   (    0.07 ms per token, 14266.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4664.00 ms /   379 tokens (   12.31 ms per token,    81.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5959.27 ms /    85 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   10729.90 ms /   464 tokens\n",
      " 89%|████████▉ | 90/101 [26:46<02:34, 14.08s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    67 runs   (    0.08 ms per token, 13214.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2893.83 ms /   169 tokens (   17.12 ms per token,    58.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4405.55 ms /    66 runs   (   66.75 ms per token,    14.98 tokens per second)\n",
      "llama_print_timings:       total time =    7389.63 ms /   235 tokens\n",
      " 90%|█████████ | 91/101 [26:53<02:00, 12.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.08 ms /   125 runs   (    0.07 ms per token, 13768.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11879.31 ms /   947 tokens (   12.54 ms per token,    79.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9665.86 ms /   124 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   21711.44 ms /  1071 tokens\n",
      " 91%|█████████ | 92/101 [27:15<02:14, 14.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    55 runs   (    0.08 ms per token, 13205.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4606.09 ms /   337 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_print_timings:        eval time =    3958.55 ms /    54 runs   (   73.31 ms per token,    13.64 tokens per second)\n",
      "llama_print_timings:       total time =    8636.29 ms /   391 tokens\n",
      " 92%|█████████▏| 93/101 [27:23<01:44, 13.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    67 runs   (    0.07 ms per token, 14096.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3499.24 ms /   249 tokens (   14.05 ms per token,    71.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4537.54 ms /    66 runs   (   68.75 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =    8120.09 ms /   315 tokens\n",
      " 93%|█████████▎| 94/101 [27:32<01:21, 11.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.63 ms /    51 runs   (    0.07 ms per token, 14061.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2899.99 ms /   169 tokens (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:        eval time =    3309.40 ms /    50 runs   (   66.19 ms per token,    15.11 tokens per second)\n",
      "llama_print_timings:       total time =    6273.09 ms /   219 tokens\n",
      " 94%|█████████▍| 95/101 [27:38<00:59,  9.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.56 ms /   103 runs   (    0.07 ms per token, 13622.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11306.60 ms /   889 tokens (   12.72 ms per token,    78.63 tokens per second)\n",
      "llama_print_timings:        eval time =    8064.82 ms /   102 runs   (   79.07 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =   19508.79 ms /   991 tokens\n",
      " 95%|█████████▌| 96/101 [27:57<01:04, 12.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.68 ms /   105 runs   (    0.07 ms per token, 13668.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9216.07 ms /   648 tokens (   14.22 ms per token,    70.31 tokens per second)\n",
      "llama_print_timings:        eval time =    7695.00 ms /   104 runs   (   73.99 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   17051.61 ms /   752 tokens\n",
      " 96%|█████████▌| 97/101 [28:14<00:56, 14.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.59 ms /    80 runs   (    0.08 ms per token, 12135.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5456.51 ms /   438 tokens (   12.46 ms per token,    80.27 tokens per second)\n",
      "llama_print_timings:        eval time =    8668.05 ms /    79 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =   14244.87 ms /   517 tokens\n",
      " 97%|█████████▋| 98/101 [28:29<00:42, 14.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    96 runs   (    0.07 ms per token, 13588.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33454.15 ms /  2127 tokens (   15.73 ms per token,    63.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9081.20 ms /    95 runs   (   95.59 ms per token,    10.46 tokens per second)\n",
      "llama_print_timings:       total time =   42674.84 ms /  2222 tokens\n",
      " 98%|█████████▊| 99/101 [29:11<00:45, 22.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    69 runs   (    0.09 ms per token, 11318.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5112.34 ms /   399 tokens (   12.81 ms per token,    78.05 tokens per second)\n",
      "llama_print_timings:        eval time =   11435.72 ms /    68 runs   (  168.17 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:       total time =   16669.98 ms /   467 tokens\n",
      " 99%|█████████▉| 100/101 [29:28<00:20, 20.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.52 ms /    60 runs   (    0.08 ms per token, 13286.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3736.08 ms /   228 tokens (   16.39 ms per token,    61.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4187.31 ms /    59 runs   (   70.97 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    8002.00 ms /   287 tokens\n",
      "100%|██████████| 101/101 [29:36<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_splitter_512 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/261 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    65 runs   (    0.08 ms per token, 11909.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10631.16 ms /   254 tokens (   41.85 ms per token,    23.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5145.43 ms /    64 runs   (   80.40 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   15882.94 ms /   318 tokens\n",
      "  0%|          | 1/261 [00:15<1:08:51, 15.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.19 ms /    52 runs   (    0.08 ms per token, 12419.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7575.13 ms /   539 tokens (   14.05 ms per token,    71.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3721.38 ms /    51 runs   (   72.97 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =   11428.87 ms /   590 tokens\n",
      "  1%|          | 2/261 [00:27<57:16, 13.27s/it]  Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.71 ms /    81 runs   (    0.19 ms per token,  5156.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3974.54 ms /   269 tokens (   14.78 ms per token,    67.68 tokens per second)\n",
      "llama_print_timings:        eval time =    6267.09 ms /    80 runs   (   78.34 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =   10398.67 ms /   349 tokens\n",
      "  1%|          | 3/261 [00:37<51:26, 11.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    69 runs   (    0.08 ms per token, 12916.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8241.73 ms /   565 tokens (   14.59 ms per token,    68.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4935.57 ms /    68 runs   (   72.58 ms per token,    13.78 tokens per second)\n",
      "llama_print_timings:       total time =   13309.67 ms /   633 tokens\n",
      "  2%|▏         | 4/261 [00:51<53:31, 12.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    70 runs   (    0.07 ms per token, 13894.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4323.99 ms /   338 tokens (   12.79 ms per token,    78.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5043.81 ms /    69 runs   (   73.10 ms per token,    13.68 tokens per second)\n",
      "llama_print_timings:       total time =    9505.97 ms /   407 tokens\n",
      "  2%|▏         | 5/261 [01:00<48:43, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    84 runs   (    0.07 ms per token, 13480.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6294.07 ms /   512 tokens (   12.29 ms per token,    81.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5996.12 ms /    83 runs   (   72.24 ms per token,    13.84 tokens per second)\n",
      "llama_print_timings:       total time =   12421.82 ms /   595 tokens\n",
      "  2%|▏         | 6/261 [01:12<49:59, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.72 ms /    63 runs   (    0.07 ms per token, 13336.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.36 ms /   223 tokens (   14.58 ms per token,    68.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4227.03 ms /    62 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    7558.20 ms /   285 tokens\n",
      "  3%|▎         | 7/261 [01:20<43:58, 10.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.84 ms /    64 runs   (    0.08 ms per token, 13209.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4918.22 ms /   400 tokens (   12.30 ms per token,    81.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4890.90 ms /    63 runs   (   77.63 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    9891.32 ms /   463 tokens\n",
      "  3%|▎         | 8/261 [01:30<43:08, 10.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.96 ms /   141 runs   (    0.08 ms per token, 12861.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4109.07 ms /   306 tokens (   13.43 ms per token,    74.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9915.92 ms /   140 runs   (   70.83 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:       total time =   14206.90 ms /   446 tokens\n",
      "  3%|▎         | 9/261 [01:44<48:11, 11.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    68 runs   (    0.07 ms per token, 13473.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3242.05 ms /   205 tokens (   15.81 ms per token,    63.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4733.33 ms /    67 runs   (   70.65 ms per token,    14.15 tokens per second)\n",
      "llama_print_timings:       total time =    8085.81 ms /   272 tokens\n",
      "  4%|▍         | 10/261 [01:52<43:38, 10.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    68 runs   (    0.08 ms per token, 13099.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3251.81 ms /   193 tokens (   16.85 ms per token,    59.35 tokens per second)\n",
      "llama_print_timings:        eval time =    4668.30 ms /    67 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =    8018.88 ms /   260 tokens\n",
      "  4%|▍         | 11/261 [02:00<40:23,  9.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    68 runs   (    0.07 ms per token, 13829.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3169.41 ms /   197 tokens (   16.09 ms per token,    62.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4502.80 ms /    67 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    7757.01 ms /   264 tokens\n",
      "  5%|▍         | 12/261 [02:08<37:47,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    67 runs   (    0.08 ms per token, 12786.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2907.96 ms /   174 tokens (   16.71 ms per token,    59.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4693.95 ms /    66 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    7705.81 ms /   240 tokens\n",
      "  5%|▍         | 13/261 [02:16<35:53,  8.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    65 runs   (    0.08 ms per token, 13249.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3228.57 ms /   196 tokens (   16.47 ms per token,    60.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4378.58 ms /    64 runs   (   68.42 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    7690.11 ms /   260 tokens\n",
      "  5%|▌         | 14/261 [02:23<34:30,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.45 ms /    62 runs   (    0.07 ms per token, 13926.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2982.27 ms /   188 tokens (   15.86 ms per token,    63.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4048.48 ms /    61 runs   (   66.37 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    7108.21 ms /   249 tokens\n",
      "  6%|▌         | 15/261 [02:31<32:48,  8.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.40 ms /    69 runs   (    0.08 ms per token, 12780.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3946.05 ms /   191 tokens (   20.66 ms per token,    48.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5208.14 ms /    68 runs   (   76.59 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =    9245.16 ms /   259 tokens\n",
      "  6%|▌         | 16/261 [02:40<34:12,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    95 runs   (    0.08 ms per token, 12496.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3959.28 ms /   238 tokens (   16.64 ms per token,    60.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8742.84 ms /    94 runs   (   93.01 ms per token,    10.75 tokens per second)\n",
      "llama_print_timings:       total time =   12827.67 ms /   332 tokens\n",
      "  7%|▋         | 17/261 [02:53<39:31,  9.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.51 ms /   131 runs   (    0.08 ms per token, 12470.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3925.85 ms /   225 tokens (   17.45 ms per token,    57.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9114.75 ms /   130 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   13215.00 ms /   355 tokens\n",
      "  7%|▋         | 18/261 [03:06<43:37, 10.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.57 ms /    58 runs   (    0.08 ms per token, 12680.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3488.18 ms /   234 tokens (   14.91 ms per token,    67.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4293.73 ms /    57 runs   (   75.33 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =    7856.09 ms /   291 tokens\n",
      "  7%|▋         | 19/261 [03:14<39:54,  9.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    80 runs   (    0.08 ms per token, 13251.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3442.11 ms /   221 tokens (   15.58 ms per token,    64.20 tokens per second)\n",
      "llama_print_timings:        eval time =    5498.26 ms /    79 runs   (   69.60 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =    9047.02 ms /   300 tokens\n",
      "  8%|▊         | 20/261 [03:23<38:43,  9.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.47 ms /   155 runs   (    0.07 ms per token, 13511.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3483.27 ms /   225 tokens (   15.48 ms per token,    64.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10694.69 ms /   154 runs   (   69.45 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   14380.89 ms /   379 tokens\n",
      "  8%|▊         | 21/261 [03:37<44:15, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.32 ms /   110 runs   (    0.08 ms per token, 13217.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4329.61 ms /   325 tokens (   13.32 ms per token,    75.06 tokens per second)\n",
      "llama_print_timings:        eval time =    8062.68 ms /   109 runs   (   73.97 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   12537.27 ms /   434 tokens\n",
      "  8%|▊         | 22/261 [03:50<45:50, 11.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.53 ms /    99 runs   (    0.08 ms per token, 13154.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8185.48 ms /   575 tokens (   14.24 ms per token,    70.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7170.99 ms /    98 runs   (   73.17 ms per token,    13.67 tokens per second)\n",
      "llama_print_timings:       total time =   15529.99 ms /   673 tokens\n",
      "  9%|▉         | 23/261 [04:05<50:26, 12.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.62 ms /   100 runs   (    0.08 ms per token, 13125.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4947.78 ms /   406 tokens (   12.19 ms per token,    82.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7052.95 ms /    99 runs   (   71.24 ms per token,    14.04 tokens per second)\n",
      "llama_print_timings:       total time =   12187.47 ms /   505 tokens\n",
      "  9%|▉         | 24/261 [04:17<49:36, 12.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    68 runs   (    0.08 ms per token, 12722.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8457.57 ms /   588 tokens (   14.38 ms per token,    69.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5332.14 ms /    67 runs   (   79.58 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =   13879.12 ms /   655 tokens\n",
      " 10%|▉         | 25/261 [04:31<50:57, 12.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    64 runs   (    0.07 ms per token, 13813.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.51 ms /   270 tokens (   15.59 ms per token,    64.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4366.68 ms /    63 runs   (   69.31 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =    8657.59 ms /   333 tokens\n",
      " 10%|▉         | 26/261 [04:40<45:41, 11.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    63 runs   (    0.07 ms per token, 13645.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.49 ms /   198 tokens (   16.68 ms per token,    59.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4204.22 ms /    62 runs   (   67.81 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =    7589.21 ms /   260 tokens\n",
      " 10%|█         | 27/261 [04:48<40:44, 10.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    69 runs   (    0.09 ms per token, 10989.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2976.72 ms /   191 tokens (   15.58 ms per token,    64.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5045.40 ms /    68 runs   (   74.20 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =    8113.48 ms /   259 tokens\n",
      " 11%|█         | 28/261 [04:56<37:50,  9.75s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    73 runs   (    0.08 ms per token, 13089.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3256.15 ms /   198 tokens (   16.45 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =    4993.65 ms /    72 runs   (   69.36 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =    8346.09 ms /   270 tokens\n",
      " 11%|█         | 29/261 [05:04<36:04,  9.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    71 runs   (    0.07 ms per token, 13797.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3186.58 ms /   205 tokens (   15.54 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4794.13 ms /    70 runs   (   68.49 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =    8071.14 ms /   275 tokens\n",
      " 11%|█▏        | 30/261 [05:12<34:27,  8.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    65 runs   (    0.08 ms per token, 13067.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3377.01 ms /   201 tokens (   16.80 ms per token,    59.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.98 ms /    64 runs   (   68.92 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =    7872.31 ms /   265 tokens\n",
      " 12%|█▏        | 31/261 [05:20<33:04,  8.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /    69 runs   (    0.08 ms per token, 13246.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3308.14 ms /   222 tokens (   14.90 ms per token,    67.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4636.48 ms /    68 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    8033.95 ms /   290 tokens\n",
      " 12%|█▏        | 32/261 [05:28<32:15,  8.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    73 runs   (    0.07 ms per token, 13576.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3187.01 ms /   205 tokens (   15.55 ms per token,    64.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5036.58 ms /    72 runs   (   69.95 ms per token,    14.30 tokens per second)\n",
      "llama_print_timings:       total time =    8317.36 ms /   277 tokens\n",
      " 13%|█▎        | 33/261 [05:36<31:57,  8.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.12 ms /    97 runs   (    0.07 ms per token, 13629.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3169.31 ms /   196 tokens (   16.17 ms per token,    61.84 tokens per second)\n",
      "llama_print_timings:        eval time =    6578.85 ms /    96 runs   (   68.53 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    9871.90 ms /   292 tokens\n",
      " 13%|█▎        | 34/261 [05:46<33:29,  8.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.50 ms /    59 runs   (    0.08 ms per token, 13114.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.02 ms /   192 tokens (   15.68 ms per token,    63.79 tokens per second)\n",
      "llama_print_timings:        eval time =    3978.30 ms /    58 runs   (   68.59 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    7064.70 ms /   250 tokens\n",
      " 13%|█▎        | 35/261 [05:53<31:19,  8.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    69 runs   (    0.08 ms per token, 13188.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2954.97 ms /   185 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4857.67 ms /    68 runs   (   71.44 ms per token,    14.00 tokens per second)\n",
      "llama_print_timings:       total time =    7902.68 ms /   253 tokens\n",
      " 14%|█▍        | 36/261 [06:01<30:43,  8.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    84 runs   (    0.07 ms per token, 13723.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3054.22 ms /   191 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5637.88 ms /    83 runs   (   67.93 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    8800.81 ms /   274 tokens\n",
      " 14%|█▍        | 37/261 [06:10<31:16,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.81 ms /    75 runs   (    0.08 ms per token, 12899.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3241.15 ms /   200 tokens (   16.21 ms per token,    61.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4995.33 ms /    74 runs   (   67.50 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    8332.55 ms /   274 tokens\n",
      " 15%|█▍        | 38/261 [06:18<31:05,  8.36s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    68 runs   (    0.07 ms per token, 13401.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3166.35 ms /   196 tokens (   16.15 ms per token,    61.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4484.63 ms /    67 runs   (   66.93 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =    7736.37 ms /   263 tokens\n",
      " 15%|█▍        | 39/261 [06:26<30:15,  8.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.35 ms /    68 runs   (    0.11 ms per token,  9247.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4052.97 ms /   197 tokens (   20.57 ms per token,    48.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7927.53 ms /    67 runs   (  118.32 ms per token,     8.45 tokens per second)\n",
      "llama_print_timings:       total time =   12082.13 ms /   264 tokens\n",
      " 15%|█▌        | 40/261 [06:38<34:26,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.54 ms /    70 runs   (    0.08 ms per token, 12646.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3273.02 ms /   185 tokens (   17.69 ms per token,    56.52 tokens per second)\n",
      "llama_print_timings:        eval time =    6584.79 ms /    69 runs   (   95.43 ms per token,    10.48 tokens per second)\n",
      "llama_print_timings:       total time =    9958.66 ms /   254 tokens\n",
      " 16%|█▌        | 41/261 [06:48<34:57,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    62 runs   (    0.08 ms per token, 12678.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3923.63 ms /   242 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4745.37 ms /    61 runs   (   77.79 ms per token,    12.85 tokens per second)\n",
      "llama_print_timings:       total time =    8754.01 ms /   303 tokens\n",
      " 16%|█▌        | 42/261 [06:57<33:57,  9.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.51 ms /    71 runs   (    0.08 ms per token, 12888.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7748.72 ms /   538 tokens (   14.40 ms per token,    69.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4855.68 ms /    70 runs   (   69.37 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =   12701.05 ms /   608 tokens\n",
      " 16%|█▋        | 43/261 [07:10<37:30, 10.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    68 runs   (    0.07 ms per token, 13452.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3044.58 ms /   189 tokens (   16.11 ms per token,    62.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4415.92 ms /    67 runs   (   65.91 ms per token,    15.17 tokens per second)\n",
      "llama_print_timings:       total time =    7549.73 ms /   256 tokens\n",
      " 17%|█▋        | 44/261 [07:17<34:19,  9.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.58 ms /    94 runs   (    0.08 ms per token, 12405.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.54 ms /   585 tokens (   14.84 ms per token,    67.41 tokens per second)\n",
      "llama_print_timings:        eval time =    8413.40 ms /    93 runs   (   90.47 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =   17239.89 ms /   678 tokens\n",
      " 17%|█▋        | 45/261 [07:34<42:32, 11.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    94 runs   (    0.08 ms per token, 12374.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4009.57 ms /   246 tokens (   16.30 ms per token,    61.35 tokens per second)\n",
      "llama_print_timings:        eval time =    7817.61 ms /    93 runs   (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   11965.44 ms /   339 tokens\n",
      " 18%|█▊        | 46/261 [07:46<42:30, 11.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.08 ms /    56 runs   (    0.07 ms per token, 13718.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5019.11 ms /   394 tokens (   12.74 ms per token,    78.50 tokens per second)\n",
      "llama_print_timings:        eval time =    3832.22 ms /    55 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =    8925.29 ms /   449 tokens\n",
      " 18%|█▊        | 47/261 [07:55<39:10, 10.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.67 ms /    75 runs   (    0.08 ms per token, 13225.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2960.61 ms /   187 tokens (   15.83 ms per token,    63.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5006.40 ms /    74 runs   (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time =    8063.43 ms /   261 tokens\n",
      " 18%|█▊        | 48/261 [08:03<35:56, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.53 ms /    64 runs   (    0.07 ms per token, 14121.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2935.23 ms /   187 tokens (   15.70 ms per token,    63.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4264.65 ms /    63 runs   (   67.69 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =    7280.66 ms /   250 tokens\n",
      " 19%|█▉        | 49/261 [08:11<32:45,  9.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    64 runs   (    0.07 ms per token, 13400.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2945.36 ms /   192 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4196.65 ms /    63 runs   (   66.61 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7223.21 ms /   255 tokens\n",
      " 19%|█▉        | 50/261 [08:18<30:26,  8.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      20.50 ms /   275 runs   (    0.07 ms per token, 13413.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2944.09 ms /   192 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_print_timings:        eval time =   19488.19 ms /   274 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =   22809.72 ms /   466 tokens\n",
      " 20%|█▉        | 51/261 [08:41<45:10, 12.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.92 ms /    79 runs   (    0.07 ms per token, 13349.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3317.10 ms /   197 tokens (   16.84 ms per token,    59.39 tokens per second)\n",
      "llama_print_timings:        eval time =    5336.55 ms /    78 runs   (   68.42 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    8756.78 ms /   275 tokens\n",
      " 20%|█▉        | 52/261 [08:49<40:37, 11.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.38 ms /    96 runs   (    0.08 ms per token, 13004.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3203.32 ms /   208 tokens (   15.40 ms per token,    64.93 tokens per second)\n",
      "llama_print_timings:        eval time =    6438.42 ms /    95 runs   (   67.77 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    9763.62 ms /   303 tokens\n",
      " 20%|██        | 53/261 [08:59<38:27, 11.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    61 runs   (    0.08 ms per token, 12482.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3234.08 ms /   196 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    4543.36 ms /    60 runs   (   75.72 ms per token,    13.21 tokens per second)\n",
      "llama_print_timings:       total time =    7856.53 ms /   256 tokens\n",
      " 21%|██        | 54/261 [09:07<34:55, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.42 ms /    60 runs   (    0.09 ms per token, 11074.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3018.79 ms /   189 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6476.65 ms /    59 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =    9584.51 ms /   248 tokens\n",
      " 21%|██        | 55/261 [09:17<34:12,  9.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.72 ms /   102 runs   (    0.08 ms per token, 13209.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4543.17 ms /   350 tokens (   12.98 ms per token,    77.04 tokens per second)\n",
      "llama_print_timings:        eval time =    7363.59 ms /   101 runs   (   72.91 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =   12043.96 ms /   451 tokens\n",
      " 21%|██▏       | 56/261 [09:29<36:10, 10.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.70 ms /   150 runs   (    0.08 ms per token, 12821.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3580.31 ms /   238 tokens (   15.04 ms per token,    66.47 tokens per second)\n",
      "llama_print_timings:        eval time =   11303.96 ms /   149 runs   (   75.87 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =   15085.55 ms /   387 tokens\n",
      " 22%|██▏       | 57/261 [09:44<40:35, 11.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.70 ms /    92 runs   (    0.07 ms per token, 13731.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10223.72 ms /   596 tokens (   17.15 ms per token,    58.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6712.64 ms /    91 runs   (   73.77 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   17069.02 ms /   687 tokens\n",
      " 22%|██▏       | 58/261 [10:01<45:36, 13.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.26 ms /    59 runs   (    0.07 ms per token, 13849.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3833.09 ms /   272 tokens (   14.09 ms per token,    70.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4021.08 ms /    58 runs   (   69.33 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =    7930.65 ms /   330 tokens\n",
      " 23%|██▎       | 59/261 [10:09<39:46, 11.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    58 runs   (    0.08 ms per token, 13299.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5554.52 ms /   460 tokens (   12.08 ms per token,    82.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4055.45 ms /    57 runs   (   71.15 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    9694.16 ms /   517 tokens\n",
      " 23%|██▎       | 60/261 [10:19<37:27, 11.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    76 runs   (    0.08 ms per token, 13279.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2950.53 ms /   178 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5081.89 ms /    75 runs   (   67.76 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    8129.13 ms /   253 tokens\n",
      " 23%|██▎       | 61/261 [10:27<34:13, 10.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.39 ms /    87 runs   (    0.07 ms per token, 13619.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2971.99 ms /   190 tokens (   15.64 ms per token,    63.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5813.53 ms /    86 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    8893.57 ms /   276 tokens\n",
      " 24%|██▍       | 62/261 [10:36<32:41,  9.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    58 runs   (    0.08 ms per token, 12666.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2955.04 ms /   191 tokens (   15.47 ms per token,    64.64 tokens per second)\n",
      "llama_print_timings:        eval time =    3914.25 ms /    57 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    6941.59 ms /   248 tokens\n",
      " 24%|██▍       | 63/261 [10:43<29:38,  8.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    66 runs   (    0.07 ms per token, 13333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3150.23 ms /   185 tokens (   17.03 ms per token,    58.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4475.13 ms /    65 runs   (   68.85 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    7710.30 ms /   250 tokens\n",
      " 25%|██▍       | 64/261 [10:50<28:14,  8.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    60 runs   (    0.08 ms per token, 12412.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2977.97 ms /   184 tokens (   16.18 ms per token,    61.79 tokens per second)\n",
      "llama_print_timings:        eval time =    5099.58 ms /    59 runs   (   86.43 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =    8157.34 ms /   243 tokens\n",
      " 25%|██▍       | 65/261 [10:58<27:40,  8.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.90 ms /    52 runs   (    0.08 ms per token, 13329.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3159.40 ms /   186 tokens (   16.99 ms per token,    58.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3478.34 ms /    51 runs   (   68.20 ms per token,    14.66 tokens per second)\n",
      "llama_print_timings:       total time =    6702.54 ms /   237 tokens\n",
      " 25%|██▌       | 66/261 [11:05<25:48,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    74 runs   (    0.07 ms per token, 13831.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3167.79 ms /   196 tokens (   16.16 ms per token,    61.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4901.41 ms /    73 runs   (   67.14 ms per token,    14.89 tokens per second)\n",
      "llama_print_timings:       total time =    8162.90 ms /   269 tokens\n",
      " 26%|██▌       | 67/261 [11:13<25:53,  8.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    72 runs   (    0.07 ms per token, 13483.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2929.41 ms /   188 tokens (   15.58 ms per token,    64.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4710.77 ms /    71 runs   (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    7729.51 ms /   259 tokens\n",
      " 26%|██▌       | 68/261 [11:21<25:29,  7.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    60 runs   (    0.07 ms per token, 13808.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2892.72 ms /   173 tokens (   16.72 ms per token,    59.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3913.36 ms /    59 runs   (   66.33 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:       total time =    6881.00 ms /   232 tokens\n",
      " 26%|██▋       | 69/261 [11:28<24:21,  7.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    67 runs   (    0.07 ms per token, 13554.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2910.90 ms /   182 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.21 ms /    66 runs   (   66.82 ms per token,    14.97 tokens per second)\n",
      "llama_print_timings:       total time =    7403.86 ms /   248 tokens\n",
      " 27%|██▋       | 70/261 [11:35<24:02,  7.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    83 runs   (    0.08 ms per token, 11869.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2919.82 ms /   183 tokens (   15.96 ms per token,    62.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7144.80 ms /    82 runs   (   87.13 ms per token,    11.48 tokens per second)\n",
      "llama_print_timings:       total time =   10180.37 ms /   265 tokens\n",
      " 27%|██▋       | 71/261 [11:45<26:24,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.62 ms /    50 runs   (    0.07 ms per token, 13812.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3078.51 ms /   180 tokens (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3241.37 ms /    49 runs   (   66.15 ms per token,    15.12 tokens per second)\n",
      "llama_print_timings:       total time =    6385.00 ms /   229 tokens\n",
      " 28%|██▊       | 72/261 [11:52<24:25,  7.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.03 ms /    67 runs   (    0.08 ms per token, 13325.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2998.15 ms /   187 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4335.02 ms /    66 runs   (   65.68 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:       total time =    7416.69 ms /   253 tokens\n",
      " 28%|██▊       | 73/261 [11:59<23:59,  7.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    63 runs   (    0.08 ms per token, 13288.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3224.07 ms /   198 tokens (   16.28 ms per token,    61.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4066.30 ms /    62 runs   (   65.59 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    7370.51 ms /   260 tokens\n",
      " 28%|██▊       | 74/261 [12:07<23:35,  7.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.47 ms /    72 runs   (    0.08 ms per token, 13160.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2994.69 ms /   191 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4655.77 ms /    71 runs   (   65.57 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    7740.98 ms /   262 tokens\n",
      " 29%|██▊       | 75/261 [12:14<23:37,  7.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    55 runs   (    0.07 ms per token, 13691.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3005.75 ms /   187 tokens (   16.07 ms per token,    62.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3540.37 ms /    54 runs   (   65.56 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    6613.81 ms /   241 tokens\n",
      " 29%|██▉       | 76/261 [12:21<22:34,  7.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    65 runs   (    0.08 ms per token, 13289.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2990.47 ms /   187 tokens (   15.99 ms per token,    62.53 tokens per second)\n",
      "llama_print_timings:        eval time =    4895.93 ms /    64 runs   (   76.50 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:       total time =    7969.42 ms /   251 tokens\n",
      " 30%|██▉       | 77/261 [12:29<23:03,  7.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    78 runs   (    0.08 ms per token, 13209.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3466.39 ms /   197 tokens (   17.60 ms per token,    56.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5064.98 ms /    77 runs   (   65.78 ms per token,    15.20 tokens per second)\n",
      "llama_print_timings:       total time =    8630.85 ms /   274 tokens\n",
      " 30%|██▉       | 78/261 [12:38<23:56,  7.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.97 ms /    81 runs   (    0.07 ms per token, 13570.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3000.61 ms /   189 tokens (   15.88 ms per token,    62.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5249.49 ms /    80 runs   (   65.62 ms per token,    15.24 tokens per second)\n",
      "llama_print_timings:       total time =    8351.43 ms /   269 tokens\n",
      " 30%|███       | 79/261 [12:46<24:16,  8.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    78 runs   (    0.08 ms per token, 12926.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.88 ms /   189 tokens (   15.93 ms per token,    62.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5115.89 ms /    77 runs   (   66.44 ms per token,    15.05 tokens per second)\n",
      "llama_print_timings:       total time =    8224.18 ms /   266 tokens\n",
      " 31%|███       | 80/261 [12:54<24:20,  8.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.57 ms /    63 runs   (    0.07 ms per token, 13776.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3323.29 ms /   187 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4106.81 ms /    62 runs   (   66.24 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    7508.11 ms /   249 tokens\n",
      " 31%|███       | 81/261 [13:02<23:42,  7.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    62 runs   (    0.07 ms per token, 13638.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3026.99 ms /   185 tokens (   16.36 ms per token,    61.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4078.33 ms /    61 runs   (   66.86 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =    7182.68 ms /   246 tokens\n",
      " 31%|███▏      | 82/261 [13:09<22:56,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.91 ms /    50 runs   (    0.08 ms per token, 12787.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3094.47 ms /   178 tokens (   17.38 ms per token,    57.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3628.64 ms /    49 runs   (   74.05 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =    6788.08 ms /   227 tokens\n",
      " 32%|███▏      | 83/261 [13:16<22:00,  7.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.77 ms /    91 runs   (    0.07 ms per token, 13443.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6695.61 ms /   520 tokens (   12.88 ms per token,    77.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6570.00 ms /    90 runs   (   73.00 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =   13383.19 ms /   610 tokens\n",
      " 32%|███▏      | 84/261 [13:29<27:10,  9.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.53 ms /   128 runs   (    0.07 ms per token, 13429.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5304.31 ms /   426 tokens (   12.45 ms per token,    80.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9074.01 ms /   127 runs   (   71.45 ms per token,    14.00 tokens per second)\n",
      "llama_print_timings:       total time =   14542.72 ms /   553 tokens\n",
      " 33%|███▎      | 85/261 [13:44<31:42, 10.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    86 runs   (    0.08 ms per token, 13216.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8219.22 ms /   561 tokens (   14.65 ms per token,    68.25 tokens per second)\n",
      "llama_print_timings:        eval time =    6193.99 ms /    85 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =   14522.39 ms /   646 tokens\n",
      " 33%|███▎      | 86/261 [13:58<34:46, 11.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.77 ms /    66 runs   (    0.07 ms per token, 13833.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4039.18 ms /   289 tokens (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4518.98 ms /    65 runs   (   69.52 ms per token,    14.38 tokens per second)\n",
      "llama_print_timings:       total time =    8639.30 ms /   354 tokens\n",
      " 33%|███▎      | 87/261 [14:07<31:43, 10.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.97 ms /   131 runs   (    0.08 ms per token, 11946.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5838.42 ms /   483 tokens (   12.09 ms per token,    82.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9374.86 ms /   130 runs   (   72.11 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =   15377.58 ms /   613 tokens\n",
      " 34%|███▎      | 88/261 [14:22<35:23, 12.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    84 runs   (    0.08 ms per token, 13015.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4616.75 ms /   371 tokens (   12.44 ms per token,    80.36 tokens per second)\n",
      "llama_print_timings:        eval time =    6177.16 ms /    83 runs   (   74.42 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   10902.26 ms /   454 tokens\n",
      " 34%|███▍      | 89/261 [14:33<34:00, 11.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.09 ms /    95 runs   (    0.09 ms per token, 11737.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8348.77 ms /   606 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10342.80 ms /    94 runs   (  110.03 ms per token,     9.09 tokens per second)\n",
      "llama_print_timings:       total time =   18853.94 ms /   700 tokens\n",
      " 34%|███▍      | 90/261 [14:52<39:47, 13.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.61 ms /    63 runs   (    0.10 ms per token,  9528.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2767.79 ms /   143 tokens (   19.36 ms per token,    51.67 tokens per second)\n",
      "llama_print_timings:        eval time =   13509.22 ms /    62 runs   (  217.89 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =   16400.01 ms /   205 tokens\n",
      " 35%|███▍      | 91/261 [15:08<41:38, 14.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    85 runs   (    0.07 ms per token, 13485.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8293.32 ms /   569 tokens (   14.58 ms per token,    68.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6212.67 ms /    84 runs   (   73.96 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   14615.07 ms /   653 tokens\n",
      " 35%|███▌      | 92/261 [15:23<41:19, 14.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.69 ms /   124 runs   (    0.08 ms per token, 12799.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.85 ms /   192 tokens (   16.12 ms per token,    62.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8882.12 ms /   123 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =   12138.74 ms /   315 tokens\n",
      " 36%|███▌      | 93/261 [15:35<38:57, 13.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.60 ms /    70 runs   (    0.08 ms per token, 12504.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3208.89 ms /   194 tokens (   16.54 ms per token,    60.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5235.59 ms /    69 runs   (   75.88 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =    8534.89 ms /   263 tokens\n",
      " 36%|███▌      | 94/261 [15:44<34:14, 12.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.83 ms /    49 runs   (    0.08 ms per token, 12790.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3239.14 ms /   182 tokens (   17.80 ms per token,    56.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3631.40 ms /    48 runs   (   75.65 ms per token,    13.22 tokens per second)\n",
      "llama_print_timings:       total time =    6934.28 ms /   230 tokens\n",
      " 36%|███▋      | 95/261 [15:51<29:35, 10.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    57 runs   (    0.08 ms per token, 12250.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3159.65 ms /   180 tokens (   17.55 ms per token,    56.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4593.45 ms /    56 runs   (   82.03 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    7827.63 ms /   236 tokens\n",
      " 37%|███▋      | 96/261 [15:58<27:03,  9.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    66 runs   (    0.08 ms per token, 13030.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3564.94 ms /   199 tokens (   17.91 ms per token,    55.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4485.41 ms /    65 runs   (   69.01 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =    8133.34 ms /   264 tokens\n",
      " 37%|███▋      | 97/261 [16:07<25:29,  9.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    80 runs   (    0.07 ms per token, 13335.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2957.73 ms /   187 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5416.39 ms /    79 runs   (   68.56 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    8474.08 ms /   266 tokens\n",
      " 38%|███▊      | 98/261 [16:15<24:38,  9.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    51 runs   (    0.08 ms per token, 13253.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2990.91 ms /   187 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3441.53 ms /    50 runs   (   68.83 ms per token,    14.53 tokens per second)\n",
      "llama_print_timings:       total time =    6494.40 ms /   237 tokens\n",
      " 38%|███▊      | 99/261 [16:22<22:24,  8.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.77 ms /    46 runs   (    0.08 ms per token, 12188.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2948.21 ms /   187 tokens (   15.77 ms per token,    63.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3627.53 ms /    45 runs   (   80.61 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =    6634.80 ms /   232 tokens\n",
      " 38%|███▊      | 100/261 [16:28<20:56,  7.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    57 runs   (    0.09 ms per token, 11143.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3277.28 ms /   183 tokens (   17.91 ms per token,    55.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.84 ms /    56 runs   (   81.00 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    7892.76 ms /   239 tokens\n",
      " 39%|███▊      | 101/261 [16:36<20:53,  7.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.16 ms /    60 runs   (    0.10 ms per token,  9733.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3408.51 ms /   192 tokens (   17.75 ms per token,    56.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6198.84 ms /    59 runs   (  105.06 ms per token,     9.52 tokens per second)\n",
      "llama_print_timings:       total time =    9695.78 ms /   251 tokens\n",
      " 39%|███▉      | 102/261 [16:46<22:14,  8.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.77 ms /    69 runs   (    0.11 ms per token,  8879.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3492.83 ms /   194 tokens (   18.00 ms per token,    55.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7976.07 ms /    68 runs   (  117.30 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:       total time =   11575.91 ms /   262 tokens\n",
      " 39%|███▉      | 103/261 [16:57<24:37,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    61 runs   (    0.09 ms per token, 10718.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3789.78 ms /   199 tokens (   19.04 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6055.88 ms /    60 runs   (  100.93 ms per token,     9.91 tokens per second)\n",
      "llama_print_timings:       total time =    9931.88 ms /   259 tokens\n",
      " 40%|███▉      | 104/261 [17:07<24:56,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    69 runs   (    0.08 ms per token, 11987.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3226.79 ms /   190 tokens (   16.98 ms per token,    58.88 tokens per second)\n",
      "llama_print_timings:        eval time =    5602.34 ms /    68 runs   (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    8920.83 ms /   258 tokens\n",
      " 40%|████      | 105/261 [17:16<24:18,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.57 ms /    75 runs   (    0.07 ms per token, 13474.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2989.90 ms /   190 tokens (   15.74 ms per token,    63.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4975.89 ms /    74 runs   (   67.24 ms per token,    14.87 tokens per second)\n",
      "llama_print_timings:       total time =    8059.69 ms /   264 tokens\n",
      " 41%|████      | 106/261 [17:24<23:09,  8.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    71 runs   (    0.08 ms per token, 13278.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3236.85 ms /   199 tokens (   16.27 ms per token,    61.48 tokens per second)\n",
      "llama_print_timings:        eval time =    4791.88 ms /    70 runs   (   68.46 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =    8117.23 ms /   269 tokens\n",
      " 41%|████      | 107/261 [17:32<22:21,  8.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /   100 runs   (    0.08 ms per token, 13331.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3367.40 ms /   195 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6709.68 ms /    99 runs   (   67.77 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =   10201.19 ms /   294 tokens\n",
      " 41%|████▏     | 108/261 [17:43<23:21,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.38 ms /    59 runs   (    0.07 ms per token, 13482.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.82 ms /   190 tokens (   15.85 ms per token,    63.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3927.00 ms /    58 runs   (   67.71 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =    7009.13 ms /   248 tokens\n",
      " 42%|████▏     | 109/261 [17:50<21:34,  8.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.41 ms /    73 runs   (    0.07 ms per token, 13503.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7069.57 ms /   526 tokens (   13.44 ms per token,    74.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5209.11 ms /    72 runs   (   72.35 ms per token,    13.82 tokens per second)\n",
      "llama_print_timings:       total time =   12371.09 ms /   598 tokens\n",
      " 42%|████▏     | 110/261 [18:02<24:20,  9.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    71 runs   (    0.07 ms per token, 13383.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4016.35 ms /   292 tokens (   13.75 ms per token,    72.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4849.58 ms /    70 runs   (   69.28 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =    8954.08 ms /   362 tokens\n",
      " 43%|████▎     | 111/261 [18:11<23:38,  9.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.76 ms /    93 runs   (    0.07 ms per token, 13751.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3154.79 ms /   196 tokens (   16.10 ms per token,    62.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6192.40 ms /    92 runs   (   67.31 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =    9459.27 ms /   288 tokens\n",
      " 43%|████▎     | 112/261 [18:20<23:29,  9.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.52 ms /    48 runs   (    0.07 ms per token, 13648.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3218.66 ms /   193 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3112.81 ms /    47 runs   (   66.23 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    6389.06 ms /   240 tokens\n",
      " 43%|████▎     | 113/261 [18:27<21:03,  8.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    52 runs   (    0.08 ms per token, 12351.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2960.85 ms /   192 tokens (   15.42 ms per token,    64.85 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.23 ms /    51 runs   (   88.93 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =    7573.56 ms /   243 tokens\n",
      " 44%|████▎     | 114/261 [18:34<20:13,  8.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    59 runs   (    0.08 ms per token, 12452.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3754.95 ms /   193 tokens (   19.46 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4043.73 ms /    58 runs   (   69.72 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =    7877.23 ms /   251 tokens\n",
      " 44%|████▍     | 115/261 [18:42<19:48,  8.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    75 runs   (    0.08 ms per token, 13171.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3100.61 ms /   185 tokens (   16.76 ms per token,    59.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4974.69 ms /    74 runs   (   67.23 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    8170.25 ms /   259 tokens\n",
      " 44%|████▍     | 116/261 [18:50<19:41,  8.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    63 runs   (    0.08 ms per token, 12670.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3185.35 ms /   189 tokens (   16.85 ms per token,    59.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4240.80 ms /    62 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    7506.49 ms /   251 tokens\n",
      " 45%|████▍     | 117/261 [18:58<19:06,  7.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    72 runs   (    0.08 ms per token, 13026.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.22 ms /   191 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5332.62 ms /    71 runs   (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =    8724.57 ms /   262 tokens\n",
      " 45%|████▌     | 118/261 [19:07<19:31,  8.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.82 ms /    84 runs   (    0.08 ms per token, 12322.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3089.73 ms /   176 tokens (   17.56 ms per token,    56.96 tokens per second)\n",
      "llama_print_timings:        eval time =    6574.77 ms /    83 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    9774.57 ms /   259 tokens\n",
      " 46%|████▌     | 119/261 [19:17<20:30,  8.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    58 runs   (    0.07 ms per token, 13364.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3340.44 ms /   196 tokens (   17.04 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =    3838.63 ms /    57 runs   (   67.34 ms per token,    14.85 tokens per second)\n",
      "llama_print_timings:       total time =    7252.83 ms /   253 tokens\n",
      " 46%|████▌     | 120/261 [19:24<19:22,  8.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    51 runs   (    0.08 ms per token, 12318.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3096.86 ms /   191 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4108.34 ms /    50 runs   (   82.17 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    7272.65 ms /   241 tokens\n",
      " 46%|████▋     | 121/261 [19:31<18:33,  7.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    58 runs   (    0.07 ms per token, 13504.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3116.21 ms /   190 tokens (   16.40 ms per token,    60.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3868.67 ms /    57 runs   (   67.87 ms per token,    14.73 tokens per second)\n",
      "llama_print_timings:       total time =    7057.97 ms /   247 tokens\n",
      " 47%|████▋     | 122/261 [19:38<17:48,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    62 runs   (    0.08 ms per token, 12304.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3271.08 ms /   193 tokens (   16.95 ms per token,    59.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4675.45 ms /    61 runs   (   76.65 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:       total time =    8028.12 ms /   254 tokens\n",
      " 47%|████▋     | 123/261 [19:46<17:55,  7.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    74 runs   (    0.10 ms per token, 10456.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3444.88 ms /   198 tokens (   17.40 ms per token,    57.48 tokens per second)\n",
      "llama_print_timings:        eval time =    8575.06 ms /    73 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:       total time =   12154.83 ms /   271 tokens\n",
      " 48%|████▊     | 124/261 [19:58<20:46,  9.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    69 runs   (    0.08 ms per token, 12421.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3463.82 ms /   197 tokens (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4964.07 ms /    68 runs   (   73.00 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =    8516.58 ms /   265 tokens\n",
      " 48%|████▊     | 125/261 [20:07<20:14,  8.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    73 runs   (    0.08 ms per token, 12746.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3316.05 ms /   194 tokens (   17.09 ms per token,    58.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5600.63 ms /    72 runs   (   77.79 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =    9011.85 ms /   266 tokens\n",
      " 48%|████▊     | 126/261 [20:16<20:08,  8.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    56 runs   (    0.08 ms per token, 13068.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3225.04 ms /   202 tokens (   15.97 ms per token,    62.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3941.98 ms /    55 runs   (   71.67 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =    7240.00 ms /   257 tokens\n",
      " 49%|████▊     | 127/261 [20:23<18:51,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.56 ms /    64 runs   (    0.07 ms per token, 14028.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3193.22 ms /   196 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4211.77 ms /    63 runs   (   66.85 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =    7482.89 ms /   259 tokens\n",
      " 49%|████▉     | 128/261 [20:31<18:04,  8.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.16 ms /    82 runs   (    0.08 ms per token, 13320.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.08 ms /   195 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5620.04 ms /    81 runs   (   69.38 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    8897.83 ms /   276 tokens\n",
      " 49%|████▉     | 129/261 [20:39<18:26,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    59 runs   (    0.08 ms per token, 12159.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3201.65 ms /   190 tokens (   16.85 ms per token,    59.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4945.29 ms /    58 runs   (   85.26 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    8229.56 ms /   248 tokens\n",
      " 50%|████▉     | 130/261 [20:48<18:11,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    71 runs   (    0.08 ms per token, 11807.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2974.02 ms /   180 tokens (   16.52 ms per token,    60.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5550.24 ms /    70 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =    8617.09 ms /   250 tokens\n",
      " 50%|█████     | 131/261 [20:56<18:14,  8.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.36 ms /    78 runs   (    0.09 ms per token, 10597.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.31 ms /   204 tokens (   19.95 ms per token,    50.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8142.18 ms /    77 runs   (  105.74 ms per token,     9.46 tokens per second)\n",
      "llama_print_timings:       total time =   12342.45 ms /   281 tokens\n",
      " 51%|█████     | 132/261 [21:09<20:38,  9.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    85 runs   (    0.07 ms per token, 13558.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3210.48 ms /   193 tokens (   16.63 ms per token,    60.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5859.90 ms /    84 runs   (   69.76 ms per token,    14.33 tokens per second)\n",
      "llama_print_timings:       total time =    9176.46 ms /   277 tokens\n",
      " 51%|█████     | 133/261 [21:18<20:12,  9.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    68 runs   (    0.08 ms per token, 12905.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3189.17 ms /   202 tokens (   15.79 ms per token,    63.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4989.03 ms /    67 runs   (   74.46 ms per token,    13.43 tokens per second)\n",
      "llama_print_timings:       total time =    8262.82 ms /   269 tokens\n",
      " 51%|█████▏    | 134/261 [21:26<19:17,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.82 ms /    77 runs   (    0.08 ms per token, 13232.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2962.83 ms /   186 tokens (   15.93 ms per token,    62.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5215.62 ms /    76 runs   (   68.63 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =    8272.26 ms /   262 tokens\n",
      " 52%|█████▏    | 135/261 [21:34<18:36,  8.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    72 runs   (    0.07 ms per token, 13875.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3172.84 ms /   198 tokens (   16.02 ms per token,    62.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4774.68 ms /    71 runs   (   67.25 ms per token,    14.87 tokens per second)\n",
      "llama_print_timings:       total time =    8033.20 ms /   269 tokens\n",
      " 52%|█████▏    | 136/261 [21:42<17:56,  8.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.36 ms /    72 runs   (    0.07 ms per token, 13430.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3172.77 ms /   204 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4817.56 ms /    71 runs   (   67.85 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =    8077.05 ms /   275 tokens\n",
      " 52%|█████▏    | 137/261 [21:51<17:28,  8.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    74 runs   (    0.08 ms per token, 12774.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3235.19 ms /   201 tokens (   16.10 ms per token,    62.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5391.93 ms /    73 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =    8721.30 ms /   274 tokens\n",
      " 53%|█████▎    | 138/261 [21:59<17:29,  8.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.67 ms /   157 runs   (    0.08 ms per token, 12390.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3526.27 ms /   197 tokens (   17.90 ms per token,    55.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11894.97 ms /   156 runs   (   76.25 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =   15633.79 ms /   353 tokens\n",
      " 53%|█████▎    | 139/261 [22:15<21:41, 10.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.66 ms /    97 runs   (    0.08 ms per token, 12669.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3240.64 ms /   193 tokens (   16.79 ms per token,    59.56 tokens per second)\n",
      "llama_print_timings:        eval time =    7065.49 ms /    96 runs   (   73.60 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   10433.05 ms /   289 tokens\n",
      " 54%|█████▎    | 140/261 [22:25<21:22, 10.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    58 runs   (    0.08 ms per token, 12218.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3207.56 ms /   196 tokens (   16.37 ms per token,    61.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4347.15 ms /    57 runs   (   76.27 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =    7632.97 ms /   253 tokens\n",
      " 54%|█████▍    | 141/261 [22:33<19:25,  9.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    52 runs   (    0.08 ms per token, 11956.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3347.73 ms /   201 tokens (   16.66 ms per token,    60.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4573.40 ms /    51 runs   (   89.67 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =    7992.38 ms /   252 tokens\n",
      " 54%|█████▍    | 142/261 [22:41<18:14,  9.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    62 runs   (    0.08 ms per token, 12548.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3681.87 ms /   197 tokens (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:        eval time =    4829.19 ms /    61 runs   (   79.17 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =    8592.79 ms /   258 tokens\n",
      " 55%|█████▍    | 143/261 [22:50<17:44,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.71 ms /    54 runs   (    0.09 ms per token, 11457.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3285.25 ms /   190 tokens (   17.29 ms per token,    57.83 tokens per second)\n",
      "llama_print_timings:        eval time =    4481.73 ms /    53 runs   (   84.56 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    7841.26 ms /   243 tokens\n",
      " 55%|█████▌    | 144/261 [22:57<16:54,  8.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    60 runs   (    0.08 ms per token, 12911.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3368.95 ms /   190 tokens (   17.73 ms per token,    56.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5341.18 ms /    59 runs   (   90.53 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    8789.57 ms /   249 tokens\n",
      " 56%|█████▌    | 145/261 [23:06<16:49,  8.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.20 ms /    68 runs   (    0.08 ms per token, 13079.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2991.62 ms /   189 tokens (   15.83 ms per token,    63.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4976.39 ms /    67 runs   (   74.27 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =    8063.11 ms /   256 tokens\n",
      " 56%|█████▌    | 146/261 [23:14<16:19,  8.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    72 runs   (    0.08 ms per token, 13196.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3528.01 ms /   204 tokens (   17.29 ms per token,    57.82 tokens per second)\n",
      "llama_print_timings:        eval time =    5040.13 ms /    71 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    8657.80 ms /   275 tokens\n",
      " 56%|█████▋    | 147/261 [23:23<16:15,  8.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    72 runs   (    0.07 ms per token, 13587.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2954.75 ms /   190 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4795.10 ms /    71 runs   (   67.54 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    7838.53 ms /   261 tokens\n",
      " 57%|█████▋    | 148/261 [23:31<15:42,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.75 ms /   106 runs   (    0.07 ms per token, 13672.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2931.22 ms /   188 tokens (   15.59 ms per token,    64.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7608.36 ms /   105 runs   (   72.46 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   10670.52 ms /   293 tokens\n",
      " 57%|█████▋    | 149/261 [23:41<16:52,  9.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.42 ms /    61 runs   (    0.07 ms per token, 13816.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3312.52 ms /   193 tokens (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4313.56 ms /    60 runs   (   71.89 ms per token,    13.91 tokens per second)\n",
      "llama_print_timings:       total time =    7706.62 ms /   253 tokens\n",
      " 57%|█████▋    | 150/261 [23:49<15:59,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.22 ms /    73 runs   (    0.07 ms per token, 13992.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.86 ms /   191 tokens (   16.21 ms per token,    61.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4852.53 ms /    72 runs   (   67.40 ms per token,    14.84 tokens per second)\n",
      "llama_print_timings:       total time =    8038.32 ms /   263 tokens\n",
      " 58%|█████▊    | 151/261 [23:57<15:30,  8.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    57 runs   (    0.09 ms per token, 10586.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3419.24 ms /   185 tokens (   18.48 ms per token,    54.11 tokens per second)\n",
      "llama_print_timings:        eval time =    7764.24 ms /    56 runs   (  138.65 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:       total time =   11271.02 ms /   241 tokens\n",
      " 58%|█████▊    | 152/261 [24:08<16:54,  9.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    68 runs   (    0.08 ms per token, 12442.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3768.54 ms /   193 tokens (   19.53 ms per token,    51.21 tokens per second)\n",
      "llama_print_timings:        eval time =    5825.70 ms /    67 runs   (   86.95 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =    9684.11 ms /   260 tokens\n",
      " 59%|█████▊    | 153/261 [24:18<16:57,  9.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.80 ms /   100 runs   (    0.08 ms per token, 12815.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3293.65 ms /   202 tokens (   16.31 ms per token,    61.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7374.70 ms /    99 runs   (   74.49 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   10798.79 ms /   301 tokens\n",
      " 59%|█████▉    | 154/261 [24:29<17:30,  9.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    72 runs   (    0.08 ms per token, 12841.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3032.42 ms /   191 tokens (   15.88 ms per token,    62.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5448.99 ms /    71 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =    8576.79 ms /   262 tokens\n",
      " 59%|█████▉    | 155/261 [24:37<16:41,  9.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.25 ms /    57 runs   (    0.07 ms per token, 13408.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.67 ms /   198 tokens (   16.66 ms per token,    60.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3975.23 ms /    56 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    7347.05 ms /   254 tokens\n",
      " 60%|█████▉    | 156/261 [24:45<15:25,  8.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.29 ms /    63 runs   (    0.08 ms per token, 11904.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3224.87 ms /   185 tokens (   17.43 ms per token,    57.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4911.93 ms /    62 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    8221.62 ms /   247 tokens\n",
      " 60%|██████    | 157/261 [24:53<14:58,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.86 ms /    51 runs   (    0.08 ms per token, 13202.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3146.43 ms /   191 tokens (   16.47 ms per token,    60.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3321.75 ms /    50 runs   (   66.44 ms per token,    15.05 tokens per second)\n",
      "llama_print_timings:       total time =    6533.44 ms /   241 tokens\n",
      " 61%|██████    | 158/261 [25:00<13:45,  8.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    85 runs   (    0.07 ms per token, 13423.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3352.28 ms /   196 tokens (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5622.42 ms /    84 runs   (   66.93 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =    9084.17 ms /   280 tokens\n",
      " 61%|██████    | 159/261 [25:09<14:10,  8.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    70 runs   (    0.07 ms per token, 13765.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3288.13 ms /   203 tokens (   16.20 ms per token,    61.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.24 ms /    69 runs   (   65.73 ms per token,    15.21 tokens per second)\n",
      "llama_print_timings:       total time =    7910.93 ms /   272 tokens\n",
      " 61%|██████▏   | 160/261 [25:17<13:48,  8.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.87 ms /    51 runs   (    0.08 ms per token, 13181.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.83 ms /   192 tokens (   15.67 ms per token,    63.83 tokens per second)\n",
      "llama_print_timings:        eval time =    3273.49 ms /    50 runs   (   65.47 ms per token,    15.27 tokens per second)\n",
      "llama_print_timings:       total time =    6344.66 ms /   242 tokens\n",
      " 62%|██████▏   | 161/261 [25:23<12:44,  7.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.84 ms /   168 runs   (    0.08 ms per token, 13085.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3259.47 ms /   198 tokens (   16.46 ms per token,    60.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11143.29 ms /   167 runs   (   66.73 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =   14617.62 ms /   365 tokens\n",
      " 62%|██████▏   | 162/261 [25:38<16:04,  9.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    75 runs   (    0.08 ms per token, 12244.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3202.06 ms /   184 tokens (   17.40 ms per token,    57.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5328.82 ms /    74 runs   (   72.01 ms per token,    13.89 tokens per second)\n",
      "llama_print_timings:       total time =    8630.73 ms /   258 tokens\n",
      " 62%|██████▏   | 163/261 [25:46<15:22,  9.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.22 ms /    68 runs   (    0.08 ms per token, 13031.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3538.78 ms /   194 tokens (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4709.15 ms /    67 runs   (   70.29 ms per token,    14.23 tokens per second)\n",
      "llama_print_timings:       total time =    8340.31 ms /   261 tokens\n",
      " 63%|██████▎   | 164/261 [25:55<14:41,  9.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.02 ms /    70 runs   (    0.07 ms per token, 13935.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3175.78 ms /   198 tokens (   16.04 ms per token,    62.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5176.07 ms /    69 runs   (   75.02 ms per token,    13.33 tokens per second)\n",
      "llama_print_timings:       total time =    8439.25 ms /   267 tokens\n",
      " 63%|██████▎   | 165/261 [26:03<14:13,  8.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.51 ms /    78 runs   (    0.07 ms per token, 14163.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3236.38 ms /   193 tokens (   16.77 ms per token,    59.63 tokens per second)\n",
      "llama_print_timings:        eval time =    5199.53 ms /    77 runs   (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    8531.65 ms /   270 tokens\n",
      " 64%|██████▎   | 166/261 [26:12<13:54,  8.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.07 ms /    76 runs   (    0.08 ms per token, 12522.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2943.13 ms /   184 tokens (   16.00 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5039.74 ms /    75 runs   (   67.20 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    8078.13 ms /   259 tokens\n",
      " 64%|██████▍   | 167/261 [26:20<13:26,  8.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /   140 runs   (    0.08 ms per token, 12821.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3262.28 ms /   198 tokens (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_print_timings:        eval time =    9564.14 ms /   139 runs   (   68.81 ms per token,    14.53 tokens per second)\n",
      "llama_print_timings:       total time =   13006.52 ms /   337 tokens\n",
      " 64%|██████▍   | 168/261 [26:33<15:21,  9.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.09 ms /    84 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3316.79 ms /   201 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5604.65 ms /    83 runs   (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    9027.80 ms /   284 tokens\n",
      " 65%|██████▍   | 169/261 [26:42<14:47,  9.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.59 ms /    68 runs   (    0.08 ms per token, 12153.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4272.47 ms /   193 tokens (   22.14 ms per token,    45.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5467.36 ms /    67 runs   (   81.60 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =    9829.59 ms /   260 tokens\n",
      " 65%|██████▌   | 170/261 [26:52<14:43,  9.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    67 runs   (    0.07 ms per token, 13378.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3273.53 ms /   193 tokens (   16.96 ms per token,    58.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4517.24 ms /    66 runs   (   68.44 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =    7878.62 ms /   259 tokens\n",
      " 66%|██████▌   | 171/261 [26:59<13:44,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    64 runs   (    0.07 ms per token, 13386.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3200.67 ms /   194 tokens (   16.50 ms per token,    60.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4223.64 ms /    63 runs   (   67.04 ms per token,    14.92 tokens per second)\n",
      "llama_print_timings:       total time =    7503.55 ms /   257 tokens\n",
      " 66%|██████▌   | 172/261 [27:07<12:50,  8.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    69 runs   (    0.08 ms per token, 12839.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3318.14 ms /   199 tokens (   16.67 ms per token,    59.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4954.85 ms /    68 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =    8361.56 ms /   267 tokens\n",
      " 66%|██████▋   | 173/261 [27:15<12:34,  8.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.29 ms /   141 runs   (    0.08 ms per token, 12483.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3000.85 ms /   192 tokens (   15.63 ms per token,    63.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9610.98 ms /   140 runs   (   68.65 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =   12795.79 ms /   332 tokens\n",
      " 67%|██████▋   | 174/261 [27:28<14:16,  9.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    60 runs   (    0.08 ms per token, 12376.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2978.93 ms /   190 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5405.87 ms /    59 runs   (   91.62 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =    8466.56 ms /   249 tokens\n",
      " 67%|██████▋   | 175/261 [27:37<13:31,  9.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    77 runs   (    0.08 ms per token, 12618.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3429.40 ms /   194 tokens (   17.68 ms per token,    56.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5599.42 ms /    76 runs   (   73.68 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =    9134.10 ms /   270 tokens\n",
      " 67%|██████▋   | 176/261 [27:46<13:14,  9.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    54 runs   (    0.09 ms per token, 10627.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3495.79 ms /   192 tokens (   18.21 ms per token,    54.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5070.98 ms /    53 runs   (   95.68 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:       total time =    8655.89 ms /   245 tokens\n",
      " 68%|██████▊   | 177/261 [27:54<12:47,  9.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    63 runs   (    0.08 ms per token, 13105.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3388.38 ms /   201 tokens (   16.86 ms per token,    59.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4603.73 ms /    62 runs   (   74.25 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =    8073.51 ms /   263 tokens\n",
      " 68%|██████▊   | 178/261 [28:02<12:12,  8.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    51 runs   (    0.08 ms per token, 12384.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2373.30 ms /   124 tokens (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3392.01 ms /    50 runs   (   67.84 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =    5850.33 ms /   174 tokens\n",
      " 69%|██████▊   | 179/261 [28:08<10:50,  7.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.06 ms /    82 runs   (    0.07 ms per token, 13540.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5725.02 ms /   469 tokens (   12.21 ms per token,    81.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5734.69 ms /    81 runs   (   70.80 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:       total time =   11571.14 ms /   550 tokens\n",
      " 69%|██████▉   | 180/261 [28:20<12:10,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    81 runs   (    0.07 ms per token, 13982.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3027.55 ms /   187 tokens (   16.19 ms per token,    61.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5396.90 ms /    80 runs   (   67.46 ms per token,    14.82 tokens per second)\n",
      "llama_print_timings:       total time =    8526.67 ms /   267 tokens\n",
      " 69%|██████▉   | 181/261 [28:28<11:50,  8.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    65 runs   (    0.07 ms per token, 14051.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.12 ms /   187 tokens (   15.65 ms per token,    63.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4262.53 ms /    64 runs   (   66.60 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7268.78 ms /   251 tokens\n",
      " 70%|██████▉   | 182/261 [28:36<11:03,  8.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    82 runs   (    0.07 ms per token, 13582.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3335.45 ms /   200 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5555.10 ms /    81 runs   (   68.58 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    8992.59 ms /   281 tokens\n",
      " 70%|███████   | 183/261 [28:45<11:08,  8.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    71 runs   (    0.07 ms per token, 13682.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3222.89 ms /   208 tokens (   15.49 ms per token,    64.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4808.29 ms /    70 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    8121.29 ms /   278 tokens\n",
      " 70%|███████   | 184/261 [28:53<10:49,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    89 runs   (    0.07 ms per token, 13660.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.93 ms /   200 tokens (   16.52 ms per token,    60.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6019.23 ms /    88 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    9436.25 ms /   288 tokens\n",
      " 71%|███████   | 185/261 [29:02<11:04,  8.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.96 ms /    65 runs   (    0.08 ms per token, 13091.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3180.40 ms /   212 tokens (   15.00 ms per token,    66.66 tokens per second)\n",
      "llama_print_timings:        eval time =    4354.76 ms /    64 runs   (   68.04 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =    7616.37 ms /   276 tokens\n",
      " 71%|███████▏  | 186/261 [29:10<10:30,  8.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.43 ms /    60 runs   (    0.07 ms per token, 13544.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3258.36 ms /   196 tokens (   16.62 ms per token,    60.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4056.44 ms /    59 runs   (   68.75 ms per token,    14.54 tokens per second)\n",
      "llama_print_timings:       total time =    7390.30 ms /   255 tokens\n",
      " 72%|███████▏  | 187/261 [29:17<09:59,  8.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    67 runs   (    0.09 ms per token, 10780.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.17 ms /   190 tokens (   17.39 ms per token,    57.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5019.44 ms /    66 runs   (   76.05 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:       total time =    8419.12 ms /   256 tokens\n",
      " 72%|███████▏  | 188/261 [29:26<09:58,  8.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    64 runs   (    0.08 ms per token, 11911.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3588.28 ms /   198 tokens (   18.12 ms per token,    55.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4747.46 ms /    63 runs   (   75.36 ms per token,    13.27 tokens per second)\n",
      "llama_print_timings:       total time =    8420.98 ms /   261 tokens\n",
      " 72%|███████▏  | 189/261 [29:34<09:55,  8.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       2.95 ms /    41 runs   (    0.07 ms per token, 13907.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3184.12 ms /   200 tokens (   15.92 ms per token,    62.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2722.53 ms /    40 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    5959.17 ms /   240 tokens\n",
      " 73%|███████▎  | 190/261 [29:40<08:57,  7.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.38 ms /    57 runs   (    0.08 ms per token, 13001.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3226.00 ms /   198 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4654.29 ms /    56 runs   (   83.11 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =    7955.80 ms /   254 tokens\n",
      " 73%|███████▎  | 191/261 [29:48<08:58,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    66 runs   (    0.08 ms per token, 12607.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3216.83 ms /   189 tokens (   17.02 ms per token,    58.75 tokens per second)\n",
      "llama_print_timings:        eval time =    4720.36 ms /    65 runs   (   72.62 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    8026.08 ms /   254 tokens\n",
      " 74%|███████▎  | 192/261 [29:56<08:57,  7.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    71 runs   (    0.07 ms per token, 13583.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3223.17 ms /   196 tokens (   16.44 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =    5036.25 ms /    70 runs   (   71.95 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =    8353.11 ms /   266 tokens\n",
      " 74%|███████▍  | 193/261 [30:04<09:01,  7.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    46 runs   (    0.08 ms per token, 11960.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2952.78 ms /   191 tokens (   15.46 ms per token,    64.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3279.14 ms /    45 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =    6292.76 ms /   236 tokens\n",
      " 74%|███████▍  | 194/261 [30:11<08:20,  7.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.09 ms /    94 runs   (    0.08 ms per token, 13252.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3304.44 ms /   194 tokens (   17.03 ms per token,    58.71 tokens per second)\n",
      "llama_print_timings:        eval time =    6551.38 ms /    93 runs   (   70.44 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:       total time =    9977.38 ms /   287 tokens\n",
      " 75%|███████▍  | 195/261 [30:21<09:02,  8.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.25 ms /    84 runs   (    0.07 ms per token, 13442.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3022.11 ms /   189 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.02 ms /    83 runs   (   70.02 ms per token,    14.28 tokens per second)\n",
      "llama_print_timings:       total time =    8940.99 ms /   272 tokens\n",
      " 75%|███████▌  | 196/261 [30:30<09:08,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.95 ms /    53 runs   (    0.07 ms per token, 13407.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2701.47 ms /   152 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3541.85 ms /    52 runs   (   68.11 ms per token,    14.68 tokens per second)\n",
      "llama_print_timings:       total time =    6310.35 ms /   204 tokens\n",
      " 75%|███████▌  | 197/261 [30:36<08:19,  7.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.31 ms /    71 runs   (    0.09 ms per token, 11259.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3781.91 ms /   266 tokens (   14.22 ms per token,    70.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5655.17 ms /    70 runs   (   80.79 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    9539.32 ms /   336 tokens\n",
      " 76%|███████▌  | 198/261 [30:45<08:44,  8.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.39 ms /   134 runs   (    0.09 ms per token, 11763.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3459.18 ms /   203 tokens (   17.04 ms per token,    58.68 tokens per second)\n",
      "llama_print_timings:        eval time =   13104.31 ms /   133 runs   (   98.53 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =   16748.76 ms /   336 tokens\n",
      " 76%|███████▌  | 199/261 [31:02<11:12, 10.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   171 runs   (    0.08 ms per token, 12620.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4112.25 ms /   210 tokens (   19.58 ms per token,    51.07 tokens per second)\n",
      "llama_print_timings:        eval time =   12933.35 ms /   170 runs   (   76.08 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:       total time =   17278.11 ms /   380 tokens\n",
      " 77%|███████▋  | 200/261 [31:20<12:59, 12.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /    91 runs   (    0.09 ms per token, 11656.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3535.66 ms /   233 tokens (   15.17 ms per token,    65.90 tokens per second)\n",
      "llama_print_timings:        eval time =    6905.32 ms /    90 runs   (   76.73 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =   10564.91 ms /   323 tokens\n",
      " 77%|███████▋  | 201/261 [31:30<12:07, 12.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    79 runs   (    0.11 ms per token,  9364.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4459.49 ms /   235 tokens (   18.98 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:        eval time =    8446.66 ms /    78 runs   (  108.29 ms per token,     9.23 tokens per second)\n",
      "llama_print_timings:       total time =   13023.63 ms /   313 tokens\n",
      " 77%|███████▋  | 202/261 [31:43<12:11, 12.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.82 ms /    66 runs   (    0.07 ms per token, 13704.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5424.11 ms /   200 tokens (   27.12 ms per token,    36.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5664.81 ms /    65 runs   (   87.15 ms per token,    11.47 tokens per second)\n",
      "llama_print_timings:       total time =   11178.22 ms /   265 tokens\n",
      " 78%|███████▊  | 203/261 [31:54<11:37, 12.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    81 runs   (    0.08 ms per token, 12490.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3421.03 ms /   194 tokens (   17.63 ms per token,    56.71 tokens per second)\n",
      "llama_print_timings:        eval time =    7011.94 ms /    80 runs   (   87.65 ms per token,    11.41 tokens per second)\n",
      "llama_print_timings:       total time =   10547.12 ms /   274 tokens\n",
      " 78%|███████▊  | 204/261 [32:05<11:00, 11.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.14 ms /    82 runs   (    0.07 ms per token, 13363.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3366.81 ms /   193 tokens (   17.44 ms per token,    57.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5750.02 ms /    81 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    9232.52 ms /   274 tokens\n",
      " 79%|███████▊  | 205/261 [32:14<10:09, 10.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    85 runs   (    0.07 ms per token, 13492.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3458.45 ms /   204 tokens (   16.95 ms per token,    58.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6065.78 ms /    84 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =    9642.93 ms /   288 tokens\n",
      " 79%|███████▉  | 206/261 [32:24<09:38, 10.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      16.78 ms /   210 runs   (    0.08 ms per token, 12516.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4006.66 ms /   227 tokens (   17.65 ms per token,    56.66 tokens per second)\n",
      "llama_print_timings:        eval time =   15515.04 ms /   209 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   19836.37 ms /   436 tokens\n",
      " 79%|███████▉  | 207/261 [32:44<11:58, 13.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.08 ms /   137 runs   (    0.08 ms per token, 12361.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3295.34 ms /   221 tokens (   14.91 ms per token,    67.06 tokens per second)\n",
      "llama_print_timings:        eval time =   11429.36 ms /   136 runs   (   84.04 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   14927.99 ms /   357 tokens\n",
      " 80%|███████▉  | 208/261 [32:59<12:11, 13.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.31 ms /    59 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3015.04 ms /   191 tokens (   15.79 ms per token,    63.35 tokens per second)\n",
      "llama_print_timings:        eval time =    3940.80 ms /    58 runs   (   67.94 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    7036.83 ms /   249 tokens\n",
      " 80%|████████  | 209/261 [33:06<10:12, 11.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    80 runs   (    0.07 ms per token, 13538.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3190.69 ms /   193 tokens (   16.53 ms per token,    60.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6507.59 ms /    79 runs   (   82.37 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    9813.43 ms /   272 tokens\n",
      " 80%|████████  | 210/261 [33:15<09:30, 11.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.10 ms /    65 runs   (    0.08 ms per token, 12747.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3350.39 ms /   201 tokens (   16.67 ms per token,    59.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5793.10 ms /    64 runs   (   90.52 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    9239.76 ms /   265 tokens\n",
      " 81%|████████  | 211/261 [33:25<08:50, 10.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    53 runs   (    0.07 ms per token, 13621.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3185.87 ms /   187 tokens (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3855.06 ms /    52 runs   (   74.14 ms per token,    13.49 tokens per second)\n",
      "llama_print_timings:       total time =    7115.15 ms /   239 tokens\n",
      " 81%|████████  | 212/261 [33:32<07:48,  9.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.98 ms /    81 runs   (    0.07 ms per token, 13554.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2970.02 ms /   188 tokens (   15.80 ms per token,    63.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6024.85 ms /    80 runs   (   75.31 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =    9106.79 ms /   268 tokens\n",
      " 82%|████████▏ | 213/261 [33:41<07:32,  9.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    71 runs   (    0.07 ms per token, 13477.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3286.08 ms /   205 tokens (   16.03 ms per token,    62.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5756.99 ms /    70 runs   (   82.24 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:       total time =    9144.60 ms /   275 tokens\n",
      " 82%|████████▏ | 214/261 [33:50<07:19,  9.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    61 runs   (    0.08 ms per token, 12861.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3349.74 ms /   194 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5294.32 ms /    60 runs   (   88.24 ms per token,    11.33 tokens per second)\n",
      "llama_print_timings:       total time =    8730.82 ms /   254 tokens\n",
      " 82%|████████▏ | 215/261 [33:59<07:01,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    81 runs   (    0.07 ms per token, 13511.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3239.36 ms /   195 tokens (   16.61 ms per token,    60.20 tokens per second)\n",
      "llama_print_timings:        eval time =    5485.20 ms /    80 runs   (   68.56 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    8833.57 ms /   275 tokens\n",
      " 83%|████████▎ | 216/261 [34:08<06:47,  9.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    69 runs   (    0.07 ms per token, 13497.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.82 ms /   196 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4635.19 ms /    68 runs   (   68.16 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    7905.82 ms /   264 tokens\n",
      " 83%|████████▎ | 217/261 [34:16<06:23,  8.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.45 ms /    76 runs   (    0.07 ms per token, 13934.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3206.68 ms /   208 tokens (   15.42 ms per token,    64.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5166.87 ms /    75 runs   (   68.89 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    8476.99 ms /   283 tokens\n",
      " 84%|████████▎ | 218/261 [34:24<06:11,  8.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    85 runs   (    0.07 ms per token, 13939.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.44 ms /   201 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5770.20 ms /    84 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    9063.78 ms /   285 tokens\n",
      " 84%|████████▍ | 219/261 [34:33<06:08,  8.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.27 ms /    59 runs   (    0.07 ms per token, 13827.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.31 ms /   201 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3947.50 ms /    58 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    7204.73 ms /   259 tokens\n",
      " 84%|████████▍ | 220/261 [34:40<05:40,  8.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.80 ms /    65 runs   (    0.07 ms per token, 13527.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2656.89 ms /   158 tokens (   16.82 ms per token,    59.47 tokens per second)\n",
      "llama_print_timings:        eval time =    4350.16 ms /    64 runs   (   67.97 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =    7096.98 ms /   222 tokens\n",
      " 85%|████████▍ | 221/261 [34:47<05:17,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.10 ms /    69 runs   (    0.07 ms per token, 13529.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3220.41 ms /   210 tokens (   15.34 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4647.50 ms /    68 runs   (   68.35 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =    7961.11 ms /   278 tokens\n",
      " 85%|████████▌ | 222/261 [34:55<05:10,  7.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    73 runs   (    0.07 ms per token, 14172.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2936.64 ms /   187 tokens (   15.70 ms per token,    63.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4889.72 ms /    72 runs   (   67.91 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    7923.92 ms /   259 tokens\n",
      " 85%|████████▌ | 223/261 [35:03<05:01,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.28 ms /    60 runs   (    0.07 ms per token, 14015.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.50 ms /   187 tokens (   15.65 ms per token,    63.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4044.84 ms /    59 runs   (   68.56 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    7053.99 ms /   246 tokens\n",
      " 86%|████████▌ | 224/261 [35:10<04:44,  7.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.77 ms /   131 runs   (    0.07 ms per token, 13405.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3178.65 ms /   196 tokens (   16.22 ms per token,    61.66 tokens per second)\n",
      "llama_print_timings:        eval time =    8969.51 ms /   130 runs   (   69.00 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   12330.77 ms /   326 tokens\n",
      " 86%|████████▌ | 225/261 [35:23<05:26,  9.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    72 runs   (    0.07 ms per token, 13556.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2656.36 ms /   160 tokens (   16.60 ms per token,    60.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.89 ms /    71 runs   (   67.51 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    7546.81 ms /   231 tokens\n",
      " 87%|████████▋ | 226/261 [35:30<05:01,  8.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      30.05 ms /   406 runs   (    0.07 ms per token, 13513.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5545.48 ms /   460 tokens (   12.06 ms per token,    82.95 tokens per second)\n",
      "llama_print_timings:        eval time =   30338.58 ms /   405 runs   (   74.91 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =   36668.67 ms /   865 tokens\n",
      " 87%|████████▋ | 227/261 [36:07<09:39, 17.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    87 runs   (    0.08 ms per token, 13039.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4218.53 ms /   230 tokens (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:        eval time =  194155.29 ms /    86 runs   ( 2257.62 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time =  198522.77 ms /   316 tokens\n",
      " 87%|████████▋ | 228/261 [39:25<39:18, 71.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.18 ms /    66 runs   (    0.08 ms per token, 12746.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7586.46 ms /   538 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_print_timings:        eval time =    4909.96 ms /    65 runs   (   75.54 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   12592.50 ms /   603 tokens\n",
      " 88%|████████▊ | 229/261 [39:38<28:42, 53.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      16.74 ms /   214 runs   (    0.08 ms per token, 12783.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8122.72 ms /   532 tokens (   15.27 ms per token,    65.50 tokens per second)\n",
      "llama_print_timings:        eval time =   17438.91 ms /   213 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   25872.42 ms /   745 tokens\n",
      " 88%|████████▊ | 230/261 [40:04<23:28, 45.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    75 runs   (    0.07 ms per token, 13943.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9411.16 ms /   567 tokens (   16.60 ms per token,    60.25 tokens per second)\n",
      "llama_print_timings:        eval time =    5391.56 ms /    74 runs   (   72.86 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time =   14909.19 ms /   641 tokens\n",
      " 89%|████████▊ | 231/261 [40:19<18:08, 36.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    72 runs   (    0.08 ms per token, 12190.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.81 ms /   153 tokens (   20.23 ms per token,    49.42 tokens per second)\n",
      "llama_print_timings:        eval time =    7307.02 ms /    71 runs   (  102.92 ms per token,     9.72 tokens per second)\n",
      "llama_print_timings:       total time =   10515.69 ms /   224 tokens\n",
      " 89%|████████▉ | 232/261 [40:29<13:48, 28.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /   102 runs   (    0.07 ms per token, 13517.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10286.93 ms /   585 tokens (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7586.20 ms /   101 runs   (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   18016.36 ms /   686 tokens\n",
      " 89%|████████▉ | 233/261 [40:47<11:51, 25.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    80 runs   (    0.08 ms per token, 13260.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6930.84 ms /   514 tokens (   13.48 ms per token,    74.16 tokens per second)\n",
      "llama_print_timings:        eval time =    6048.76 ms /    79 runs   (   76.57 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   13095.42 ms /   593 tokens\n",
      " 90%|████████▉ | 234/261 [41:00<09:46, 21.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    66 runs   (    0.07 ms per token, 13349.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8656.37 ms /   598 tokens (   14.48 ms per token,    69.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4796.28 ms /    65 runs   (   73.79 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   13543.96 ms /   663 tokens\n",
      " 90%|█████████ | 235/261 [41:14<08:20, 19.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    80 runs   (    0.08 ms per token, 13247.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4298.72 ms /   312 tokens (   13.78 ms per token,    72.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5820.29 ms /    79 runs   (   73.67 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   10231.68 ms /   391 tokens\n",
      " 90%|█████████ | 236/261 [41:24<06:53, 16.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.72 ms /   131 runs   (    0.07 ms per token, 13480.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6501.85 ms /   499 tokens (   13.03 ms per token,    76.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9908.00 ms /   130 runs   (   76.22 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:       total time =   16589.37 ms /   629 tokens\n",
      " 91%|█████████ | 237/261 [41:41<06:37, 16.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    74 runs   (    0.07 ms per token, 13754.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3231.57 ms /   192 tokens (   16.83 ms per token,    59.41 tokens per second)\n",
      "llama_print_timings:        eval time =    6232.46 ms /    73 runs   (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =    9574.67 ms /   265 tokens\n",
      " 91%|█████████ | 238/261 [41:50<05:32, 14.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.96 ms /    63 runs   (    0.08 ms per token, 12704.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3457.18 ms /   203 tokens (   17.03 ms per token,    58.72 tokens per second)\n",
      "llama_print_timings:        eval time =    5631.75 ms /    62 runs   (   90.83 ms per token,    11.01 tokens per second)\n",
      "llama_print_timings:       total time =    9179.49 ms /   265 tokens\n",
      " 92%|█████████▏| 239/261 [42:00<04:43, 12.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.87 ms /    92 runs   (    0.07 ms per token, 13399.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3502.60 ms /   211 tokens (   16.60 ms per token,    60.24 tokens per second)\n",
      "llama_print_timings:        eval time =    6406.11 ms /    91 runs   (   70.40 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =   10033.55 ms /   302 tokens\n",
      " 92%|█████████▏| 240/261 [42:10<04:12, 12.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.63 ms /    77 runs   (    0.07 ms per token, 13671.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3436.93 ms /   199 tokens (   17.27 ms per token,    57.90 tokens per second)\n",
      "llama_print_timings:        eval time =    5300.31 ms /    76 runs   (   69.74 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =    8840.47 ms /   275 tokens\n",
      " 92%|█████████▏| 241/261 [42:18<03:41, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    55 runs   (    0.07 ms per token, 13356.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.94 ms /   195 tokens (   16.92 ms per token,    59.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4047.95 ms /    54 runs   (   74.96 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =    7422.87 ms /   249 tokens\n",
      " 93%|█████████▎| 242/261 [42:26<03:09,  9.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.19 ms /    57 runs   (    0.07 ms per token, 13616.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3269.84 ms /   208 tokens (   15.72 ms per token,    63.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3730.95 ms /    56 runs   (   66.62 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7077.74 ms /   264 tokens\n",
      " 93%|█████████▎| 243/261 [42:33<02:43,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.39 ms /    61 runs   (    0.07 ms per token, 13901.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3408.52 ms /   195 tokens (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4056.30 ms /    60 runs   (   67.61 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    7545.32 ms /   255 tokens\n",
      " 93%|█████████▎| 244/261 [42:41<02:26,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    81 runs   (    0.07 ms per token, 13484.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.48 ms /   201 tokens (   16.17 ms per token,    61.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5997.32 ms /    80 runs   (   74.97 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =    9356.41 ms /   281 tokens\n",
      " 94%|█████████▍| 245/261 [42:50<02:21,  8.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.46 ms /    59 runs   (    0.08 ms per token, 13225.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3539.99 ms /   200 tokens (   17.70 ms per token,    56.50 tokens per second)\n",
      "llama_print_timings:        eval time =    4691.89 ms /    58 runs   (   80.89 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    8317.48 ms /   258 tokens\n",
      " 94%|█████████▍| 246/261 [42:58<02:10,  8.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.65 ms /    98 runs   (    0.09 ms per token, 11325.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3498.59 ms /   190 tokens (   18.41 ms per token,    54.31 tokens per second)\n",
      "llama_print_timings:        eval time =   10496.63 ms /    97 runs   (  108.21 ms per token,     9.24 tokens per second)\n",
      "llama_print_timings:       total time =   14141.59 ms /   287 tokens\n",
      " 95%|█████████▍| 247/261 [43:12<02:24, 10.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    63 runs   (    0.08 ms per token, 12497.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3910.30 ms /   202 tokens (   19.36 ms per token,    51.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6953.48 ms /    62 runs   (  112.15 ms per token,     8.92 tokens per second)\n",
      "llama_print_timings:       total time =   10958.13 ms /   264 tokens\n",
      " 95%|█████████▌| 248/261 [43:23<02:16, 10.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.93 ms /    51 runs   (    0.08 ms per token, 12993.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3198.86 ms /   191 tokens (   16.75 ms per token,    59.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3935.09 ms /    50 runs   (   78.70 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =    7205.78 ms /   241 tokens\n",
      " 95%|█████████▌| 249/261 [43:31<01:54,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.00 ms /    68 runs   (    0.07 ms per token, 13605.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3457.43 ms /   196 tokens (   17.64 ms per token,    56.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5000.84 ms /    67 runs   (   74.64 ms per token,    13.40 tokens per second)\n",
      "llama_print_timings:       total time =    8550.76 ms /   263 tokens\n",
      " 96%|█████████▌| 250/261 [43:39<01:41,  9.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    85 runs   (    0.07 ms per token, 13576.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3170.47 ms /   186 tokens (   17.05 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =    6024.18 ms /    84 runs   (   71.72 ms per token,    13.94 tokens per second)\n",
      "llama_print_timings:       total time =    9311.59 ms /   270 tokens\n",
      " 96%|█████████▌| 251/261 [43:48<01:32,  9.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.14 ms /    67 runs   (    0.08 ms per token, 13040.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3291.78 ms /   189 tokens (   17.42 ms per token,    57.42 tokens per second)\n",
      "llama_print_timings:        eval time =    5075.55 ms /    66 runs   (   76.90 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:       total time =    8459.48 ms /   255 tokens\n",
      " 97%|█████████▋| 252/261 [43:57<01:21,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.20 ms /    52 runs   (    0.08 ms per token, 12375.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3581.89 ms /   198 tokens (   18.09 ms per token,    55.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6144.84 ms /    51 runs   (  120.49 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =    9803.88 ms /   249 tokens\n",
      " 97%|█████████▋| 253/261 [44:07<01:14,  9.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    66 runs   (    0.08 ms per token, 12913.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.97 ms /   261 tokens (   16.07 ms per token,    62.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5379.33 ms /    65 runs   (   82.76 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    9665.62 ms /   326 tokens\n",
      " 97%|█████████▋| 254/261 [44:16<01:05,  9.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /   107 runs   (    0.08 ms per token, 13323.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8971.55 ms /   585 tokens (   15.34 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8394.06 ms /   106 runs   (   79.19 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   17520.47 ms /   691 tokens\n",
      " 98%|█████████▊| 255/261 [44:34<01:10, 11.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.83 ms /   143 runs   (    0.08 ms per token, 13202.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9004.50 ms /   597 tokens (   15.08 ms per token,    66.30 tokens per second)\n",
      "llama_print_timings:        eval time =   11337.83 ms /   142 runs   (   79.84 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   20548.34 ms /   739 tokens\n",
      " 98%|█████████▊| 256/261 [44:54<01:12, 14.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.20 ms /    69 runs   (    0.08 ms per token, 13269.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7775.48 ms /   533 tokens (   14.59 ms per token,    68.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5727.12 ms /    68 runs   (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =   13611.20 ms /   601 tokens\n",
      " 98%|█████████▊| 257/261 [45:08<00:56, 14.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      14.52 ms /   195 runs   (    0.07 ms per token, 13427.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5961.61 ms /   487 tokens (   12.24 ms per token,    81.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15081.12 ms /   194 runs   (   77.74 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   21322.46 ms /   681 tokens\n",
      " 99%|█████████▉| 258/261 [45:29<00:49, 16.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   176 runs   (    0.08 ms per token, 12646.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8205.51 ms /   555 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:        eval time =   12901.10 ms /   175 runs   (   73.72 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   21351.56 ms /   730 tokens\n",
      " 99%|█████████▉| 259/261 [45:51<00:35, 17.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    88 runs   (    0.07 ms per token, 13499.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8538.24 ms /   589 tokens (   14.50 ms per token,    68.98 tokens per second)\n",
      "llama_print_timings:        eval time =    6381.37 ms /    87 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   15039.46 ms /   676 tokens\n",
      "100%|█████████▉| 260/261 [46:06<00:17, 17.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    83 runs   (    0.07 ms per token, 13404.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8633.82 ms /   609 tokens (   14.18 ms per token,    70.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7969.59 ms /    82 runs   (   97.19 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   16725.52 ms /   691 tokens\n",
      "100%|██████████| 261/261 [46:23<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_splitter_1024 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      14.24 ms /   177 runs   (    0.08 ms per token, 12428.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20554.30 ms /   857 tokens (   23.98 ms per token,    41.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15493.65 ms /   176 runs   (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   36469.64 ms /  1033 tokens\n",
      "  1%|          | 1/105 [00:36<1:03:13, 36.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    67 runs   (    0.08 ms per token, 13096.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9079.79 ms /   565 tokens (   16.07 ms per token,    62.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6003.13 ms /    66 runs   (   90.96 ms per token,    10.99 tokens per second)\n",
      "llama_print_timings:       total time =   15245.31 ms /   631 tokens\n",
      "  2%|▏         | 2/105 [00:51<41:11, 23.99s/it]  Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    78 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11197.73 ms /   882 tokens (   12.70 ms per token,    78.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5666.11 ms /    77 runs   (   73.59 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   16966.86 ms /   959 tokens\n",
      "  3%|▎         | 3/105 [01:08<35:20, 20.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    61 runs   (    0.08 ms per token, 13310.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4921.91 ms /   400 tokens (   12.30 ms per token,    81.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4297.47 ms /    60 runs   (   71.62 ms per token,    13.96 tokens per second)\n",
      "llama_print_timings:       total time =    9362.03 ms /   460 tokens\n",
      "  4%|▍         | 4/105 [01:18<27:24, 16.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.55 ms /    85 runs   (    0.08 ms per token, 12975.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15085.05 ms /  1102 tokens (   13.69 ms per token,    73.05 tokens per second)\n",
      "llama_print_timings:        eval time =    6155.90 ms /    84 runs   (   73.28 ms per token,    13.65 tokens per second)\n",
      "llama_print_timings:       total time =   21442.97 ms /  1186 tokens\n",
      "  5%|▍         | 5/105 [01:39<30:14, 18.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.42 ms /   116 runs   (    0.07 ms per token, 13784.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12298.48 ms /   934 tokens (   13.17 ms per token,    75.94 tokens per second)\n",
      "llama_print_timings:        eval time =   10598.08 ms /   115 runs   (   92.16 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   23073.97 ms /  1049 tokens\n",
      "  6%|▌         | 6/105 [02:02<32:42, 19.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    88 runs   (    0.07 ms per token, 13732.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10763.48 ms /   850 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6412.32 ms /    87 runs   (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   17300.87 ms /   937 tokens\n",
      "  7%|▋         | 7/105 [02:19<31:01, 19.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    68 runs   (    0.08 ms per token, 12741.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9308.88 ms /   588 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5804.33 ms /    67 runs   (   86.63 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =   15232.36 ms /   655 tokens\n",
      "  8%|▊         | 8/105 [02:35<28:46, 17.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.51 ms /    65 runs   (    0.07 ms per token, 14418.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10800.46 ms /   853 tokens (   12.66 ms per token,    78.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4700.40 ms /    64 runs   (   73.44 ms per token,    13.62 tokens per second)\n",
      "llama_print_timings:       total time =   15585.06 ms /   917 tokens\n",
      "  9%|▊         | 9/105 [02:50<27:22, 17.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    96 runs   (    0.08 ms per token, 12917.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5084.15 ms /   358 tokens (   14.20 ms per token,    70.41 tokens per second)\n",
      "llama_print_timings:        eval time =   13159.93 ms /    95 runs   (  138.53 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:       total time =   18405.04 ms /   453 tokens\n",
      " 10%|▉         | 10/105 [03:09<27:43, 17.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    63 runs   (    0.08 ms per token, 12778.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4320.70 ms /   315 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4887.71 ms /    62 runs   (   78.83 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =    9326.21 ms /   377 tokens\n",
      " 10%|█         | 11/105 [03:18<23:30, 15.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.94 ms /    77 runs   (    0.08 ms per token, 12954.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4824.52 ms /   322 tokens (   14.98 ms per token,    66.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6467.26 ms /    76 runs   (   85.10 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   11399.64 ms /   398 tokens\n",
      " 11%|█▏        | 12/105 [03:29<21:33, 13.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    71 runs   (    0.08 ms per token, 13080.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4767.07 ms /   323 tokens (   14.76 ms per token,    67.76 tokens per second)\n",
      "llama_print_timings:        eval time =    6075.86 ms /    70 runs   (   86.80 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =   10945.08 ms /   393 tokens\n",
      " 12%|█▏        | 13/105 [03:40<19:57, 13.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.15 ms /    71 runs   (    0.09 ms per token, 11550.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5151.63 ms /   378 tokens (   13.63 ms per token,    73.37 tokens per second)\n",
      "llama_print_timings:        eval time =   14830.27 ms /    70 runs   (  211.86 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   20112.81 ms /   448 tokens\n",
      " 13%|█▎        | 14/105 [04:00<22:59, 15.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.16 ms /    67 runs   (    0.08 ms per token, 12984.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9398.53 ms /   644 tokens (   14.59 ms per token,    68.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5722.45 ms /    66 runs   (   86.70 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =   15259.00 ms /   710 tokens\n",
      " 14%|█▍        | 15/105 [04:16<22:47, 15.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.84 ms /    82 runs   (    0.08 ms per token, 11993.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11035.31 ms /   756 tokens (   14.60 ms per token,    68.51 tokens per second)\n",
      "llama_print_timings:        eval time =    8669.82 ms /    81 runs   (  107.03 ms per token,     9.34 tokens per second)\n",
      "llama_print_timings:       total time =   19885.55 ms /   837 tokens\n",
      " 15%|█▌        | 16/105 [04:36<24:38, 16.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    56 runs   (    0.07 ms per token, 13526.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4582.43 ms /   338 tokens (   13.56 ms per token,    73.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4088.23 ms /    55 runs   (   74.33 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =    8745.56 ms /   393 tokens\n",
      " 16%|█▌        | 17/105 [04:44<20:53, 14.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    82 runs   (    0.07 ms per token, 13746.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12017.75 ms /   949 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6062.76 ms /    81 runs   (   74.85 ms per token,    13.36 tokens per second)\n",
      "llama_print_timings:       total time =   18207.33 ms /  1030 tokens\n",
      " 17%|█▋        | 18/105 [05:03<22:23, 15.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    81 runs   (    0.07 ms per token, 13846.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4484.09 ms /   341 tokens (   13.15 ms per token,    76.05 tokens per second)\n",
      "llama_print_timings:        eval time =    5660.04 ms /    80 runs   (   70.75 ms per token,    14.13 tokens per second)\n",
      "llama_print_timings:       total time =   10251.57 ms /   421 tokens\n",
      " 18%|█▊        | 19/105 [05:13<19:53, 13.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   182 runs   (    0.09 ms per token, 11735.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3515.39 ms /   238 tokens (   14.77 ms per token,    67.70 tokens per second)\n",
      "llama_print_timings:        eval time =   15927.05 ms /   181 runs   (   87.99 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   19766.08 ms /   419 tokens\n",
      " 19%|█▉        | 20/105 [05:33<22:10, 15.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.07 ms /   101 runs   (    0.08 ms per token, 12518.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15536.59 ms /  1083 tokens (   14.35 ms per token,    69.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9227.53 ms /   100 runs   (   92.28 ms per token,    10.84 tokens per second)\n",
      "llama_print_timings:       total time =   24946.59 ms /  1183 tokens\n",
      " 20%|██        | 21/105 [05:58<25:49, 18.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.85 ms /   110 runs   (    0.08 ms per token, 12432.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12172.04 ms /   808 tokens (   15.06 ms per token,    66.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9465.85 ms /   109 runs   (   86.84 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =   21811.00 ms /   917 tokens\n",
      " 21%|██        | 22/105 [06:19<26:54, 19.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    71 runs   (    0.07 ms per token, 13373.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4226.15 ms /   293 tokens (   14.42 ms per token,    69.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5265.66 ms /    70 runs   (   75.22 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:       total time =    9590.16 ms /   363 tokens\n",
      " 22%|██▏       | 23/105 [06:29<22:32, 16.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.70 ms /    78 runs   (    0.07 ms per token, 13696.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4247.79 ms /   296 tokens (   14.35 ms per token,    69.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5520.36 ms /    77 runs   (   71.69 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =    9869.79 ms /   373 tokens\n",
      " 23%|██▎       | 24/105 [06:39<19:35, 14.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    64 runs   (    0.07 ms per token, 13976.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4120.01 ms /   308 tokens (   13.38 ms per token,    74.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.20 ms /    63 runs   (   68.88 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    8540.67 ms /   371 tokens\n",
      " 24%|██▍       | 25/105 [06:47<16:57, 12.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    78 runs   (    0.07 ms per token, 13499.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4065.81 ms /   315 tokens (   12.91 ms per token,    77.48 tokens per second)\n",
      "llama_print_timings:        eval time =    5348.35 ms /    77 runs   (   69.46 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =    9512.26 ms /   392 tokens\n",
      " 25%|██▍       | 26/105 [06:57<15:28, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.22 ms /    56 runs   (    0.08 ms per token, 13263.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3847.68 ms /   259 tokens (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_print_timings:        eval time =    5008.56 ms /    55 runs   (   91.06 ms per token,    10.98 tokens per second)\n",
      "llama_print_timings:       total time =    8936.40 ms /   314 tokens\n",
      " 26%|██▌       | 27/105 [07:06<14:11, 10.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.99 ms /    81 runs   (    0.07 ms per token, 13518.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17521.50 ms /  1057 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7041.06 ms /    80 runs   (   88.01 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   24709.36 ms /  1137 tokens\n",
      " 27%|██▋       | 28/105 [07:31<19:19, 15.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.92 ms /   123 runs   (    0.08 ms per token, 12396.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12603.20 ms /   930 tokens (   13.55 ms per token,    73.79 tokens per second)\n",
      "llama_print_timings:        eval time =   10309.66 ms /   122 runs   (   84.51 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =   23095.25 ms /  1052 tokens\n",
      " 28%|██▊       | 29/105 [07:54<22:07, 17.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.87 ms /   104 runs   (    0.08 ms per token, 13211.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9833.53 ms /   697 tokens (   14.11 ms per token,    70.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7737.02 ms /   103 runs   (   75.12 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   17768.61 ms /   800 tokens\n",
      " 29%|██▊       | 30/105 [08:11<21:57, 17.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    84 runs   (    0.07 ms per token, 13401.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4805.73 ms /   371 tokens (   12.95 ms per token,    77.20 tokens per second)\n",
      "llama_print_timings:        eval time =    6118.11 ms /    83 runs   (   73.71 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   11038.59 ms /   454 tokens\n",
      " 30%|██▉       | 31/105 [08:23<19:14, 15.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.20 ms /   111 runs   (    0.07 ms per token, 13531.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9006.80 ms /   626 tokens (   14.39 ms per token,    69.50 tokens per second)\n",
      "llama_print_timings:        eval time =    8161.48 ms /   110 runs   (   74.20 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =   17318.64 ms /   736 tokens\n",
      " 30%|███       | 32/105 [08:40<19:36, 16.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.11 ms /   127 runs   (    0.07 ms per token, 13939.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9731.61 ms /   711 tokens (   13.69 ms per token,    73.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9324.93 ms /   126 runs   (   74.01 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   19227.06 ms /   837 tokens\n",
      " 31%|███▏      | 33/105 [08:59<20:27, 17.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    61 runs   (    0.07 ms per token, 13994.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4126.23 ms /   304 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4175.11 ms /    60 runs   (   69.59 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =    8379.34 ms /   364 tokens\n",
      " 32%|███▏      | 34/105 [09:07<17:06, 14.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    72 runs   (    0.07 ms per token, 13667.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4071.03 ms /   316 tokens (   12.88 ms per token,    77.62 tokens per second)\n",
      "llama_print_timings:        eval time =    5331.68 ms /    71 runs   (   75.09 ms per token,    13.32 tokens per second)\n",
      "llama_print_timings:       total time =    9496.14 ms /   387 tokens\n",
      " 33%|███▎      | 35/105 [09:17<15:07, 12.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.88 ms /    61 runs   (    0.08 ms per token, 12487.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4322.47 ms /   314 tokens (   13.77 ms per token,    72.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5330.58 ms /    60 runs   (   88.84 ms per token,    11.26 tokens per second)\n",
      "llama_print_timings:       total time =    9741.41 ms /   374 tokens\n",
      " 34%|███▍      | 36/105 [09:27<13:48, 12.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    68 runs   (    0.07 ms per token, 13580.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4607.35 ms /   310 tokens (   14.86 ms per token,    67.28 tokens per second)\n",
      "llama_print_timings:        eval time =    5317.82 ms /    67 runs   (   79.37 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   10021.82 ms /   377 tokens\n",
      " 35%|███▌      | 37/105 [09:37<12:55, 11.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.14 ms /    98 runs   (    0.07 ms per token, 13721.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4367.67 ms /   328 tokens (   13.32 ms per token,    75.10 tokens per second)\n",
      "llama_print_timings:        eval time =    7200.18 ms /    97 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   11699.65 ms /   425 tokens\n",
      " 36%|███▌      | 38/105 [09:48<12:50, 11.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.70 ms /    76 runs   (    0.13 ms per token,  7836.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4418.74 ms /   314 tokens (   14.07 ms per token,    71.06 tokens per second)\n",
      "llama_print_timings:        eval time =    6237.05 ms /    75 runs   (   83.16 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =   10771.34 ms /   389 tokens\n",
      " 37%|███▋      | 39/105 [09:59<12:24, 11.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.60 ms /    86 runs   (    0.08 ms per token, 13022.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4441.21 ms /   324 tokens (   13.71 ms per token,    72.95 tokens per second)\n",
      "llama_print_timings:        eval time =    6506.57 ms /    85 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   11060.68 ms /   409 tokens\n",
      " 38%|███▊      | 40/105 [10:10<12:09, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    62 runs   (    0.07 ms per token, 13632.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3620.27 ms /   194 tokens (   18.66 ms per token,    53.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4190.22 ms /    61 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    7942.11 ms /   255 tokens\n",
      " 39%|███▉      | 41/105 [10:18<10:55, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.80 ms /    68 runs   (    0.07 ms per token, 14157.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10070.95 ms /   775 tokens (   12.99 ms per token,    76.95 tokens per second)\n",
      "llama_print_timings:        eval time =    4937.98 ms /    67 runs   (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   15098.97 ms /   842 tokens\n",
      " 40%|████      | 42/105 [10:33<12:16, 11.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.33 ms /    73 runs   (    0.07 ms per token, 13693.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4436.19 ms /   315 tokens (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_print_timings:        eval time =    5065.36 ms /    72 runs   (   70.35 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =    9597.98 ms /   387 tokens\n",
      " 41%|████      | 43/105 [10:43<11:26, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    69 runs   (    0.07 ms per token, 13942.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4486.76 ms /   311 tokens (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4871.93 ms /    68 runs   (   71.65 ms per token,    13.96 tokens per second)\n",
      "llama_print_timings:       total time =    9452.34 ms /   379 tokens\n",
      " 42%|████▏     | 44/105 [10:52<10:45, 10.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    64 runs   (    0.07 ms per token, 13736.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4213.04 ms /   310 tokens (   13.59 ms per token,    73.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.82 ms /    63 runs   (   79.03 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    9285.74 ms /   373 tokens\n",
      " 43%|████▎     | 45/105 [11:02<10:11, 10.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.06 ms /    55 runs   (    0.07 ms per token, 13546.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4146.68 ms /   309 tokens (   13.42 ms per token,    74.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4321.90 ms /    54 runs   (   80.04 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =    8542.50 ms /   363 tokens\n",
      " 44%|████▍     | 46/105 [11:10<09:32,  9.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    64 runs   (    0.08 ms per token, 12497.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4440.80 ms /   319 tokens (   13.92 ms per token,    71.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5867.98 ms /    63 runs   (   93.14 ms per token,    10.74 tokens per second)\n",
      "llama_print_timings:       total time =   10408.36 ms /   382 tokens\n",
      " 45%|████▍     | 47/105 [11:21<09:35,  9.92s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    81 runs   (    0.07 ms per token, 13477.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4493.69 ms /   324 tokens (   13.87 ms per token,    72.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5919.42 ms /    80 runs   (   73.99 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   10522.54 ms /   404 tokens\n",
      " 46%|████▌     | 48/105 [11:31<09:35, 10.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    76 runs   (    0.07 ms per token, 13556.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4740.74 ms /   324 tokens (   14.63 ms per token,    68.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5555.56 ms /    75 runs   (   74.07 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =   10400.82 ms /   399 tokens\n",
      " 47%|████▋     | 49/105 [11:42<09:31, 10.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    66 runs   (    0.07 ms per token, 13403.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4572.53 ms /   333 tokens (   13.73 ms per token,    72.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5701.34 ms /    65 runs   (   87.71 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =   10362.27 ms /   398 tokens\n",
      " 48%|████▊     | 50/105 [11:52<09:23, 10.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.62 ms /    77 runs   (    0.07 ms per token, 13688.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.80 ms /   314 tokens (   13.35 ms per token,    74.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5755.48 ms /    76 runs   (   75.73 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:       total time =   10053.69 ms /   390 tokens\n",
      " 49%|████▊     | 51/105 [12:02<09:10, 10.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.26 ms /   131 runs   (    0.08 ms per token, 12768.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4514.67 ms /   332 tokens (   13.60 ms per token,    73.54 tokens per second)\n",
      "llama_print_timings:        eval time =    9574.88 ms /   130 runs   (   73.65 ms per token,    13.58 tokens per second)\n",
      "llama_print_timings:       total time =   14270.87 ms /   462 tokens\n",
      " 50%|████▉     | 52/105 [12:16<10:05, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.77 ms /    81 runs   (    0.07 ms per token, 14043.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.86 ms /   320 tokens (   13.11 ms per token,    76.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5613.53 ms /    80 runs   (   70.17 ms per token,    14.25 tokens per second)\n",
      "llama_print_timings:       total time =    9914.02 ms /   400 tokens\n",
      " 50%|█████     | 53/105 [12:26<09:30, 10.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    67 runs   (    0.07 ms per token, 13546.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4648.98 ms /   331 tokens (   14.05 ms per token,    71.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4758.04 ms /    66 runs   (   72.09 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =    9496.04 ms /   397 tokens\n",
      " 51%|█████▏    | 54/105 [12:36<08:56, 10.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    91 runs   (    0.07 ms per token, 14110.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4488.54 ms /   338 tokens (   13.28 ms per token,    75.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6494.47 ms /    90 runs   (   72.16 ms per token,    13.86 tokens per second)\n",
      "llama_print_timings:       total time =   11102.74 ms /   428 tokens\n",
      " 52%|█████▏    | 55/105 [12:47<08:55, 10.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    71 runs   (    0.08 ms per token, 13001.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4220.82 ms /   319 tokens (   13.23 ms per token,    75.58 tokens per second)\n",
      "llama_print_timings:        eval time =    7029.03 ms /    70 runs   (  100.41 ms per token,     9.96 tokens per second)\n",
      "llama_print_timings:       total time =   11354.66 ms /   389 tokens\n",
      " 53%|█████▎    | 56/105 [12:58<08:54, 10.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.00 ms /    67 runs   (    0.07 ms per token, 13389.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4883.53 ms /   335 tokens (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5277.21 ms /    66 runs   (   79.96 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =   10257.99 ms /   401 tokens\n",
      " 54%|█████▍    | 57/105 [13:08<08:34, 10.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    61 runs   (    0.09 ms per token, 11026.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6070.87 ms /   315 tokens (   19.27 ms per token,    51.89 tokens per second)\n",
      "llama_print_timings:        eval time =   10581.76 ms /    60 runs   (  176.36 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   16765.97 ms /   375 tokens\n",
      " 55%|█████▌    | 58/105 [13:25<09:48, 12.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    75 runs   (    0.08 ms per token, 12481.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4882.95 ms /   328 tokens (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:        eval time =    6817.42 ms /    74 runs   (   92.13 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   11813.78 ms /   402 tokens\n",
      " 56%|█████▌    | 59/105 [13:37<09:26, 12.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    69 runs   (    0.08 ms per token, 11977.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4382.81 ms /   310 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_print_timings:        eval time =    8225.75 ms /    68 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   12738.98 ms /   378 tokens\n",
      " 57%|█████▋    | 60/105 [13:50<09:20, 12.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.26 ms /    68 runs   (    0.08 ms per token, 12920.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4881.43 ms /   323 tokens (   15.11 ms per token,    66.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5568.03 ms /    67 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   10549.72 ms /   390 tokens\n",
      " 58%|█████▊    | 61/105 [14:00<08:42, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    88 runs   (    0.08 ms per token, 12454.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5372.31 ms /   313 tokens (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:        eval time =    8534.59 ms /    87 runs   (   98.10 ms per token,    10.19 tokens per second)\n",
      "llama_print_timings:       total time =   14034.08 ms /   400 tokens\n",
      " 59%|█████▉    | 62/105 [14:14<08:58, 12.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.22 ms /    79 runs   (    0.08 ms per token, 12705.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5511.11 ms /   321 tokens (   17.17 ms per token,    58.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7583.33 ms /    78 runs   (   97.22 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   13208.67 ms /   399 tokens\n",
      " 60%|██████    | 63/105 [14:28<08:54, 12.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.18 ms /    68 runs   (    0.08 ms per token, 13137.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5034.60 ms /   314 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5964.39 ms /    67 runs   (   89.02 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   11094.65 ms /   381 tokens\n",
      " 61%|██████    | 64/105 [14:39<08:22, 12.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    67 runs   (    0.08 ms per token, 13196.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4702.23 ms /   328 tokens (   14.34 ms per token,    69.75 tokens per second)\n",
      "llama_print_timings:        eval time =    5756.94 ms /    66 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =   10554.38 ms /   394 tokens\n",
      " 62%|██████▏   | 65/105 [14:49<07:49, 11.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.98 ms /    70 runs   (    0.07 ms per token, 14061.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4376.77 ms /   322 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4817.30 ms /    69 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =    9284.69 ms /   391 tokens\n",
      " 63%|██████▎   | 66/105 [14:59<07:09, 11.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /   145 runs   (    0.08 ms per token, 13274.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5049.51 ms /   320 tokens (   15.78 ms per token,    63.37 tokens per second)\n",
      "llama_print_timings:        eval time =   13034.02 ms /   144 runs   (   90.51 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =   18287.23 ms /   464 tokens\n",
      " 64%|██████▍   | 67/105 [15:17<08:21, 13.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    62 runs   (    0.07 ms per token, 13540.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4503.94 ms /   322 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_print_timings:        eval time =    5259.46 ms /    61 runs   (   86.22 ms per token,    11.60 tokens per second)\n",
      "llama_print_timings:       total time =    9850.95 ms /   383 tokens\n",
      " 65%|██████▍   | 68/105 [15:27<07:31, 12.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    85 runs   (    0.07 ms per token, 13634.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4479.93 ms /   305 tokens (   14.69 ms per token,    68.08 tokens per second)\n",
      "llama_print_timings:        eval time =    6544.20 ms /    84 runs   (   77.91 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =   11138.60 ms /   389 tokens\n",
      " 66%|██████▌   | 69/105 [15:38<07:07, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.44 ms /   122 runs   (    0.08 ms per token, 12919.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15791.34 ms /  1139 tokens (   13.86 ms per token,    72.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8874.82 ms /   121 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   24903.48 ms /  1260 tokens\n",
      " 67%|██████▋   | 70/105 [16:03<09:12, 15.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    78 runs   (    0.08 ms per token, 13087.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8680.26 ms /   589 tokens (   14.74 ms per token,    67.86 tokens per second)\n",
      "llama_print_timings:        eval time =    6454.06 ms /    77 runs   (   83.82 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   15250.63 ms /   666 tokens\n",
      " 68%|██████▊   | 71/105 [16:18<08:51, 15.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.60 ms /   173 runs   (    0.07 ms per token, 13732.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4675.94 ms /   327 tokens (   14.30 ms per token,    69.93 tokens per second)\n",
      "llama_print_timings:        eval time =   12378.50 ms /   172 runs   (   71.97 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =   17326.14 ms /   499 tokens\n",
      " 69%|██████▊   | 72/105 [16:35<08:52, 16.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.78 ms /   158 runs   (    0.07 ms per token, 13408.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4552.41 ms /   350 tokens (   13.01 ms per token,    76.88 tokens per second)\n",
      "llama_print_timings:        eval time =   12186.29 ms /   157 runs   (   77.62 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =   16960.40 ms /   507 tokens\n",
      " 70%|██████▉   | 73/105 [16:52<08:44, 16.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.07 ms /    84 runs   (    0.07 ms per token, 13831.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4379.36 ms /   336 tokens (   13.03 ms per token,    76.72 tokens per second)\n",
      "llama_print_timings:        eval time =    5774.36 ms /    83 runs   (   69.57 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =   10261.96 ms /   419 tokens\n",
      " 70%|███████   | 74/105 [17:03<07:31, 14.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    60 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4432.49 ms /   337 tokens (   13.15 ms per token,    76.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4128.95 ms /    59 runs   (   69.98 ms per token,    14.29 tokens per second)\n",
      "llama_print_timings:       total time =    8640.77 ms /   396 tokens\n",
      " 71%|███████▏  | 75/105 [17:11<06:23, 12.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.16 ms /    58 runs   (    0.07 ms per token, 13949.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4367.05 ms /   327 tokens (   13.35 ms per token,    74.88 tokens per second)\n",
      "llama_print_timings:        eval time =    3929.28 ms /    57 runs   (   68.93 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =    8370.63 ms /   384 tokens\n",
      " 72%|███████▏  | 76/105 [17:20<05:32, 11.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.05 ms /    75 runs   (    0.08 ms per token, 12400.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5002.88 ms /   320 tokens (   15.63 ms per token,    63.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5766.64 ms /    74 runs   (   77.93 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   10875.72 ms /   394 tokens\n",
      " 73%|███████▎  | 77/105 [17:31<05:15, 11.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.97 ms /    76 runs   (    0.08 ms per token, 12732.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4720.40 ms /   318 tokens (   14.84 ms per token,    67.37 tokens per second)\n",
      "llama_print_timings:        eval time =    6327.79 ms /    75 runs   (   84.37 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =   11162.61 ms /   393 tokens\n",
      " 74%|███████▍  | 78/105 [17:42<05:03, 11.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.80 ms /    52 runs   (    0.07 ms per token, 13691.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2654.93 ms /   146 tokens (   18.18 ms per token,    54.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3493.33 ms /    51 runs   (   68.50 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =    6273.22 ms /   197 tokens\n",
      " 75%|███████▌  | 79/105 [17:48<04:13,  9.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    92 runs   (    0.07 ms per token, 13795.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5193.46 ms /   407 tokens (   12.76 ms per token,    78.37 tokens per second)\n",
      "llama_print_timings:        eval time =    7173.16 ms /    91 runs   (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =   12492.71 ms /   498 tokens\n",
      " 76%|███████▌  | 80/105 [18:00<04:24, 10.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    59 runs   (    0.07 ms per token, 13597.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5549.97 ms /   387 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4262.49 ms /    58 runs   (   73.49 ms per token,    13.61 tokens per second)\n",
      "llama_print_timings:       total time =    9893.96 ms /   445 tokens\n",
      " 77%|███████▋  | 81/105 [18:10<04:09, 10.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    75 runs   (    0.07 ms per token, 13748.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4683.48 ms /   355 tokens (   13.19 ms per token,    75.80 tokens per second)\n",
      "llama_print_timings:        eval time =    5188.92 ms /    74 runs   (   70.12 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =    9971.89 ms /   429 tokens\n",
      " 78%|███████▊  | 82/105 [18:20<03:55, 10.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.88 ms /    76 runs   (    0.08 ms per token, 12929.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4334.12 ms /   334 tokens (   12.98 ms per token,    77.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5577.45 ms /    75 runs   (   74.37 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =   10013.67 ms /   409 tokens\n",
      " 79%|███████▉  | 83/105 [18:30<03:44, 10.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.35 ms /   126 runs   (    0.07 ms per token, 13477.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4757.94 ms /   380 tokens (   12.52 ms per token,    79.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9321.91 ms /   125 runs   (   74.58 ms per token,    13.41 tokens per second)\n",
      "llama_print_timings:       total time =   14254.99 ms /   505 tokens\n",
      " 80%|████████  | 84/105 [18:45<03:59, 11.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    90 runs   (    0.08 ms per token, 13090.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4541.60 ms /   327 tokens (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7609.56 ms /    89 runs   (   85.50 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   12289.77 ms /   416 tokens\n",
      " 81%|████████  | 85/105 [18:57<03:53, 11.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.35 ms /    88 runs   (    0.07 ms per token, 13862.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4376.23 ms /   320 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6058.78 ms /    87 runs   (   69.64 ms per token,    14.36 tokens per second)\n",
      "llama_print_timings:       total time =   10552.40 ms /   407 tokens\n",
      " 82%|████████▏ | 86/105 [19:07<03:35, 11.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    62 runs   (    0.08 ms per token, 13301.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4415.28 ms /   325 tokens (   13.59 ms per token,    73.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4560.50 ms /    61 runs   (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =    9060.40 ms /   386 tokens\n",
      " 83%|████████▎ | 87/105 [19:17<03:11, 10.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.05 ms /    54 runs   (    0.08 ms per token, 13326.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4559.60 ms /   321 tokens (   14.20 ms per token,    70.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4064.79 ms /    53 runs   (   76.69 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:       total time =    8701.39 ms /   374 tokens\n",
      " 84%|████████▍ | 88/105 [19:25<02:51, 10.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    78 runs   (    0.08 ms per token, 12590.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4573.68 ms /   341 tokens (   13.41 ms per token,    74.56 tokens per second)\n",
      "llama_print_timings:        eval time =    6698.34 ms /    77 runs   (   86.99 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =   11387.57 ms /   418 tokens\n",
      " 85%|████████▍ | 89/105 [19:37<02:47, 10.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.17 ms /    67 runs   (    0.08 ms per token, 12961.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4819.09 ms /   365 tokens (   13.20 ms per token,    75.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.91 ms /    66 runs   (   72.62 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    9707.50 ms /   431 tokens\n",
      " 86%|████████▌ | 90/105 [19:46<02:33, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.65 ms /    75 runs   (    0.08 ms per token, 13281.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14919.27 ms /  1064 tokens (   14.02 ms per token,    71.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5777.17 ms /    74 runs   (   78.07 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =   20799.71 ms /  1138 tokens\n",
      " 87%|████████▋ | 91/105 [20:07<03:07, 13.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    91 runs   (    0.07 ms per token, 13982.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15692.59 ms /  1106 tokens (   14.19 ms per token,    70.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6643.57 ms /    90 runs   (   73.82 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   22460.06 ms /  1196 tokens\n",
      " 88%|████████▊ | 92/105 [20:30<03:29, 16.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.85 ms /   106 runs   (    0.07 ms per token, 13503.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15377.62 ms /  1147 tokens (   13.41 ms per token,    74.59 tokens per second)\n",
      "llama_print_timings:        eval time =    8463.62 ms /   105 runs   (   80.61 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   23991.32 ms /  1252 tokens\n",
      " 89%|████████▊ | 93/105 [20:54<03:41, 18.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.89 ms /    90 runs   (    0.08 ms per token, 13062.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17751.96 ms /  1205 tokens (   14.73 ms per token,    67.88 tokens per second)\n",
      "llama_print_timings:        eval time =    8864.87 ms /    89 runs   (   99.61 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   26761.35 ms /  1294 tokens\n",
      " 90%|████████▉ | 94/105 [21:20<03:50, 20.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.60 ms /   113 runs   (    0.08 ms per token, 11769.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15265.20 ms /  1086 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9282.77 ms /   112 runs   (   82.88 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   24721.91 ms /  1198 tokens\n",
      " 90%|█████████ | 95/105 [21:45<03:41, 22.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    64 runs   (    0.08 ms per token, 13195.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4773.98 ms /   336 tokens (   14.21 ms per token,    70.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5235.06 ms /    63 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   10118.43 ms /   399 tokens\n",
      " 91%|█████████▏| 96/105 [21:55<02:46, 18.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.73 ms /    84 runs   (    0.08 ms per token, 12477.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4878.62 ms /   330 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:        eval time =    7240.31 ms /    83 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =   12242.22 ms /   413 tokens\n",
      " 92%|█████████▏| 97/105 [22:08<02:13, 16.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    73 runs   (    0.07 ms per token, 14070.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4952.04 ms /   331 tokens (   14.96 ms per token,    66.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5470.87 ms /    72 runs   (   75.98 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time =   10520.57 ms /   403 tokens\n",
      " 93%|█████████▎| 98/105 [22:18<01:43, 14.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.40 ms /    89 runs   (    0.07 ms per token, 13901.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4116.96 ms /   320 tokens (   12.87 ms per token,    77.73 tokens per second)\n",
      "llama_print_timings:        eval time =    6144.33 ms /    88 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =   10377.72 ms /   408 tokens\n",
      " 94%|█████████▍| 99/105 [22:28<01:20, 13.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    67 runs   (    0.07 ms per token, 13640.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4136.23 ms /   320 tokens (   12.93 ms per token,    77.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5365.87 ms /    66 runs   (   81.30 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    9592.95 ms /   386 tokens\n",
      " 95%|█████████▌| 100/105 [22:38<01:01, 12.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    66 runs   (    0.08 ms per token, 13163.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3915.14 ms /   273 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4644.90 ms /    65 runs   (   71.46 ms per token,    13.99 tokens per second)\n",
      "llama_print_timings:       total time =    8652.12 ms /   338 tokens\n",
      " 96%|█████████▌| 101/105 [22:47<00:44, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    91 runs   (    0.08 ms per token, 12250.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15110.19 ms /  1108 tokens (   13.64 ms per token,    73.33 tokens per second)\n",
      "llama_print_timings:        eval time =    9447.23 ms /    90 runs   (  104.97 ms per token,     9.53 tokens per second)\n",
      "llama_print_timings:       total time =   24695.74 ms /  1198 tokens\n",
      " 97%|█████████▋| 102/105 [23:11<00:45, 15.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.87 ms /   163 runs   (    0.07 ms per token, 13735.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11897.76 ms /   946 tokens (   12.58 ms per token,    79.51 tokens per second)\n",
      "llama_print_timings:        eval time =   12070.48 ms /   162 runs   (   74.51 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   24195.03 ms /  1108 tokens\n",
      " 98%|█████████▊| 103/105 [23:36<00:35, 17.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.10 ms /    98 runs   (    0.07 ms per token, 13806.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14670.87 ms /  1086 tokens (   13.51 ms per token,    74.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8289.89 ms /    97 runs   (   85.46 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   23099.07 ms /  1183 tokens\n",
      " 99%|█████████▉| 104/105 [23:59<00:19, 19.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    99 runs   (    0.08 ms per token, 13201.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10012.97 ms /   681 tokens (   14.70 ms per token,    68.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7857.40 ms /    98 runs   (   80.18 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =   18013.18 ms /   779 tokens\n",
      "100%|██████████| 105/105 [24:17<00:00, 13.88s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results_dict = {}\n",
    "for parser_name, parser in parsers.items():\n",
    "    print(parser_name, \"\\n\")\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    qa_dataset = generate_question_context_pairs(\n",
    "        nodes,\n",
    "        llm=llm,\n",
    "        num_questions_per_chunk=2\n",
    "    )\n",
    "\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever)\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)  # Can't put this line in a function otherwise it raises an error\n",
    "    eval_results_dict[parser_name] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter       1.0  0.935185\n",
       "1   token_splitter_512       1.0  0.892857\n",
       "2  token_splitter_1024       1.0  0.875000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1_results = os.path.join(results_folder, \"results_2_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = pd.read_csv(batch_1_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Splitter accuracy was way below others that is why it is not going to be used (cf results_2_docs_Sentence_Semantic_Token.csv)\n",
    "\n",
    "On two documents (7 chunks for SemanticSplitter, 5 for Token512 and 2 for Token1024) : Semantic has the higher score and increased token size for token splitter seems to lower HR and MRR\n",
    "\n",
    "Try to evaluate on more documents/chunks to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.698276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.622363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter  0.801724  0.698276\n",
       "1   token_splitter_512  0.717143  0.607143\n",
       "2  token_splitter_1024  0.721519  0.622363"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2_results = os.path.join(results_folder, \"results_10_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = pd.read_csv(batch_2_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 10 documents (49 chunks for Semantic, 121 for Token512 and 52 for Token1024) : Semantic splitter is still better than TokenSplitter, but the Token1024 is slightly better than the 512\n",
    "\n",
    "Timer : 10.51 for Semantic, 20.40 for Token512, 14.57 for Token1024 ==> Meaning that, with increased database volume the Semantic Splitter is going to be more and more slow than the TokenSplitter BUT the results are way better so it is a trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.684739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>0.766615</td>\n",
       "      <td>0.649923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.685751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter  0.783133  0.684739\n",
       "1   token_splitter_512  0.766615  0.649923\n",
       "2  token_splitter_1024  0.763359  0.685751"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_3_results = os.path.join(results_folder, \"results_23_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = compute_hit_hrr_results(eval_results_dict)\n",
    "df.to_csv(batch_3_results, index=False, sep=\";\")\n",
    "\n",
    "df = pd.read_csv(batch_3_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, on 23 documents (101 chunks for Semantic, 261 for Token512 and 105 for Token 1024) : Semantic splitter is still better than TokenSplitter but TokenSPlitter has increased its HitRate and MRR\n",
    "\n",
    "Timer : 26.36 for Semantic, 46.23 for Token512 and 24.17 for Token1024 ==> Meaning that, with increased database volume the Semantic Splitter is going to be more and more slow than the TokenSplitter and the results tend to decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = token_splitter_512.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jsonpickle\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import VectorStoreIndex, load_index_from_storage, Settings\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, RetrieverEvaluator, FaithfulnessEvaluator, RelevancyEvaluator, AnswerRelevancyEvaluator, BatchEvalRunner, CorrectnessEvaluator\n",
    "import nest_asyncio\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "# To allow nested event loops\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_engine(sentence_index, similarity_top_k=3, rerank_top_n=2):\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n, model=\"BAAI/bge-reranker-base\", device=\"mps\"\n",
    "    )\n",
    "    engine = sentence_index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k, node_postprocessors=[rerank]\n",
    "    )\n",
    "\n",
    "    return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, ( 4468.25 / 10922.67)\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, ( 4789.25 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, ( 5349.27 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 10\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "batch = \"batch_2\"\n",
    "input_folder = f\"./data_evaluation/{batch}/files/\"\n",
    "batch_folder = os.path.join(\"data_evaluation\", batch)\n",
    "results_folder = os.path.join(batch_folder, \"results\")\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "# service_context_llm_base = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup your API KEY here\n",
    "api_key = input(\"Put your API key here\")\n",
    "\n",
    "# Load Mixtral 8x7b model\n",
    "llm_mixtral = MistralAI(api_key=api_key, \n",
    "                        model=\"open-mixtral-8x7b\", \n",
    "                        temperature=0.0,\n",
    "                        max_tokens=1024)\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "semantic_splitter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:40<00:00,  1.22it/s]\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.97 ms /   146 runs   (    0.10 ms per token,  9754.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14047.73 ms /   505 tokens (   27.82 ms per token,    35.95 tokens per second)\n",
      "llama_print_timings:        eval time =   13843.06 ms /   145 runs   (   95.47 ms per token,    10.47 tokens per second)\n",
      "llama_print_timings:       total time =   28089.91 ms /   650 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.65 ms /   340 runs   (    0.09 ms per token, 11094.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22957.39 ms /  1340 tokens (   17.13 ms per token,    58.37 tokens per second)\n",
      "llama_print_timings:        eval time =   26499.09 ms /   339 runs   (   78.17 ms per token,    12.79 tokens per second)\n",
      "llama_print_timings:       total time =   49937.43 ms /  1679 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.91 ms /   234 runs   (    0.10 ms per token,  9786.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5516.69 ms /   374 tokens (   14.75 ms per token,    67.79 tokens per second)\n",
      "llama_print_timings:        eval time =   20040.08 ms /   233 runs   (   86.01 ms per token,    11.63 tokens per second)\n",
      "llama_print_timings:       total time =   25889.64 ms /   607 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    34 runs   (    0.16 ms per token,  6256.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5823.39 ms /   182 tokens (   32.00 ms per token,    31.25 tokens per second)\n",
      "llama_print_timings:        eval time =   10957.98 ms /    33 runs   (  332.06 ms per token,     3.01 tokens per second)\n",
      "llama_print_timings:       total time =   16857.22 ms /   215 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    40 runs   (    0.13 ms per token,  7642.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7781.91 ms /   373 tokens (   20.86 ms per token,    47.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5550.57 ms /    39 runs   (  142.32 ms per token,     7.03 tokens per second)\n",
      "llama_print_timings:       total time =   13398.35 ms /   412 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.29 ms /    85 runs   (    0.10 ms per token, 10247.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2908.86 ms /    47 tokens (   61.89 ms per token,    16.16 tokens per second)\n",
      "llama_print_timings:        eval time =    6948.90 ms /    84 runs   (   82.73 ms per token,    12.09 tokens per second)\n",
      "llama_print_timings:       total time =    9978.49 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    64 runs   (    0.10 ms per token,  9560.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4948.30 ms /   203 tokens (   24.38 ms per token,    41.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5578.10 ms /    63 runs   (   88.54 ms per token,    11.29 tokens per second)\n",
      "llama_print_timings:       total time =   10616.73 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.81 ms /   228 runs   (    0.09 ms per token, 10954.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4901.48 ms /   250 tokens (   19.61 ms per token,    51.01 tokens per second)\n",
      "llama_print_timings:        eval time =   17146.87 ms /   227 runs   (   75.54 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   22369.17 ms /   477 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.35 ms /   144 runs   (    0.09 ms per token, 11660.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12043.82 ms /   798 tokens (   15.09 ms per token,    66.26 tokens per second)\n",
      "llama_print_timings:        eval time =   10969.21 ms /   143 runs   (   76.71 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:       total time =   23211.85 ms /   941 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.76 ms /   101 runs   (    0.10 ms per token, 10349.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5966.18 ms /   386 tokens (   15.46 ms per token,    64.70 tokens per second)\n",
      "llama_print_timings:        eval time =    7335.35 ms /   100 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   13436.25 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.83 ms /   126 runs   (    0.11 ms per token,  9108.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14495.74 ms /   925 tokens (   15.67 ms per token,    63.81 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.03 ms /   125 runs   (   90.35 ms per token,    11.07 tokens per second)\n",
      "llama_print_timings:       total time =   25970.29 ms /  1050 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.47 ms /    94 runs   (    0.10 ms per token,  9920.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12194.96 ms /   720 tokens (   16.94 ms per token,    59.04 tokens per second)\n",
      "llama_print_timings:        eval time =    7145.97 ms /    93 runs   (   76.84 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:       total time =   19480.68 ms /   813 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.41 ms /   323 runs   (    0.11 ms per token,  9386.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19761.44 ms /  1012 tokens (   19.53 ms per token,    51.21 tokens per second)\n",
      "llama_print_timings:        eval time =   44144.34 ms /   322 runs   (  137.09 ms per token,     7.29 tokens per second)\n",
      "llama_print_timings:       total time =   64394.76 ms /  1334 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.54 ms /   101 runs   (    0.10 ms per token,  9578.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5839.69 ms /   386 tokens (   15.13 ms per token,    66.10 tokens per second)\n",
      "llama_print_timings:        eval time =    8205.20 ms /   100 runs   (   82.05 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =   14193.00 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.92 ms /   314 runs   (    0.11 ms per token,  8990.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6283.40 ms /   445 tokens (   14.12 ms per token,    70.82 tokens per second)\n",
      "llama_print_timings:        eval time =   23718.34 ms /   313 runs   (   75.78 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:       total time =   30590.17 ms /   758 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      40.07 ms /   429 runs   (    0.09 ms per token, 10705.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14174.66 ms /  1009 tokens (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_print_timings:        eval time =   35045.25 ms /   428 runs   (   81.88 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   49937.21 ms /  1437 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.43 ms /   146 runs   (    0.10 ms per token, 10119.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4915.75 ms /   278 tokens (   17.68 ms per token,    56.55 tokens per second)\n",
      "llama_print_timings:        eval time =   10700.50 ms /   145 runs   (   73.80 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   15815.38 ms /   423 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.14 ms /   441 runs   (    0.10 ms per token, 10466.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11444.00 ms /   674 tokens (   16.98 ms per token,    58.90 tokens per second)\n",
      "llama_print_timings:        eval time =   43075.74 ms /   440 runs   (   97.90 ms per token,    10.21 tokens per second)\n",
      "llama_print_timings:       total time =   55192.49 ms /  1114 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.58 ms /    66 runs   (    0.10 ms per token, 10025.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2744.33 ms /    37 tokens (   74.17 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6412.20 ms /    65 runs   (   98.65 ms per token,    10.14 tokens per second)\n",
      "llama_print_timings:       total time =    9251.98 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.55 ms /   116 runs   (    0.11 ms per token,  9241.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5659.19 ms /   384 tokens (   14.74 ms per token,    67.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9242.18 ms /   115 runs   (   80.37 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   15066.36 ms /   499 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.51 ms /   241 runs   (    0.09 ms per token, 11205.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20450.41 ms /  1409 tokens (   14.51 ms per token,    68.90 tokens per second)\n",
      "llama_print_timings:        eval time =   17605.17 ms /   240 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   38596.93 ms /  1649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.39 ms /   151 runs   (    0.10 ms per token,  9811.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5973.08 ms /   385 tokens (   15.51 ms per token,    64.46 tokens per second)\n",
      "llama_print_timings:        eval time =   11977.28 ms /   150 runs   (   79.85 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   18162.54 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.80 ms /   135 runs   (    0.11 ms per token,  9120.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15256.85 ms /   988 tokens (   15.44 ms per token,    64.76 tokens per second)\n",
      "llama_print_timings:        eval time =   16684.11 ms /   134 runs   (  124.51 ms per token,     8.03 tokens per second)\n",
      "llama_print_timings:       total time =   32168.66 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      47.90 ms /   504 runs   (    0.10 ms per token, 10521.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4529.14 ms /   213 tokens (   21.26 ms per token,    47.03 tokens per second)\n",
      "llama_print_timings:        eval time =   39086.72 ms /   503 runs   (   77.71 ms per token,    12.87 tokens per second)\n",
      "llama_print_timings:       total time =   44371.20 ms /   716 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.04 ms /   116 runs   (    0.10 ms per token,  9632.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5199.85 ms /   278 tokens (   18.70 ms per token,    53.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9415.15 ms /   115 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   14781.38 ms /   393 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.99 ms /   241 runs   (    0.10 ms per token,  9644.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24475.21 ms /  1409 tokens (   17.37 ms per token,    57.57 tokens per second)\n",
      "llama_print_timings:        eval time =   36059.86 ms /   240 runs   (  150.25 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   60937.31 ms /  1649 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.25 ms /   151 runs   (    0.10 ms per token,  9904.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6160.06 ms /   385 tokens (   16.00 ms per token,    62.50 tokens per second)\n",
      "llama_print_timings:        eval time =   10856.96 ms /   150 runs   (   72.38 ms per token,    13.82 tokens per second)\n",
      "llama_print_timings:       total time =   17220.99 ms /   535 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.21 ms /   135 runs   (    0.09 ms per token, 11052.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15065.70 ms /   988 tokens (   15.25 ms per token,    65.58 tokens per second)\n",
      "llama_print_timings:        eval time =   10578.72 ms /   134 runs   (   78.95 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:       total time =   25836.18 ms /  1122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      46.74 ms /   504 runs   (    0.09 ms per token, 10782.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4122.86 ms /   213 tokens (   19.36 ms per token,    51.66 tokens per second)\n",
      "llama_print_timings:        eval time =   36254.49 ms /   503 runs   (   72.08 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =   41128.69 ms /   716 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.49 ms /   349 runs   (    0.10 ms per token, 10422.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39643.02 ms /  2114 tokens (   18.75 ms per token,    53.33 tokens per second)\n",
      "llama_print_timings:        eval time =   39369.62 ms /   348 runs   (  113.13 ms per token,     8.84 tokens per second)\n",
      "llama_print_timings:       total time =   79571.31 ms /  2462 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.68 ms /   219 runs   (    0.09 ms per token, 10590.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27341.79 ms /  1208 tokens (   22.63 ms per token,    44.18 tokens per second)\n",
      "llama_print_timings:        eval time =   27043.96 ms /   218 runs   (  124.05 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   54738.88 ms /  1426 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.74 ms /   134 runs   (    0.10 ms per token, 10517.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22223.92 ms /  1242 tokens (   17.89 ms per token,    55.89 tokens per second)\n",
      "llama_print_timings:        eval time =   10216.38 ms /   133 runs   (   76.81 ms per token,    13.02 tokens per second)\n",
      "llama_print_timings:       total time =   32638.06 ms /  1375 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.80 ms /   162 runs   (    0.10 ms per token, 10253.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13122.73 ms /   825 tokens (   15.91 ms per token,    62.87 tokens per second)\n",
      "llama_print_timings:        eval time =   15895.93 ms /   161 runs   (   98.73 ms per token,    10.13 tokens per second)\n",
      "llama_print_timings:       total time =   29258.20 ms /   986 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.41 ms /   271 runs   (    0.09 ms per token, 10663.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22687.16 ms /  1200 tokens (   18.91 ms per token,    52.89 tokens per second)\n",
      "llama_print_timings:        eval time =   28940.83 ms /   270 runs   (  107.19 ms per token,     9.33 tokens per second)\n",
      "llama_print_timings:       total time =   52052.78 ms /  1470 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      35.22 ms /   395 runs   (    0.09 ms per token, 11215.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18808.09 ms /  1078 tokens (   17.45 ms per token,    57.32 tokens per second)\n",
      "llama_print_timings:        eval time =   41262.81 ms /   394 runs   (  104.73 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =   60688.52 ms /  1472 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      30.03 ms /   317 runs   (    0.09 ms per token, 10557.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14495.65 ms /   911 tokens (   15.91 ms per token,    62.85 tokens per second)\n",
      "llama_print_timings:        eval time =   26373.21 ms /   316 runs   (   83.46 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   41343.83 ms /  1227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.56 ms /   335 runs   (    0.10 ms per token,  9981.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11789.76 ms /   669 tokens (   17.62 ms per token,    56.74 tokens per second)\n",
      "llama_print_timings:        eval time =   30679.22 ms /   334 runs   (   91.85 ms per token,    10.89 tokens per second)\n",
      "llama_print_timings:       total time =   42990.15 ms /  1003 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.28 ms /   146 runs   (    0.10 ms per token,  9555.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6267.23 ms /   384 tokens (   16.32 ms per token,    61.27 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.32 ms /   145 runs   (   77.89 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =   17771.87 ms /   529 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.77 ms /   119 runs   (    0.10 ms per token, 10111.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23219.22 ms /  1310 tokens (   17.72 ms per token,    56.42 tokens per second)\n",
      "llama_print_timings:        eval time =   10699.97 ms /   118 runs   (   90.68 ms per token,    11.03 tokens per second)\n",
      "llama_print_timings:       total time =   34099.18 ms /  1428 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.76 ms /    81 runs   (    0.10 ms per token, 10435.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19165.32 ms /  1088 tokens (   17.62 ms per token,    56.77 tokens per second)\n",
      "llama_print_timings:        eval time =    8694.89 ms /    80 runs   (  108.69 ms per token,     9.20 tokens per second)\n",
      "llama_print_timings:       total time =   28035.66 ms /  1168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    66 runs   (    0.11 ms per token,  9443.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3120.70 ms /    68 tokens (   45.89 ms per token,    21.79 tokens per second)\n",
      "llama_print_timings:        eval time =    8490.47 ms /    65 runs   (  130.62 ms per token,     7.66 tokens per second)\n",
      "llama_print_timings:       total time =   11713.28 ms /   133 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.38 ms /   116 runs   (    0.10 ms per token, 10195.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6785.09 ms /    79 tokens (   85.89 ms per token,    11.64 tokens per second)\n",
      "llama_print_timings:        eval time =   14473.22 ms /   115 runs   (  125.85 ms per token,     7.95 tokens per second)\n",
      "llama_print_timings:       total time =   21431.28 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.84 ms /   101 runs   (    0.10 ms per token, 10260.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3234.03 ms /    68 tokens (   47.56 ms per token,    21.03 tokens per second)\n",
      "llama_print_timings:        eval time =  593595.01 ms /   100 runs   ( 5935.95 ms per token,     0.17 tokens per second)\n",
      "llama_print_timings:       total time =  597000.17 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.55 ms /   175 runs   (    0.09 ms per token, 10577.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11550.12 ms /   726 tokens (   15.91 ms per token,    62.86 tokens per second)\n",
      "llama_print_timings:        eval time =   13797.06 ms /   174 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =   25605.43 ms /   900 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.63 ms /   101 runs   (    0.11 ms per token,  9501.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5721.26 ms /   386 tokens (   14.82 ms per token,    67.47 tokens per second)\n",
      "llama_print_timings:        eval time =    7424.12 ms /   100 runs   (   74.24 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   13291.15 ms /   486 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.09 ms /   279 runs   (    0.09 ms per token, 10691.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52152.84 ms /  2914 tokens (   17.90 ms per token,    55.87 tokens per second)\n",
      "llama_print_timings:        eval time =   35452.09 ms /   278 runs   (  127.53 ms per token,     7.84 tokens per second)\n",
      "llama_print_timings:       total time =   88068.75 ms /  3192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      37.17 ms /   414 runs   (    0.09 ms per token, 11137.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25651.54 ms /  1544 tokens (   16.61 ms per token,    60.19 tokens per second)\n",
      "llama_print_timings:        eval time =   33905.22 ms /   413 runs   (   82.09 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =   60229.41 ms /  1957 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.06 ms /   412 runs   (    0.09 ms per token, 10548.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23854.21 ms /  1583 tokens (   15.07 ms per token,    66.36 tokens per second)\n",
      "llama_print_timings:        eval time =   51021.88 ms /   411 runs   (  124.14 ms per token,     8.06 tokens per second)\n",
      "llama_print_timings:       total time =   75554.30 ms /  1994 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.70 ms /    35 runs   (    0.11 ms per token,  9469.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14121.80 ms /   702 tokens (   20.12 ms per token,    49.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3479.20 ms /    34 runs   (  102.33 ms per token,     9.77 tokens per second)\n",
      "llama_print_timings:       total time =   17662.80 ms /   736 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.50 ms /    53 runs   (    0.12 ms per token,  8150.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10474.47 ms /   456 tokens (   22.97 ms per token,    43.53 tokens per second)\n",
      "llama_print_timings:        eval time =   40910.65 ms /    52 runs   (  786.74 ms per token,     1.27 tokens per second)\n",
      "llama_print_timings:       total time =   51490.49 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.43 ms /    35 runs   (    0.10 ms per token, 10195.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3087.07 ms /    29 tokens (  106.45 ms per token,     9.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4880.75 ms /    34 runs   (  143.55 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =    8032.21 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.26 ms /    69 runs   (    0.12 ms per token,  8353.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5169.09 ms /    97 tokens (   53.29 ms per token,    18.77 tokens per second)\n",
      "llama_print_timings:        eval time =   10608.91 ms /    68 runs   (  156.01 ms per token,     6.41 tokens per second)\n",
      "llama_print_timings:       total time =   15917.72 ms /   165 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.93 ms /   376 runs   (    0.10 ms per token,  9659.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17453.05 ms /   710 tokens (   24.58 ms per token,    40.68 tokens per second)\n",
      "llama_print_timings:        eval time =   53811.94 ms /   375 runs   (  143.50 ms per token,     6.97 tokens per second)\n",
      "llama_print_timings:       total time =   72110.95 ms /  1085 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.33 ms /   331 runs   (    0.10 ms per token, 10237.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33457.28 ms /  1740 tokens (   19.23 ms per token,    52.01 tokens per second)\n",
      "llama_print_timings:        eval time =   39476.11 ms /   330 runs   (  119.62 ms per token,     8.36 tokens per second)\n",
      "llama_print_timings:       total time =   73625.52 ms /  2070 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       2.60 ms /    29 runs   (    0.09 ms per token, 11140.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9985.17 ms /   634 tokens (   15.75 ms per token,    63.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1935.67 ms /    28 runs   (   69.13 ms per token,    14.47 tokens per second)\n",
      "llama_print_timings:       total time =   11998.21 ms /   662 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       2.06 ms /    20 runs   (    0.10 ms per token,  9699.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4420.61 ms /   203 tokens (   21.78 ms per token,    45.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1543.65 ms /    19 runs   (   81.24 ms per token,    12.31 tokens per second)\n",
      "llama_print_timings:       total time =    6015.73 ms /   222 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.79 ms /   146 runs   (    0.10 ms per token,  9870.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4506.74 ms /   278 tokens (   16.21 ms per token,    61.69 tokens per second)\n",
      "llama_print_timings:        eval time =   11723.48 ms /   145 runs   (   80.85 ms per token,    12.37 tokens per second)\n",
      "llama_print_timings:       total time =   16572.87 ms /   423 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      47.22 ms /   512 runs   (    0.09 ms per token, 10841.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18094.20 ms /  1228 tokens (   14.73 ms per token,    67.87 tokens per second)\n",
      "llama_print_timings:        eval time =   45509.67 ms /   511 runs   (   89.06 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   64769.63 ms /  1739 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.68 ms /   248 runs   (    0.10 ms per token, 10471.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27445.70 ms /  1482 tokens (   18.52 ms per token,    54.00 tokens per second)\n",
      "llama_print_timings:        eval time =   32116.52 ms /   247 runs   (  130.03 ms per token,     7.69 tokens per second)\n",
      "llama_print_timings:       total time =   59961.75 ms /  1729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.88 ms /   440 runs   (    0.09 ms per token, 11031.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19151.06 ms /  1367 tokens (   14.01 ms per token,    71.38 tokens per second)\n",
      "llama_print_timings:        eval time =   33409.01 ms /   439 runs   (   76.10 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:       total time =   53231.34 ms /  1806 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.38 ms /   334 runs   (    0.09 ms per token, 10644.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10019.55 ms /   576 tokens (   17.40 ms per token,    57.49 tokens per second)\n",
      "llama_print_timings:        eval time =   24047.53 ms /   333 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =   34684.69 ms /   909 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.93 ms /   130 runs   (    0.08 ms per token, 11892.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9275.15 ms /   630 tokens (   14.72 ms per token,    67.92 tokens per second)\n",
      "llama_print_timings:        eval time =    9968.51 ms /   129 runs   (   77.28 ms per token,    12.94 tokens per second)\n",
      "llama_print_timings:       total time =   19435.32 ms /   759 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.12 ms /   213 runs   (    0.09 ms per token, 11139.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18523.35 ms /  1328 tokens (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_print_timings:        eval time =   17702.53 ms /   212 runs   (   83.50 ms per token,    11.98 tokens per second)\n",
      "llama_print_timings:       total time =   36555.41 ms /  1540 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.84 ms /   219 runs   (    0.10 ms per token, 10506.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19610.76 ms /  1370 tokens (   14.31 ms per token,    69.86 tokens per second)\n",
      "llama_print_timings:        eval time =   16668.17 ms /   218 runs   (   76.46 ms per token,    13.08 tokens per second)\n",
      "llama_print_timings:       total time =   36601.46 ms /  1588 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.31 ms /   176 runs   (    0.09 ms per token, 11495.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14592.90 ms /  1035 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_print_timings:        eval time =   12427.72 ms /   175 runs   (   71.02 ms per token,    14.08 tokens per second)\n",
      "llama_print_timings:       total time =   27263.95 ms /  1210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.55 ms /   179 runs   (    0.09 ms per token, 11513.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   41556.85 ms /  2961 tokens (   14.03 ms per token,    71.25 tokens per second)\n",
      "llama_print_timings:        eval time =   14130.35 ms /   178 runs   (   79.38 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   56133.36 ms /  3139 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.98 ms /   182 runs   (    0.09 ms per token, 10715.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12444.76 ms /   939 tokens (   13.25 ms per token,    75.45 tokens per second)\n",
      "llama_print_timings:        eval time =   12757.23 ms /   181 runs   (   70.48 ms per token,    14.19 tokens per second)\n",
      "llama_print_timings:       total time =   25456.07 ms /  1120 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.10 ms /   234 runs   (    0.10 ms per token, 10131.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5057.93 ms /   384 tokens (   13.17 ms per token,    75.92 tokens per second)\n",
      "llama_print_timings:        eval time =   15862.23 ms /   233 runs   (   68.08 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =   21340.17 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    73 runs   (    0.10 ms per token, 10446.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38053.07 ms /  2598 tokens (   14.65 ms per token,    68.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5600.01 ms /    72 runs   (   77.78 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   43758.60 ms /  2670 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.79 ms /   234 runs   (    0.10 ms per token, 10267.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5075.33 ms /   384 tokens (   13.22 ms per token,    75.66 tokens per second)\n",
      "llama_print_timings:        eval time =   15849.54 ms /   233 runs   (   68.02 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =   21248.82 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   149 runs   (    0.09 ms per token, 11049.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   30518.17 ms /  2208 tokens (   13.82 ms per token,    72.35 tokens per second)\n",
      "llama_print_timings:        eval time =   11481.92 ms /   148 runs   (   77.58 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:       total time =   42413.93 ms /  2356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.04 ms /   141 runs   (    0.09 ms per token, 10817.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5368.09 ms /   376 tokens (   14.28 ms per token,    70.04 tokens per second)\n",
      "llama_print_timings:        eval time =    9563.29 ms /   140 runs   (   68.31 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   15123.16 ms /   516 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      39.78 ms /   424 runs   (    0.09 ms per token, 10657.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17017.74 ms /  1241 tokens (   13.71 ms per token,    72.92 tokens per second)\n",
      "llama_print_timings:        eval time =   30623.61 ms /   423 runs   (   72.40 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:       total time =   48268.33 ms /  1664 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.66 ms /    62 runs   (    0.09 ms per token, 10957.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1750.03 ms /    25 tokens (   70.00 ms per token,    14.29 tokens per second)\n",
      "llama_print_timings:        eval time =    4417.07 ms /    61 runs   (   72.41 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:       total time =    6250.94 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.32 ms /   105 runs   (    0.09 ms per token, 11268.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3526.85 ms /   203 tokens (   17.37 ms per token,    57.56 tokens per second)\n",
      "llama_print_timings:        eval time =    7323.60 ms /   104 runs   (   70.42 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:       total time =   10995.12 ms /   307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.33 ms /   368 runs   (    0.09 ms per token, 11381.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31141.25 ms /  2152 tokens (   14.47 ms per token,    69.10 tokens per second)\n",
      "llama_print_timings:        eval time =   28133.31 ms /   367 runs   (   76.66 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:       total time =   59946.90 ms /  2519 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.02 ms /    91 runs   (    0.09 ms per token, 11348.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9346.53 ms /   625 tokens (   14.95 ms per token,    66.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6225.33 ms /    90 runs   (   69.17 ms per token,    14.46 tokens per second)\n",
      "llama_print_timings:       total time =   15696.79 ms /   715 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      22.47 ms /   268 runs   (    0.08 ms per token, 11929.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   38868.73 ms /  2753 tokens (   14.12 ms per token,    70.83 tokens per second)\n",
      "llama_print_timings:        eval time =   21053.89 ms /   267 runs   (   78.85 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =   60438.22 ms /  3020 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.45 ms /   512 runs   (    0.09 ms per token, 11265.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16063.33 ms /  1079 tokens (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:        eval time =   37008.38 ms /   511 runs   (   72.42 ms per token,    13.81 tokens per second)\n",
      "llama_print_timings:       total time =   53832.08 ms /  1590 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.82 ms /   175 runs   (    0.08 ms per token, 11806.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39345.88 ms /  2747 tokens (   14.32 ms per token,    69.82 tokens per second)\n",
      "llama_print_timings:        eval time =   13685.77 ms /   174 runs   (   78.65 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =   53285.73 ms /  2921 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.62 ms /   234 runs   (    0.10 ms per token,  9906.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5377.69 ms /   384 tokens (   14.00 ms per token,    71.41 tokens per second)\n",
      "llama_print_timings:        eval time =   15973.03 ms /   233 runs   (   68.55 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =   21681.55 ms /   617 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.66 ms /   284 runs   (    0.09 ms per token, 11516.63 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1083130.80 ms /  2531 tokens (  427.95 ms per token,     2.34 tokens per second)\n",
      "llama_print_timings:        eval time =   22418.41 ms /   283 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time = 1105965.35 ms /  2814 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      32.30 ms /   351 runs   (    0.09 ms per token, 10867.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4747.13 ms /   318 tokens (   14.93 ms per token,    66.99 tokens per second)\n",
      "llama_print_timings:        eval time =  247305.14 ms /   350 runs   (  706.59 ms per token,     1.42 tokens per second)\n",
      "llama_print_timings:       total time =  252591.29 ms /   668 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       9.89 ms /   106 runs   (    0.09 ms per token, 10721.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11223.92 ms /   714 tokens (   15.72 ms per token,    63.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7248.59 ms /   105 runs   (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   18619.29 ms /   819 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      34.83 ms /   241 runs   (    0.14 ms per token,  6920.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11254.22 ms /   646 tokens (   17.42 ms per token,    57.40 tokens per second)\n",
      "llama_print_timings:        eval time =   23776.91 ms /   240 runs   (   99.07 ms per token,    10.09 tokens per second)\n",
      "llama_print_timings:       total time =   35536.86 ms /   886 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.46 ms /   114 runs   (    0.09 ms per token, 10901.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12698.28 ms /   725 tokens (   17.51 ms per token,    57.09 tokens per second)\n",
      "llama_print_timings:        eval time =    7918.89 ms /   113 runs   (   70.08 ms per token,    14.27 tokens per second)\n",
      "llama_print_timings:       total time =   20772.43 ms /   838 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    56 runs   (    0.10 ms per token,  9569.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5541.90 ms /   424 tokens (   13.07 ms per token,    76.51 tokens per second)\n",
      "llama_print_timings:        eval time =  356274.88 ms /    55 runs   ( 6477.73 ms per token,     0.15 tokens per second)\n",
      "llama_print_timings:       total time =  361917.92 ms /   479 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.96 ms /   470 runs   (    0.10 ms per token, 10225.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10912.18 ms /   517 tokens (   21.11 ms per token,    47.38 tokens per second)\n",
      "llama_print_timings:        eval time =   54903.32 ms /   469 runs   (  117.06 ms per token,     8.54 tokens per second)\n",
      "llama_print_timings:       total time =   66605.51 ms /   986 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.91 ms /   124 runs   (    0.12 ms per token,  8316.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20333.42 ms /   872 tokens (   23.32 ms per token,    42.89 tokens per second)\n",
      "llama_print_timings:        eval time =   41196.53 ms /   123 runs   (  334.93 ms per token,     2.99 tokens per second)\n",
      "llama_print_timings:       total time =   61770.20 ms /   995 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      29.27 ms /   259 runs   (    0.11 ms per token,  8848.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15456.51 ms /   799 tokens (   19.34 ms per token,    51.69 tokens per second)\n",
      "llama_print_timings:        eval time =   68261.00 ms /   258 runs   (  264.58 ms per token,     3.78 tokens per second)\n",
      "llama_print_timings:       total time =   84179.07 ms /  1057 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.76 ms /    67 runs   (    0.10 ms per token,  9905.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5062.74 ms /   314 tokens (   16.12 ms per token,    62.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5145.88 ms /    66 runs   (   77.97 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   10313.39 ms /   380 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.01 ms /   254 runs   (    0.09 ms per token, 11040.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14165.59 ms /   937 tokens (   15.12 ms per token,    66.15 tokens per second)\n",
      "llama_print_timings:        eval time =   17990.52 ms /   253 runs   (   71.11 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =   32529.12 ms /  1190 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "token_splitter_512\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [01:28<00:00,  1.38it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.33 ms /   273 runs   (    0.11 ms per token,  8713.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   39878.79 ms /   780 tokens (   51.13 ms per token,    19.56 tokens per second)\n",
      "llama_print_timings:        eval time =   51463.24 ms /   272 runs   (  189.20 ms per token,     5.29 tokens per second)\n",
      "llama_print_timings:       total time =   92134.24 ms /  1052 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.64 ms /   439 runs   (    0.10 ms per token, 10296.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12590.05 ms /   498 tokens (   25.28 ms per token,    39.56 tokens per second)\n",
      "llama_print_timings:        eval time =  105837.68 ms /   438 runs   (  241.64 ms per token,     4.14 tokens per second)\n",
      "llama_print_timings:       total time =  119491.41 ms /   936 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    38 runs   (    0.11 ms per token,  9019.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10783.66 ms /   378 tokens (   28.53 ms per token,    35.05 tokens per second)\n",
      "llama_print_timings:        eval time =    9888.74 ms /    37 runs   (  267.26 ms per token,     3.74 tokens per second)\n",
      "llama_print_timings:       total time =   20773.24 ms /   415 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    65 runs   (    0.11 ms per token,  9188.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8504.89 ms /   501 tokens (   16.98 ms per token,    58.91 tokens per second)\n",
      "llama_print_timings:        eval time =   15122.42 ms /    64 runs   (  236.29 ms per token,     4.23 tokens per second)\n",
      "llama_print_timings:       total time =   23781.70 ms /   565 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      31.48 ms /   336 runs   (    0.09 ms per token, 10673.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16918.70 ms /   868 tokens (   19.49 ms per token,    51.30 tokens per second)\n",
      "llama_print_timings:        eval time =   54163.59 ms /   335 runs   (  161.68 ms per token,     6.18 tokens per second)\n",
      "llama_print_timings:       total time =   71941.11 ms /  1203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.93 ms /   151 runs   (    0.10 ms per token, 10117.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17076.76 ms /   843 tokens (   20.26 ms per token,    49.37 tokens per second)\n",
      "llama_print_timings:        eval time =   29804.28 ms /   150 runs   (  198.70 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   47184.26 ms /   993 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.09 ms /   161 runs   (    0.10 ms per token, 10003.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22843.83 ms /   729 tokens (   31.34 ms per token,    31.91 tokens per second)\n",
      "llama_print_timings:        eval time =   24236.57 ms /   160 runs   (  151.48 ms per token,     6.60 tokens per second)\n",
      "llama_print_timings:       total time =   47466.87 ms /   889 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.61 ms /   135 runs   (    0.11 ms per token,  9240.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9590.85 ms /   373 tokens (   25.71 ms per token,    38.89 tokens per second)\n",
      "llama_print_timings:        eval time =   17048.72 ms /   134 runs   (  127.23 ms per token,     7.86 tokens per second)\n",
      "llama_print_timings:       total time =   26914.97 ms /   507 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.00 ms /   220 runs   (    0.12 ms per token,  8149.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16312.24 ms /   704 tokens (   23.17 ms per token,    43.16 tokens per second)\n",
      "llama_print_timings:        eval time =   58216.55 ms /   219 runs   (  265.83 ms per token,     3.76 tokens per second)\n",
      "llama_print_timings:       total time =   75008.29 ms /   923 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      19.12 ms /   190 runs   (    0.10 ms per token,  9937.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6990.33 ms /   373 tokens (   18.74 ms per token,    53.36 tokens per second)\n",
      "llama_print_timings:        eval time =   21884.56 ms /   189 runs   (  115.79 ms per token,     8.64 tokens per second)\n",
      "llama_print_timings:       total time =   29182.20 ms /   562 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      53.13 ms /   512 runs   (    0.10 ms per token,  9636.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24421.80 ms /  1209 tokens (   20.20 ms per token,    49.50 tokens per second)\n",
      "llama_print_timings:        eval time =   91491.80 ms /   511 runs   (  179.04 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =  117134.44 ms /  1720 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.11 ms /   135 runs   (    0.13 ms per token,  7891.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12920.66 ms /   559 tokens (   23.11 ms per token,    43.26 tokens per second)\n",
      "llama_print_timings:        eval time =   22726.80 ms /   134 runs   (  169.60 ms per token,     5.90 tokens per second)\n",
      "llama_print_timings:       total time =   35985.09 ms /   693 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.68 ms /   452 runs   (    0.10 ms per token,  9895.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18421.96 ms /   591 tokens (   31.17 ms per token,    32.08 tokens per second)\n",
      "llama_print_timings:        eval time =   64053.63 ms /   451 runs   (  142.03 ms per token,     7.04 tokens per second)\n",
      "llama_print_timings:       total time =   83326.03 ms /  1042 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.93 ms /   135 runs   (    0.10 ms per token,  9691.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6837.99 ms /   373 tokens (   18.33 ms per token,    54.55 tokens per second)\n",
      "llama_print_timings:        eval time =   16244.77 ms /   134 runs   (  121.23 ms per token,     8.25 tokens per second)\n",
      "llama_print_timings:       total time =   23317.90 ms /   507 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      26.70 ms /   235 runs   (    0.11 ms per token,  8802.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16063.16 ms /   972 tokens (   16.53 ms per token,    60.51 tokens per second)\n",
      "llama_print_timings:        eval time =   45101.01 ms /   234 runs   (  192.74 ms per token,     5.19 tokens per second)\n",
      "llama_print_timings:       total time =   61595.23 ms /  1206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.07 ms /   135 runs   (    0.10 ms per token,  9592.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12737.32 ms /   550 tokens (   23.16 ms per token,    43.18 tokens per second)\n",
      "llama_print_timings:        eval time =   16942.62 ms /   134 runs   (  126.44 ms per token,     7.91 tokens per second)\n",
      "llama_print_timings:       total time =   29911.66 ms /   684 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      46.27 ms /   452 runs   (    0.10 ms per token,  9767.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14063.36 ms /   591 tokens (   23.80 ms per token,    42.02 tokens per second)\n",
      "llama_print_timings:        eval time =   59566.33 ms /   451 runs   (  132.08 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:       total time =   74449.26 ms /  1042 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.46 ms /   135 runs   (    0.11 ms per token,  9338.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7842.96 ms /   373 tokens (   21.03 ms per token,    47.56 tokens per second)\n",
      "llama_print_timings:        eval time =   16525.51 ms /   134 runs   (  123.32 ms per token,     8.11 tokens per second)\n",
      "llama_print_timings:       total time =   24590.36 ms /   507 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.91 ms /   235 runs   (    0.10 ms per token,  9829.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16654.75 ms /   972 tokens (   17.13 ms per token,    58.36 tokens per second)\n",
      "llama_print_timings:        eval time =   28629.54 ms /   234 runs   (  122.35 ms per token,     8.17 tokens per second)\n",
      "llama_print_timings:       total time =   45682.47 ms /  1206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      35.26 ms /   359 runs   (    0.10 ms per token, 10182.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =   22582.65 ms /  1209 tokens (   18.68 ms per token,    53.54 tokens per second)\n",
      "llama_print_timings:        eval time =   52240.97 ms /   358 runs   (  145.92 ms per token,     6.85 tokens per second)\n",
      "llama_print_timings:       total time =   75484.38 ms /  1567 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    47 runs   (    0.12 ms per token,  8462.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16111.99 ms /   722 tokens (   22.32 ms per token,    44.81 tokens per second)\n",
      "llama_print_timings:        eval time =    6079.49 ms /    46 runs   (  132.16 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:       total time =   22277.42 ms /   768 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      25.97 ms /   250 runs   (    0.10 ms per token,  9625.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7966.88 ms /   392 tokens (   20.32 ms per token,    49.20 tokens per second)\n",
      "llama_print_timings:        eval time =   33248.35 ms /   249 runs   (  133.53 ms per token,     7.49 tokens per second)\n",
      "llama_print_timings:       total time =   41651.81 ms /   641 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.11 ms /   181 runs   (    0.11 ms per token,  9002.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11573.26 ms /   487 tokens (   23.76 ms per token,    42.08 tokens per second)\n",
      "llama_print_timings:        eval time =   29783.10 ms /   180 runs   (  165.46 ms per token,     6.04 tokens per second)\n",
      "llama_print_timings:       total time =   41693.32 ms /   667 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.04 ms /   121 runs   (    0.11 ms per token,  9281.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8923.90 ms /   373 tokens (   23.92 ms per token,    41.80 tokens per second)\n",
      "llama_print_timings:        eval time =   14854.48 ms /   120 runs   (  123.79 ms per token,     8.08 tokens per second)\n",
      "llama_print_timings:       total time =   23986.77 ms /   493 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.64 ms /   136 runs   (    0.11 ms per token,  9288.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6318.29 ms /   202 tokens (   31.28 ms per token,    31.97 tokens per second)\n",
      "llama_print_timings:        eval time =   17255.18 ms /   135 runs   (  127.82 ms per token,     7.82 tokens per second)\n",
      "llama_print_timings:       total time =   23808.78 ms /   337 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.22 ms /   229 runs   (    0.12 ms per token,  8412.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.08 ms /   372 tokens (   23.33 ms per token,    42.87 tokens per second)\n",
      "llama_print_timings:        eval time =   55345.24 ms /   228 runs   (  242.74 ms per token,     4.12 tokens per second)\n",
      "llama_print_timings:       total time =   64495.29 ms /   600 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.63 ms /   102 runs   (    0.15 ms per token,  6526.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8083.56 ms /   377 tokens (   21.44 ms per token,    46.64 tokens per second)\n",
      "llama_print_timings:        eval time =   53035.51 ms /   101 runs   (  525.10 ms per token,     1.90 tokens per second)\n",
      "llama_print_timings:       total time =   61344.76 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    47 runs   (    0.16 ms per token,  6267.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8009.94 ms /   370 tokens (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:        eval time =   37378.27 ms /    46 runs   (  812.57 ms per token,     1.23 tokens per second)\n",
      "llama_print_timings:       total time =   45497.53 ms /   416 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.23 ms /   373 runs   (    0.11 ms per token,  8832.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10997.44 ms /   357 tokens (   30.81 ms per token,    32.46 tokens per second)\n",
      "llama_print_timings:        eval time =   66381.70 ms /   372 runs   (  178.45 ms per token,     5.60 tokens per second)\n",
      "llama_print_timings:       total time =   78202.87 ms /   729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    58 runs   (    0.11 ms per token,  8906.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8816.48 ms /   387 tokens (   22.78 ms per token,    43.90 tokens per second)\n",
      "llama_print_timings:        eval time =    7228.26 ms /    57 runs   (  126.81 ms per token,     7.89 tokens per second)\n",
      "llama_print_timings:       total time =   16143.62 ms /   444 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.54 ms /    55 runs   (    0.16 ms per token,  6443.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20492.52 ms /   710 tokens (   28.86 ms per token,    34.65 tokens per second)\n",
      "llama_print_timings:        eval time =   13292.24 ms /    54 runs   (  246.15 ms per token,     4.06 tokens per second)\n",
      "llama_print_timings:       total time =   33928.09 ms /   764 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      27.00 ms /   189 runs   (    0.14 ms per token,  6998.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10450.82 ms /   364 tokens (   28.71 ms per token,    34.83 tokens per second)\n",
      "llama_print_timings:        eval time =   48035.20 ms /   188 runs   (  255.51 ms per token,     3.91 tokens per second)\n",
      "llama_print_timings:       total time =   58960.70 ms /   552 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.73 ms /   144 runs   (    0.10 ms per token,  9775.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8944.22 ms /   398 tokens (   22.47 ms per token,    44.50 tokens per second)\n",
      "llama_print_timings:        eval time =   16563.60 ms /   143 runs   (  115.83 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:       total time =   25745.20 ms /   541 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    33 runs   (    0.12 ms per token,  8196.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8430.96 ms /   359 tokens (   23.48 ms per token,    42.58 tokens per second)\n",
      "llama_print_timings:        eval time =    6509.16 ms /    32 runs   (  203.41 ms per token,     4.92 tokens per second)\n",
      "llama_print_timings:       total time =   15010.42 ms /   391 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      42.82 ms /   382 runs   (    0.11 ms per token,  8921.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8105.71 ms /   386 tokens (   21.00 ms per token,    47.62 tokens per second)\n",
      "llama_print_timings:        eval time =   75078.69 ms /   381 runs   (  197.06 ms per token,     5.07 tokens per second)\n",
      "llama_print_timings:       total time =   83936.27 ms /   767 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      45.32 ms /   373 runs   (    0.12 ms per token,  8229.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8597.90 ms /   357 tokens (   24.08 ms per token,    41.52 tokens per second)\n",
      "llama_print_timings:        eval time =   71014.43 ms /   372 runs   (  190.90 ms per token,     5.24 tokens per second)\n",
      "llama_print_timings:       total time =   80366.93 ms /   729 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    34 runs   (    0.15 ms per token,  6838.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10290.93 ms /   392 tokens (   26.25 ms per token,    38.09 tokens per second)\n",
      "llama_print_timings:        eval time =    7131.05 ms /    33 runs   (  216.09 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =   17491.80 ms /   425 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.30 ms /   118 runs   (    0.11 ms per token,  8873.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8228.72 ms /   361 tokens (   22.79 ms per token,    43.87 tokens per second)\n",
      "llama_print_timings:        eval time =   16421.51 ms /   117 runs   (  140.35 ms per token,     7.12 tokens per second)\n",
      "llama_print_timings:       total time =   24859.45 ms /   478 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.81 ms /    99 runs   (    0.12 ms per token,  8380.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7420.02 ms /   373 tokens (   19.89 ms per token,    50.27 tokens per second)\n",
      "llama_print_timings:        eval time =   13962.76 ms /    98 runs   (  142.48 ms per token,     7.02 tokens per second)\n",
      "llama_print_timings:       total time =   21553.14 ms /   471 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.71 ms /   300 runs   (    0.13 ms per token,  7750.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12069.05 ms /   368 tokens (   32.80 ms per token,    30.49 tokens per second)\n",
      "llama_print_timings:        eval time =   57724.16 ms /   299 runs   (  193.06 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time =   70421.66 ms /   667 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       3.99 ms /    34 runs   (    0.12 ms per token,  8527.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7882.68 ms /   384 tokens (   20.53 ms per token,    48.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3756.66 ms /    33 runs   (  113.84 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:       total time =   11700.43 ms /   417 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    44 runs   (    0.12 ms per token,  8479.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7757.48 ms /   367 tokens (   21.14 ms per token,    47.31 tokens per second)\n",
      "llama_print_timings:        eval time =    6508.62 ms /    43 runs   (  151.36 ms per token,     6.61 tokens per second)\n",
      "llama_print_timings:       total time =   14353.13 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.84 ms /    90 runs   (    0.13 ms per token,  7601.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9140.79 ms /   384 tokens (   23.80 ms per token,    42.01 tokens per second)\n",
      "llama_print_timings:        eval time =   21173.45 ms /    89 runs   (  237.90 ms per token,     4.20 tokens per second)\n",
      "llama_print_timings:       total time =   30501.03 ms /   473 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    54 runs   (    0.11 ms per token,  8730.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7998.94 ms /   350 tokens (   22.85 ms per token,    43.76 tokens per second)\n",
      "llama_print_timings:        eval time =    7956.21 ms /    53 runs   (  150.12 ms per token,     6.66 tokens per second)\n",
      "llama_print_timings:       total time =   16053.49 ms /   403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      14.13 ms /   102 runs   (    0.14 ms per token,  7218.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7784.49 ms /   177 tokens (   43.98 ms per token,    22.74 tokens per second)\n",
      "llama_print_timings:        eval time =   24170.20 ms /   101 runs   (  239.31 ms per token,     4.18 tokens per second)\n",
      "llama_print_timings:       total time =   32178.31 ms /   278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      20.23 ms /   175 runs   (    0.12 ms per token,  8651.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6272.16 ms /   155 tokens (   40.47 ms per token,    24.71 tokens per second)\n",
      "llama_print_timings:        eval time =   27784.08 ms /   174 runs   (  159.68 ms per token,     6.26 tokens per second)\n",
      "llama_print_timings:       total time =   34400.08 ms /   329 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      36.80 ms /   311 runs   (    0.12 ms per token,  8450.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.79 ms /   387 tokens (   22.43 ms per token,    44.59 tokens per second)\n",
      "llama_print_timings:        eval time =   50118.38 ms /   310 runs   (  161.67 ms per token,     6.19 tokens per second)\n",
      "llama_print_timings:       total time =   59386.61 ms /   697 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.62 ms /    44 runs   (    0.13 ms per token,  7827.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5889.09 ms /   173 tokens (   34.04 ms per token,    29.38 tokens per second)\n",
      "llama_print_timings:        eval time =    7584.28 ms /    43 runs   (  176.38 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   13551.33 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      12.03 ms /   102 runs   (    0.12 ms per token,  8478.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8611.01 ms /   356 tokens (   24.19 ms per token,    41.34 tokens per second)\n",
      "llama_print_timings:        eval time =   13597.93 ms /   101 runs   (  134.63 ms per token,     7.43 tokens per second)\n",
      "llama_print_timings:       total time =   22375.53 ms /   457 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.26 ms /   120 runs   (    0.13 ms per token,  7864.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8514.85 ms /   361 tokens (   23.59 ms per token,    42.40 tokens per second)\n",
      "llama_print_timings:        eval time =   19744.68 ms /   119 runs   (  165.92 ms per token,     6.03 tokens per second)\n",
      "llama_print_timings:       total time =   28487.65 ms /   480 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.45 ms /    46 runs   (    0.16 ms per token,  6175.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7749.68 ms /   344 tokens (   22.53 ms per token,    44.39 tokens per second)\n",
      "llama_print_timings:        eval time =   10512.04 ms /    45 runs   (  233.60 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:       total time =   18353.02 ms /   389 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /    60 runs   (    0.13 ms per token,  7476.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6260.36 ms /   180 tokens (   34.78 ms per token,    28.75 tokens per second)\n",
      "llama_print_timings:        eval time =   10216.12 ms /    59 runs   (  173.15 ms per token,     5.78 tokens per second)\n",
      "llama_print_timings:       total time =   16588.29 ms /   239 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      23.31 ms /   181 runs   (    0.13 ms per token,  7764.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8809.44 ms /   348 tokens (   25.31 ms per token,    39.50 tokens per second)\n",
      "llama_print_timings:        eval time =   32426.82 ms /   180 runs   (  180.15 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   41582.69 ms /   528 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    44 runs   (    0.13 ms per token,  7844.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7769.01 ms /   366 tokens (   21.23 ms per token,    47.11 tokens per second)\n",
      "llama_print_timings:        eval time =    9229.60 ms /    43 runs   (  214.64 ms per token,     4.66 tokens per second)\n",
      "llama_print_timings:       total time =   17085.55 ms /   409 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    57 runs   (    0.10 ms per token,  9868.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8832.49 ms /   413 tokens (   21.39 ms per token,    46.76 tokens per second)\n",
      "llama_print_timings:        eval time =    8128.91 ms /    56 runs   (  145.16 ms per token,     6.89 tokens per second)\n",
      "llama_print_timings:       total time =   17060.90 ms /   469 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      38.37 ms /   347 runs   (    0.11 ms per token,  9043.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6654.05 ms /   193 tokens (   34.48 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:        eval time =   55663.05 ms /   346 runs   (  160.88 ms per token,     6.22 tokens per second)\n",
      "llama_print_timings:       total time =   62997.42 ms /   539 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      17.26 ms /   147 runs   (    0.12 ms per token,  8515.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9045.80 ms /   360 tokens (   25.13 ms per token,    39.80 tokens per second)\n",
      "llama_print_timings:        eval time =   21861.41 ms /   146 runs   (  149.74 ms per token,     6.68 tokens per second)\n",
      "llama_print_timings:       total time =   31168.79 ms /   506 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.56 ms /   138 runs   (    0.12 ms per token,  8333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8594.91 ms /   397 tokens (   21.65 ms per token,    46.19 tokens per second)\n",
      "llama_print_timings:        eval time =   23384.92 ms /   137 runs   (  170.69 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   32225.76 ms /   534 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      21.18 ms /   177 runs   (    0.12 ms per token,  8355.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7888.22 ms /   371 tokens (   21.26 ms per token,    47.03 tokens per second)\n",
      "llama_print_timings:        eval time =   27203.53 ms /   176 runs   (  154.57 ms per token,     6.47 tokens per second)\n",
      "llama_print_timings:       total time =   35407.88 ms /   547 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       7.57 ms /    52 runs   (    0.15 ms per token,  6872.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8285.77 ms /   367 tokens (   22.58 ms per token,    44.29 tokens per second)\n",
      "llama_print_timings:        eval time =    9955.26 ms /    51 runs   (  195.20 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:       total time =   18338.64 ms /   418 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      24.10 ms /   199 runs   (    0.12 ms per token,  8257.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11327.78 ms /   420 tokens (   26.97 ms per token,    37.08 tokens per second)\n",
      "llama_print_timings:        eval time =   33813.96 ms /   198 runs   (  170.78 ms per token,     5.86 tokens per second)\n",
      "llama_print_timings:       total time =   45511.23 ms /   618 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      13.48 ms /   112 runs   (    0.12 ms per token,  8311.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14801.90 ms /   730 tokens (   20.28 ms per token,    49.32 tokens per second)\n",
      "llama_print_timings:        eval time =   19991.27 ms /   111 runs   (  180.10 ms per token,     5.55 tokens per second)\n",
      "llama_print_timings:       total time =   35053.92 ms /   841 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    44 runs   (    0.15 ms per token,  6864.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8028.61 ms /   367 tokens (   21.88 ms per token,    45.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9428.81 ms /    43 runs   (  219.27 ms per token,     4.56 tokens per second)\n",
      "llama_print_timings:       total time =   17556.01 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      16.16 ms /   130 runs   (    0.12 ms per token,  8045.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8794.62 ms /   397 tokens (   22.15 ms per token,    45.14 tokens per second)\n",
      "llama_print_timings:        eval time =   30969.39 ms /   129 runs   (  240.07 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:       total time =   40030.97 ms /   526 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =       6.81 ms /    45 runs   (    0.15 ms per token,  6607.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8198.25 ms /   366 tokens (   22.40 ms per token,    44.64 tokens per second)\n",
      "llama_print_timings:        eval time =    8835.33 ms /    44 runs   (  200.80 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:       total time =   17127.83 ms /   410 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      10.86 ms /    96 runs   (    0.11 ms per token,  8843.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9701.70 ms /   368 tokens (   26.36 ms per token,    37.93 tokens per second)\n",
      "llama_print_timings:        eval time =   16252.32 ms /    95 runs   (  171.08 ms per token,     5.85 tokens per second)\n",
      "llama_print_timings:       total time =   26145.03 ms /   463 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      15.49 ms /   136 runs   (    0.11 ms per token,  8777.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9023.40 ms /   355 tokens (   25.42 ms per token,    39.34 tokens per second)\n",
      "llama_print_timings:        eval time =   24007.90 ms /   135 runs   (  177.84 ms per token,     5.62 tokens per second)\n",
      "llama_print_timings:       total time =   33288.25 ms /   490 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      33.26 ms /   256 runs   (    0.13 ms per token,  7697.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7819.83 ms /   382 tokens (   20.47 ms per token,    48.85 tokens per second)\n",
      "llama_print_timings:        eval time =   53218.38 ms /   255 runs   (  208.70 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   61593.24 ms /   637 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      11.02 ms /    96 runs   (    0.11 ms per token,  8710.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5251.71 ms /   174 tokens (   30.18 ms per token,    33.13 tokens per second)\n",
      "llama_print_timings:        eval time =   22217.34 ms /    95 runs   (  233.87 ms per token,     4.28 tokens per second)\n",
      "llama_print_timings:       total time =   27658.56 ms /   269 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   14048.92 ms\n",
      "llama_print_timings:      sample time =      18.51 ms /   167 runs   (    0.11 ms per token,  9023.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9141.63 ms /   388 tokens (   23.56 ms per token,    42.44 tokens per second)\n",
      "llama_print_timings:        eval time =   43325.48 ms /   166 runs   (  261.00 ms per token,     3.83 tokens per second)\n",
      "llama_print_timings:       total time =   52765.22 ms /   554 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "eval_results_dict = {}\n",
    "eval_results_dict_2 = {}\n",
    "questions_per_chunk = 1\n",
    "output_name = \"MISQ4KM_bge_small_re2\"\n",
    "\n",
    "for parser_name, parser in parsers.items():\n",
    "    print(f\"\\n{parser_name}\\n\")\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    Settings.llm = llm_mixtral\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    qa_dataset = generate_question_context_pairs(\n",
    "        nodes,\n",
    "        llm=llm_mixtral,\n",
    "        num_questions_per_chunk=questions_per_chunk\n",
    "    )\n",
    "\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "    # Create retriever evaluator\n",
    "    # It evaluates a retriever using a set of metrics. (here : hit rate, is the correct context among the retrieved ones\n",
    "    # and MRR, how well the correct context is positioned among retrieved contexts\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names([\"mrr\", \"hit_rate\"], retriever=retriever)\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)  # Can't put this line in a function otherwise it raises an error\n",
    "    eval_results_dict[parser_name] = eval_results\n",
    "\n",
    "    # Create query engine\n",
    "    query_engine = get_query_engine(sentence_index=vector_index, similarity_top_k=3, rerank_top_n=2)\n",
    "\n",
    "    # Get queries created from Mixtral\n",
    "    queries = list(qa_dataset.queries.values())\n",
    "\n",
    "    # Compute faithfulness evaluation\n",
    "    # Evaluates whether a response is faithful to the contexts\n",
    "    # (i.e. whether the response is supported by the contexts or hallucinated.)\n",
    "    # For faithfulness score the base prompt had to be changed since it did not respect the instructions given and so the scores were 90% of the time at None or something with apple\n",
    "    # because of the base preprompt\n",
    "    faithfulness_custom_template = PromptTemplate(\n",
    "    \"\"\"In this task, you will act as a faithfulness evaluator for a language model's responses. Your job is to determine whether the model's response is faithful to the given context, i.e. \n",
    "    whether the response is supported by the context or if it is hallucinated.\n",
    "\n",
    "    Here are the criteria for your evaluation:\n",
    "\n",
    "    * If the response is not supported by the context, you should give it a score of 0.\n",
    "    * If the response is fully supported by the context, you should give it a score of 1.\n",
    "    * If the response is partially supported by the context, you may give it a score between 0 and 1, where a score closer to 1 indicates greater faithfulness to the context.\n",
    "\n",
    "    Here are two examples to help you understand the task:\n",
    "\n",
    "    Example 1:\n",
    "    Context: \"The capital of France is Paris.\"\n",
    "    Model response: \"The capital of France is Rome.\"\n",
    "    Faithfulness score: 0 (The response is not supported by the context, as the capital of France is Paris, not Rome.)\n",
    "\n",
    "    Example 2:\n",
    "    Context: \"The capital of France is Paris. The Eiffel Tower is a famous landmark in Paris.\"\n",
    "    Model response: \"The Eiffel Tower is a famous landmark in the capital of France.\"\n",
    "    Faithfulness score: 1 (The response is fully supported by the context, as the Eiffel Tower is indeed a famous landmark in Paris, which is the capital of France.)\n",
    "\n",
    "    Based on given context:\n",
    "    \\n\"{context_str}\"\\n\n",
    "\n",
    "    And the model answer :\n",
    "    \\n\"{query_str}\"\\n\n",
    "\n",
    "    Evaluate the faithfulness of the following model response. Put your score as \"My score = \"\"\")\n",
    "    faithfulness_mixtral = FaithfulnessEvaluator(llm=llm_mixtral, eval_template=faithfulness_custom_template)\n",
    "\n",
    "    # Compute relevancy evaluation\n",
    "    # Evaluates the relevancy of retrieved contexts and response to a query.\n",
    "    # This evaluator considers the query string, retrieved contexts, and response string.\n",
    "    relevancy_mixtral = RelevancyEvaluator(llm=llm_mixtral)\n",
    "\n",
    "    # Compute answer relevancy evaluation\n",
    "    # Evaluates the relevancy of response to a query.\n",
    "    # This evaluator considers the query string and response string.\n",
    "    # Focuses on assessing how pertinent the generated answer is to the given prompt\n",
    "    # For answer relevancy score the base prompt had to be changed since it did not respect the instructions given and so the scores were 90% of the time at None\n",
    "    answer_relevancy_custom_template = PromptTemplate(\n",
    "    \"\"\"Your goal is to evaluate the answer relevancy of an other model to a question. You have to score the model's answer between 0 and 1. 1 meaning the \n",
    "    model perfectly answered the question, 0 meaning it does not at all answer the question. For example, to the question \"What is the capital of France?\" \n",
    "    The answer \"The capital of France is Paris.\" will have an answer relevancy score of 1, whereas the answer \"France is a country in Europe with many famous cities like Paris and Lyon.\" \n",
    "    will have an answer relevancy score of 0.5 and \"The capital of Spain is Madrid.\" will have an answer relevancy score of 0.\\n\\n\n",
    "\n",
    "    Based on this query :\n",
    "    \\n\"{query}\"\\n\n",
    "\n",
    "    And the model answer :\n",
    "    \\n\"{response}\"\\n\n",
    "\n",
    "    How would you rate the model answer relevancy to the question ? Put your score as \"My score = \"\"\"\n",
    "    )\n",
    "    answer_relevancy_mixtral = AnswerRelevancyEvaluator(llm=llm_mixtral, eval_template=answer_relevancy_custom_template)\n",
    "\n",
    "    # Compute correctness evaluation\n",
    "    # Evaluate the relevance and correctness of a generated answer against a reference answer.\n",
    "    correctness_mixtral = CorrectnessEvaluator(llm=llm_mixtral)\n",
    "\n",
    "    # Initiate BatchEvalRunner to compute FaithFulness and Relevancy Evaluation.\n",
    "    runner = BatchEvalRunner({\"faithfulness\": faithfulness_mixtral, \"relevancy\": relevancy_mixtral,\n",
    "                              \"answer_relevancy\": answer_relevancy_mixtral, \"correctness\": correctness_mixtral},\n",
    "                              workers=8)\n",
    "\n",
    "    # Compute evaluation\n",
    "    eval_results_2 = await runner.aevaluate_queries(query_engine, queries=queries)\n",
    "    eval_results_dict_2[parser_name] = eval_results_2\n",
    "    \n",
    "# Format metrics and save it\n",
    "df_hrr_mrr = compute_hit_hrr_results(eval_results_dict)\n",
    "df_other_metrics = compute_correctness_relevancy_answer_relevancy_faithfulness_results(eval_results_dict_2)\n",
    "df_results = pd.merge(df_hrr_mrr, df_other_metrics, on=\"retriever_name\")\n",
    "df_results[\"nodes_nbr\"] = len(nodes)\n",
    "df_results[\"retriever_name\"] = df_results[\"retriever_name\"].apply(lambda x: x + \"_\" + output_name)\n",
    "df_results[\"mean_score\"] = df_results.select_dtypes(include=['number']).mean(axis=1)\n",
    "df_results.to_csv(os.path.join(results_folder, output_name + \".csv\"), index=False)\n",
    "\n",
    "# Save full results\n",
    "with open(os.path.join(results_folder, output_name + \"_hrr_mrr_full_results.txt\"), \"w\") as f:\n",
    "    f.write(jsonpickle.encode(eval_results_dict))\n",
    "\n",
    "with open(os.path.join(results_folder, output_name + \"_other_metrics_full_results.txt\"), \"w\") as f:\n",
    "    f.write(jsonpickle.encode(eval_results_dict_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results\n",
    "# Convert it to dataframe\n",
    "# Plot graphs\n",
    "# Test various embeddings, rerankings, model, parsers\n",
    "# Add testing time : make 10 queries and compute time spent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(folder_path: str, output_file: str):\n",
    "    # Get a list of files in the folder\n",
    "    file_list = os.listdir(folder_path)\n",
    "\n",
    "    # Filter only CSV files if needed\n",
    "    csv_files = [file for file in file_list if file.endswith('.csv')]\n",
    "\n",
    "    # Initialize an empty list to store DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Iterate over each CSV file\n",
    "    for csv_file in csv_files:\n",
    "        # Construct the full path to the CSV file\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Combine dataframes\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df[\"retriever_name\"] = combined_df[\"retriever_name\"].apply(lambda x: x.replace(\"semantic_splitter\", \"ssp\"))\n",
    "    combined_df[\"retriever_name\"] = combined_df[\"retriever_name\"].apply(lambda x: x.replace(\"token_splitter\", \"tsp\"))\n",
    "    combined_df[\"mean_score\"] = combined_df.select_dtypes(include=['number']).mean(axis=1)\n",
    "\n",
    "\n",
    "    # Convert metrics to percentage\n",
    "    metric_columns = ['hit_rate', 'mrr', 'faithfulness', 'relevancy', 'correctness', 'answer_relevancy', 'mean_score']\n",
    "    combined_df[metric_columns] *= 100\n",
    "\n",
    "    # Create figure with subplots for each metric\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Create subplots for each metric in a 2-column layout\n",
    "    fig = make_subplots(rows=math.ceil(len(metric_columns) / 2), cols=2, subplot_titles=[col.capitalize() for col in metric_columns])\n",
    "\n",
    "    for i, col in enumerate(metric_columns, start=1):\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=combined_df['retriever_name'],\n",
    "                y=combined_df[col],\n",
    "                text=combined_df[col].round(2),  # Display values rounded to 2 decimal places as text on bars\n",
    "                textposition='auto',  # Automatically place text on bars\n",
    "                marker_color=px.colors.qualitative.Plotly[i-1],  # Use Plotly qualitative color palette\n",
    "                showlegend=False,  # Hide legend for individual plots\n",
    "            ),\n",
    "            row=(i + 1) // 2, col=(i % 2) + 1  # Place subplots in 2 columns per row\n",
    "        )\n",
    "\n",
    "    # Update layout for the entire figure\n",
    "    fig.update_layout(\n",
    "        height=1500,  # Adjust height as needed\n",
    "        width=1000,  # Adjust width as needed\n",
    "        title='Comparison of Metrics by Retriever Name',\n",
    "        xaxis_title='Retriever Name',\n",
    "        yaxis_title='Percentage',\n",
    "        yaxis_tickformat='.0f',  # Format y-axis ticks as integer\n",
    "    )\n",
    "\n",
    "    # Save the figure as an HTML file\n",
    "    fig.write_html(output_file)\n",
    "\n",
    "    print(f\"Figure saved as HTML: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved as HTML: data_evaluation/batch_1/results/test.html\n"
     ]
    }
   ],
   "source": [
    "plot_results(folder_path=results_folder, output_file=os.path.join(results_folder, \"test.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mistralai.client import MistralClient\n",
    "# from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "# api_key = input(\"Enter API key\")\n",
    "# model = \"open-mixtral-8x7b\"\n",
    "\n",
    "# client = MistralClient(api_key=api_key)\n",
    "\n",
    "# chat_response = client.chat(\n",
    "#     model=model,\n",
    "#     messages=[ChatMessage(role=\"user\", content=\"\"\"I want to evaluate an LLM's faithfulness. A faithfulness evaluator is defined as such : \"Evaluates whether a response is faithful to the \n",
    "#                           contexts (i.e. whether the response is supported by the contexts or hallucinated.). This evaluator only considers the response string and the list of context \n",
    "#                           strings. The score has to be between 0 and 1, 0 meaning the answer is not supported by context and so the modle hallucinated, 1 meaning the answer is fully supported\n",
    "#                           by the given context.\n",
    "#                           Based on given information, write me a preprompt, with at least two examples, to give to my model ?\"\"\")]\n",
    "# )\n",
    "\n",
    "# print(chat_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
