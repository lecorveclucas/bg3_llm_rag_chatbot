{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import glob\n",
    "import base64\n",
    "import pandas as pd\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter, SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, RetrieverEvaluator\n",
    "from transformers import AutoTokenizer\n",
    "import nest_asyncio\n",
    "\n",
    "# To allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define constants\n",
    "results_folder = os.path.join(\"data_evaluation\", \"full_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results(eval_results_dict: dict):\n",
    "    \"\"\"Display results from evaluate.\"\"\"\n",
    "    full_df = pd.DataFrame()\n",
    "    for name, eval_results in eval_results_dict.items():\n",
    "        metric_dicts = []\n",
    "        for eval_result in eval_results:\n",
    "            metric_dict = eval_result.metric_vals_dict\n",
    "            metric_dicts.append(metric_dict)\n",
    "\n",
    "        df = pd.DataFrame(metric_dicts)\n",
    "\n",
    "        hit_rate = df[\"hit_rate\"].mean()\n",
    "        mrr = df[\"mrr\"].mean()\n",
    "\n",
    "        metric_df = pd.DataFrame(\n",
    "            {\"Retriever Name\": [name], \"Hit Rate\": [hit_rate], \"MRR\": [mrr]}\n",
    "        )\n",
    "\n",
    "        full_df = pd.concat([full_df, metric_df])\n",
    "\n",
    "    return full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, (14947.16 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, (15267.16 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, (15827.17 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "input_folder = \"./data_evaluation/batch_1\"\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic_splitter \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/101 [00:00<?, ?it/s]\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.02 ms /   154 runs   (    0.08 ms per token, 12810.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18053.94 ms /   756 tokens (   23.88 ms per token,    41.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11712.03 ms /   153 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   29966.20 ms /   909 tokens\n",
      "  1%|          | 1/101 [00:29<49:57, 29.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    62 runs   (    0.08 ms per token, 12889.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2715.31 ms /   117 tokens (   23.21 ms per token,    43.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.40 ms /    61 runs   (   71.14 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    7136.51 ms /   178 tokens\n",
      "  2%|▏         | 2/101 [00:37<27:17, 16.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.21 ms /    99 runs   (    0.07 ms per token, 13736.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2374.18 ms /   117 tokens (   20.29 ms per token,    49.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6801.93 ms /    98 runs   (   69.41 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    9300.17 ms /   215 tokens\n",
      "  3%|▎         | 3/101 [00:46<21:37, 13.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.63 ms /    49 runs   (    0.07 ms per token, 13506.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2216.13 ms /    92 tokens (   24.09 ms per token,    41.51 tokens per second)\n",
      "llama_print_timings:        eval time =    3145.71 ms /    48 runs   (   65.54 ms per token,    15.26 tokens per second)\n",
      "llama_print_timings:       total time =    5423.16 ms /   140 tokens\n",
      "  4%|▍         | 4/101 [00:51<16:25, 10.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.45 ms /    48 runs   (    0.07 ms per token, 13913.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2822.10 ms /   153 tokens (   18.45 ms per token,    54.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3074.91 ms /    47 runs   (   65.42 ms per token,    15.29 tokens per second)\n",
      "llama_print_timings:       total time =    5956.77 ms /   200 tokens\n",
      "  5%|▍         | 5/101 [00:57<13:49,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.06 ms /    75 runs   (    0.08 ms per token, 12374.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6111.74 ms /   452 tokens (   13.52 ms per token,    73.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5896.77 ms /    74 runs   (   79.69 ms per token,    12.55 tokens per second)\n",
      "llama_print_timings:       total time =   12110.01 ms /   526 tokens\n",
      "  6%|▌         | 6/101 [01:09<15:33,  9.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.59 ms /    91 runs   (    0.08 ms per token, 11986.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4366.06 ms /   192 tokens (   22.74 ms per token,    43.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9057.28 ms /    90 runs   (  100.64 ms per token,     9.94 tokens per second)\n",
      "llama_print_timings:       total time =   13555.55 ms /   282 tokens\n",
      "  7%|▋         | 7/101 [01:23<17:18, 11.05s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.03 ms /   170 runs   (    0.08 ms per token, 13045.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3868.34 ms /   269 tokens (   14.38 ms per token,    69.54 tokens per second)\n",
      "llama_print_timings:        eval time =   11294.56 ms /   169 runs   (   66.83 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =   15380.27 ms /   438 tokens\n",
      "  8%|▊         | 8/101 [01:38<19:15, 12.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.73 ms /    64 runs   (    0.07 ms per token, 13536.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8644.96 ms /   624 tokens (   13.85 ms per token,    72.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4350.16 ms /    63 runs   (   69.05 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time =   13076.46 ms /   687 tokens\n",
      "  9%|▉         | 9/101 [01:51<19:22, 12.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.09 ms /    68 runs   (    0.07 ms per token, 13351.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2853.48 ms /   146 tokens (   19.54 ms per token,    51.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4613.45 ms /    67 runs   (   68.86 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    7553.73 ms /   213 tokens\n",
      " 10%|▉         | 10/101 [01:59<16:47, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.57 ms /    68 runs   (    0.08 ms per token, 12210.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3304.26 ms /   178 tokens (   18.56 ms per token,    53.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6464.45 ms /    67 runs   (   96.48 ms per token,    10.36 tokens per second)\n",
      "llama_print_timings:       total time =    9861.08 ms /   245 tokens\n",
      " 11%|█         | 11/101 [02:09<16:03, 10.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.03 ms /    87 runs   (    0.08 ms per token, 12380.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4191.52 ms /   300 tokens (   13.97 ms per token,    71.57 tokens per second)\n",
      "llama_print_timings:        eval time =    6623.54 ms /    86 runs   (   77.02 ms per token,    12.98 tokens per second)\n",
      "llama_print_timings:       total time =   10934.55 ms /   386 tokens\n",
      " 12%|█▏        | 12/101 [02:20<15:58, 10.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /   102 runs   (    0.08 ms per token, 13058.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8880.03 ms /   618 tokens (   14.37 ms per token,    69.59 tokens per second)\n",
      "llama_print_timings:        eval time =    7994.84 ms /   101 runs   (   79.16 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   17014.98 ms /   719 tokens\n",
      " 13%|█▎        | 13/101 [02:37<18:34, 12.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    78 runs   (    0.07 ms per token, 13605.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4438.62 ms /   333 tokens (   13.33 ms per token,    75.02 tokens per second)\n",
      "llama_print_timings:        eval time =    5137.80 ms /    77 runs   (   66.72 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =    9675.69 ms /   410 tokens\n",
      " 14%|█▍        | 14/101 [02:47<17:03, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      17.00 ms /   229 runs   (    0.07 ms per token, 13466.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8395.27 ms /   599 tokens (   14.02 ms per token,    71.35 tokens per second)\n",
      "llama_print_timings:        eval time =   15630.23 ms /   228 runs   (   68.55 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =   24327.05 ms /   827 tokens\n",
      " 15%|█▍        | 15/101 [03:11<22:17, 15.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    88 runs   (    0.07 ms per token, 13645.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9384.82 ms /   611 tokens (   15.36 ms per token,    65.11 tokens per second)\n",
      "llama_print_timings:        eval time =    6306.25 ms /    87 runs   (   72.49 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   15809.17 ms /   698 tokens\n",
      " 16%|█▌        | 16/101 [03:27<22:08, 15.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.11 ms /    81 runs   (    0.08 ms per token, 13250.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4627.50 ms /   339 tokens (   13.65 ms per token,    73.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5908.62 ms /    80 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =   10644.06 ms /   419 tokens\n",
      " 17%|█▋        | 17/101 [03:37<19:47, 14.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    70 runs   (    0.08 ms per token, 12544.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9340.30 ms /   590 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5598.02 ms /    69 runs   (   81.13 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   15038.52 ms /   659 tokens\n",
      " 18%|█▊        | 18/101 [03:52<19:55, 14.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.06 ms /    63 runs   (    0.08 ms per token, 12457.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6364.06 ms /   493 tokens (   12.91 ms per token,    77.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5290.84 ms /    62 runs   (   85.34 ms per token,    11.72 tokens per second)\n",
      "llama_print_timings:       total time =   11742.35 ms /   555 tokens\n",
      " 19%|█▉        | 19/101 [04:04<18:36, 13.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    69 runs   (    0.08 ms per token, 12062.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3179.33 ms /   174 tokens (   18.27 ms per token,    54.73 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.85 ms /    68 runs   (   85.48 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =    9091.19 ms /   242 tokens\n",
      " 20%|█▉        | 20/101 [04:13<16:32, 12.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    90 runs   (    0.08 ms per token, 12710.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3654.85 ms /   195 tokens (   18.74 ms per token,    53.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9051.43 ms /    89 runs   (  101.70 ms per token,     9.83 tokens per second)\n",
      "llama_print_timings:       total time =   12829.68 ms /   284 tokens\n",
      " 21%|██        | 21/101 [04:26<16:34, 12.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    68 runs   (    0.07 ms per token, 13570.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12968.38 ms /   907 tokens (   14.30 ms per token,    69.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.62 ms /    67 runs   (   74.31 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =   18040.43 ms /   974 tokens\n",
      " 22%|██▏       | 22/101 [04:44<18:35, 14.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.82 ms /    77 runs   (    0.08 ms per token, 13239.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.10 ms /   182 tokens (   16.52 ms per token,    60.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5091.00 ms /    76 runs   (   66.99 ms per token,    14.93 tokens per second)\n",
      "llama_print_timings:       total time =    8197.42 ms /   258 tokens\n",
      " 23%|██▎       | 23/101 [04:52<16:02, 12.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.13 ms /    78 runs   (    0.08 ms per token, 12718.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3497.20 ms /   218 tokens (   16.04 ms per token,    62.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5649.99 ms /    77 runs   (   73.38 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =    9253.89 ms /   295 tokens\n",
      " 24%|██▍       | 24/101 [05:02<14:39, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.26 ms /    94 runs   (    0.08 ms per token, 12945.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11297.81 ms /   836 tokens (   13.51 ms per token,    74.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7583.21 ms /    93 runs   (   81.54 ms per token,    12.26 tokens per second)\n",
      "llama_print_timings:       total time =   19017.22 ms /   929 tokens\n",
      " 25%|██▍       | 25/101 [05:21<17:21, 13.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.36 ms /    92 runs   (    0.08 ms per token, 12505.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5946.19 ms /   456 tokens (   13.04 ms per token,    76.69 tokens per second)\n",
      "llama_print_timings:        eval time =    7150.57 ms /    91 runs   (   78.58 ms per token,    12.73 tokens per second)\n",
      "llama_print_timings:       total time =   13232.47 ms /   547 tokens\n",
      " 26%|██▌       | 26/101 [05:34<16:57, 13.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    61 runs   (    0.09 ms per token, 11344.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.48 ms /   267 tokens (   15.24 ms per token,    65.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4890.34 ms /    60 runs   (   81.51 ms per token,    12.27 tokens per second)\n",
      "llama_print_timings:       total time =    9049.37 ms /   327 tokens\n",
      " 27%|██▋       | 27/101 [05:43<15:03, 12.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    71 runs   (    0.08 ms per token, 12384.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2823.14 ms /   142 tokens (   19.88 ms per token,    50.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5084.73 ms /    70 runs   (   72.64 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    8004.18 ms /   212 tokens\n",
      " 28%|██▊       | 28/101 [05:51<13:19, 10.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    76 runs   (    0.07 ms per token, 13615.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6649.52 ms /   516 tokens (   12.89 ms per token,    77.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5149.99 ms /    75 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =   11897.63 ms /   591 tokens\n",
      " 29%|██▊       | 29/101 [06:03<13:29, 11.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    72 runs   (    0.07 ms per token, 13748.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3582.96 ms /   254 tokens (   14.11 ms per token,    70.89 tokens per second)\n",
      "llama_print_timings:        eval time =    4683.86 ms /    71 runs   (   65.97 ms per token,    15.16 tokens per second)\n",
      "llama_print_timings:       total time =    8357.64 ms /   325 tokens\n",
      " 30%|██▉       | 30/101 [06:11<12:16, 10.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.03 ms /    55 runs   (    0.07 ms per token, 13657.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5613.61 ms /   454 tokens (   12.36 ms per token,    80.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3629.55 ms /    54 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    9312.68 ms /   508 tokens\n",
      " 31%|███       | 31/101 [06:21<11:43, 10.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    54 runs   (    0.07 ms per token, 13650.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2506.56 ms /   118 tokens (   21.24 ms per token,    47.08 tokens per second)\n",
      "llama_print_timings:        eval time =    3451.56 ms /    53 runs   (   65.12 ms per token,    15.36 tokens per second)\n",
      "llama_print_timings:       total time =    6024.10 ms /   171 tokens\n",
      " 32%|███▏      | 32/101 [06:27<10:10,  8.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    84 runs   (    0.07 ms per token, 14098.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10013.06 ms /   766 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5747.01 ms /    83 runs   (   69.24 ms per token,    14.44 tokens per second)\n",
      "llama_print_timings:       total time =   15868.22 ms /   849 tokens\n",
      " 33%|███▎      | 33/101 [06:42<12:24, 10.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.73 ms /   108 runs   (    0.07 ms per token, 13966.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4652.02 ms /   353 tokens (   13.18 ms per token,    75.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7134.48 ms /   107 runs   (   66.68 ms per token,    15.00 tokens per second)\n",
      "llama_print_timings:       total time =   11923.06 ms /   460 tokens\n",
      " 34%|███▎      | 34/101 [06:54<12:33, 11.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    63 runs   (    0.08 ms per token, 13174.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2683.18 ms /   129 tokens (   20.80 ms per token,    48.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4030.98 ms /    62 runs   (   65.02 ms per token,    15.38 tokens per second)\n",
      "llama_print_timings:       total time =    6792.14 ms /   191 tokens\n",
      " 35%|███▍      | 35/101 [07:01<10:54,  9.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    64 runs   (    0.07 ms per token, 13468.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3034.21 ms /   188 tokens (   16.14 ms per token,    61.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4119.95 ms /    63 runs   (   65.40 ms per token,    15.29 tokens per second)\n",
      "llama_print_timings:       total time =    7235.23 ms /   251 tokens\n",
      " 36%|███▌      | 36/101 [07:08<09:52,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    55 runs   (    0.07 ms per token, 13681.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15335.77 ms /  1152 tokens (   13.31 ms per token,    75.12 tokens per second)\n",
      "llama_print_timings:        eval time =    3812.76 ms /    54 runs   (   70.61 ms per token,    14.16 tokens per second)\n",
      "llama_print_timings:       total time =   19221.02 ms /  1206 tokens\n",
      " 37%|███▋      | 37/101 [07:28<12:57, 12.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    85 runs   (    0.08 ms per token, 13032.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27712.60 ms /  2065 tokens (   13.42 ms per token,    74.51 tokens per second)\n",
      "llama_print_timings:        eval time =    7479.48 ms /    84 runs   (   89.04 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   35309.09 ms /  2149 tokens\n",
      " 38%|███▊      | 38/101 [08:03<20:03, 19.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    95 runs   (    0.08 ms per token, 13036.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11346.44 ms /   716 tokens (   15.85 ms per token,    63.10 tokens per second)\n",
      "llama_print_timings:        eval time =    6569.49 ms /    94 runs   (   69.89 ms per token,    14.31 tokens per second)\n",
      "llama_print_timings:       total time =   18041.89 ms /   810 tokens\n",
      " 39%|███▊      | 39/101 [08:21<19:24, 18.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.60 ms /    62 runs   (    0.07 ms per token, 13478.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2126.61 ms /    85 tokens (   25.02 ms per token,    39.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3956.54 ms /    61 runs   (   64.86 ms per token,    15.42 tokens per second)\n",
      "llama_print_timings:       total time =    6161.03 ms /   146 tokens\n",
      " 40%|███▉      | 40/101 [08:27<15:14, 15.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    52 runs   (    0.08 ms per token, 13147.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2756.61 ms /   133 tokens (   20.73 ms per token,    48.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3330.74 ms /    51 runs   (   65.31 ms per token,    15.31 tokens per second)\n",
      "llama_print_timings:       total time =    6152.98 ms /   184 tokens\n",
      " 41%|████      | 41/101 [08:33<12:20, 12.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    68 runs   (    0.08 ms per token, 12705.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2720.64 ms /   137 tokens (   19.86 ms per token,    50.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4464.39 ms /    67 runs   (   66.63 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7271.58 ms /   204 tokens\n",
      " 42%|████▏     | 42/101 [08:41<10:38, 10.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    72 runs   (    0.07 ms per token, 14299.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10358.97 ms /   759 tokens (   13.65 ms per token,    73.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4931.52 ms /    71 runs   (   69.46 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   15384.65 ms /   830 tokens\n",
      " 43%|████▎     | 43/101 [08:56<11:47, 12.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.71 ms /   101 runs   (    0.08 ms per token, 13101.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9635.84 ms /   697 tokens (   13.82 ms per token,    72.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6873.34 ms /   100 runs   (   68.73 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   16639.64 ms /   797 tokens\n",
      " 44%|████▎     | 44/101 [09:13<12:51, 13.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    69 runs   (    0.07 ms per token, 13487.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2337.28 ms /    79 tokens (   29.59 ms per token,    33.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4428.89 ms /    68 runs   (   65.13 ms per token,    15.35 tokens per second)\n",
      "llama_print_timings:       total time =    6854.89 ms /   147 tokens\n",
      " 45%|████▍     | 45/101 [09:19<10:45, 11.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.87 ms /    76 runs   (    0.08 ms per token, 12958.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2680.85 ms /   145 tokens (   18.49 ms per token,    54.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4884.47 ms /    75 runs   (   65.13 ms per token,    15.35 tokens per second)\n",
      "llama_print_timings:       total time =    7661.71 ms /   220 tokens\n",
      " 46%|████▌     | 46/101 [09:27<09:30, 10.37s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    85 runs   (    0.07 ms per token, 13571.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4267.50 ms /   304 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_print_timings:        eval time =    5564.57 ms /    84 runs   (   66.24 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    9940.29 ms /   388 tokens\n",
      " 47%|████▋     | 47/101 [09:37<09:13, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    90 runs   (    0.07 ms per token, 13499.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4733.17 ms /   358 tokens (   13.22 ms per token,    75.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5987.77 ms /    89 runs   (   67.28 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =   10834.85 ms /   447 tokens\n",
      " 48%|████▊     | 48/101 [09:48<09:12, 10.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    68 runs   (    0.07 ms per token, 13798.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4540.02 ms /   347 tokens (   13.08 ms per token,    76.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4442.06 ms /    67 runs   (   66.30 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:       total time =    9068.50 ms /   414 tokens\n",
      " 49%|████▊     | 49/101 [09:57<08:40, 10.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.39 ms /   132 runs   (    0.07 ms per token, 14056.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9243.09 ms /   662 tokens (   13.96 ms per token,    71.62 tokens per second)\n",
      "llama_print_timings:        eval time =    9025.65 ms /   131 runs   (   68.90 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =   18440.69 ms /   793 tokens\n",
      " 50%|████▉     | 50/101 [10:15<10:39, 12.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.03 ms /    40 runs   (    0.08 ms per token, 13214.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2379.99 ms /    80 tokens (   29.75 ms per token,    33.61 tokens per second)\n",
      "llama_print_timings:        eval time =    2531.40 ms /    39 runs   (   64.91 ms per token,    15.41 tokens per second)\n",
      "llama_print_timings:       total time =    4962.16 ms /   119 tokens\n",
      " 50%|█████     | 51/101 [10:20<08:33, 10.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    66 runs   (    0.07 ms per token, 13678.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2347.44 ms /   107 tokens (   21.94 ms per token,    45.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4224.37 ms /    65 runs   (   64.99 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:       total time =    6655.26 ms /   172 tokens\n",
      " 51%|█████▏    | 52/101 [10:27<07:30,  9.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.95 ms /    97 runs   (    0.07 ms per token, 13952.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3592.46 ms /   229 tokens (   15.69 ms per token,    63.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6353.25 ms /    96 runs   (   66.18 ms per token,    15.11 tokens per second)\n",
      "llama_print_timings:       total time =   10068.10 ms /   325 tokens\n",
      " 52%|█████▏    | 53/101 [10:37<07:33,  9.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.16 ms /    71 runs   (    0.07 ms per token, 13759.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15779.51 ms /  1142 tokens (   13.82 ms per token,    72.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4944.96 ms /    70 runs   (   70.64 ms per token,    14.16 tokens per second)\n",
      "llama_print_timings:       total time =   20817.61 ms /  1212 tokens\n",
      " 53%|█████▎    | 54/101 [10:58<10:04, 12.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    66 runs   (    0.07 ms per token, 13496.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3347.24 ms /   206 tokens (   16.25 ms per token,    61.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4393.87 ms /    65 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    7825.99 ms /   271 tokens\n",
      " 54%|█████▍    | 55/101 [11:06<08:42, 11.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.88 ms /    79 runs   (    0.07 ms per token, 13430.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4533.76 ms /   334 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_print_timings:        eval time =    5193.70 ms /    78 runs   (   66.59 ms per token,    15.02 tokens per second)\n",
      "llama_print_timings:       total time =    9828.69 ms /   412 tokens\n",
      " 55%|█████▌    | 56/101 [11:16<08:10, 10.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    73 runs   (    0.07 ms per token, 13729.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6682.82 ms /   516 tokens (   12.95 ms per token,    77.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4880.73 ms /    72 runs   (   67.79 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =   11658.46 ms /   588 tokens\n",
      " 56%|█████▋    | 57/101 [11:27<08:09, 11.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    68 runs   (    0.07 ms per token, 13500.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4284.06 ms /   311 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4445.35 ms /    67 runs   (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    8814.72 ms /   378 tokens\n",
      " 57%|█████▋    | 58/101 [11:36<07:28, 10.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.33 ms /    45 runs   (    0.07 ms per token, 13521.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2425.11 ms /   100 tokens (   24.25 ms per token,    41.24 tokens per second)\n",
      "llama_print_timings:        eval time =    2858.54 ms /    44 runs   (   64.97 ms per token,    15.39 tokens per second)\n",
      "llama_print_timings:       total time =    5340.26 ms /   144 tokens\n",
      " 58%|█████▊    | 59/101 [11:41<06:14,  8.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    63 runs   (    0.07 ms per token, 13840.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10063.49 ms /   762 tokens (   13.21 ms per token,    75.72 tokens per second)\n",
      "llama_print_timings:        eval time =    4284.13 ms /    62 runs   (   69.10 ms per token,    14.47 tokens per second)\n",
      "llama_print_timings:       total time =   14429.31 ms /   824 tokens\n",
      " 59%|█████▉    | 60/101 [11:56<07:13, 10.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.83 ms /    75 runs   (    0.08 ms per token, 12862.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3070.44 ms /   166 tokens (   18.50 ms per token,    54.06 tokens per second)\n",
      "llama_print_timings:        eval time =    4842.31 ms /    74 runs   (   65.44 ms per token,    15.28 tokens per second)\n",
      "llama_print_timings:       total time =    8008.11 ms /   240 tokens\n",
      " 60%|██████    | 61/101 [12:04<06:31,  9.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    63 runs   (    0.07 ms per token, 13639.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6585.83 ms /   515 tokens (   12.79 ms per token,    78.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4194.18 ms /    62 runs   (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time =   10861.43 ms /   577 tokens\n",
      " 61%|██████▏   | 62/101 [12:15<06:34, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    72 runs   (    0.08 ms per token, 12598.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8551.43 ms /   573 tokens (   14.92 ms per token,    67.01 tokens per second)\n",
      "llama_print_timings:        eval time =   70224.48 ms /    71 runs   (  989.08 ms per token,     1.01 tokens per second)\n",
      "llama_print_timings:       total time =   78887.62 ms /   644 tokens\n",
      " 62%|██████▏   | 63/101 [13:34<19:28, 30.75s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.56 ms /    88 runs   (    0.07 ms per token, 13422.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9738.12 ms /   729 tokens (   13.36 ms per token,    74.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5999.55 ms /    87 runs   (   68.96 ms per token,    14.50 tokens per second)\n",
      "llama_print_timings:       total time =   15852.73 ms /   816 tokens\n",
      " 63%|██████▎   | 64/101 [13:49<16:12, 26.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    54 runs   (    0.07 ms per token, 13636.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5696.74 ms /   453 tokens (   12.58 ms per token,    79.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3678.43 ms /    53 runs   (   69.40 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    9444.78 ms /   506 tokens\n",
      " 64%|██████▍   | 65/101 [13:59<12:44, 21.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.69 ms /   107 runs   (    0.07 ms per token, 13923.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14913.51 ms /  1076 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =    7521.71 ms /   106 runs   (   70.96 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =   22574.97 ms /  1182 tokens\n",
      " 65%|██████▌   | 66/101 [14:21<12:37, 21.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.64 ms /    61 runs   (    0.08 ms per token, 13140.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3448.49 ms /   218 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =    3962.92 ms /    60 runs   (   66.05 ms per token,    15.14 tokens per second)\n",
      "llama_print_timings:       total time =    7488.64 ms /   278 tokens\n",
      " 66%|██████▋   | 67/101 [14:29<09:51, 17.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    54 runs   (    0.07 ms per token, 13892.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2358.95 ms /   124 tokens (   19.02 ms per token,    52.57 tokens per second)\n",
      "llama_print_timings:        eval time =    3441.22 ms /    53 runs   (   64.93 ms per token,    15.40 tokens per second)\n",
      "llama_print_timings:       total time =    5868.16 ms /   177 tokens\n",
      " 67%|██████▋   | 68/101 [14:35<07:39, 13.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.70 ms /    64 runs   (    0.07 ms per token, 13611.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2971.13 ms /   167 tokens (   17.79 ms per token,    56.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4106.99 ms /    63 runs   (   65.19 ms per token,    15.34 tokens per second)\n",
      "llama_print_timings:       total time =    7158.34 ms /   230 tokens\n",
      " 68%|██████▊   | 69/101 [14:42<06:20, 11.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.44 ms /   158 runs   (    0.07 ms per token, 13812.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10063.91 ms /   739 tokens (   13.62 ms per token,    73.43 tokens per second)\n",
      "llama_print_timings:        eval time =   10837.10 ms /   157 runs   (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   21108.87 ms /   896 tokens\n",
      " 69%|██████▉   | 70/101 [15:03<07:34, 14.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    80 runs   (    0.07 ms per token, 13665.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8523.91 ms /   593 tokens (   14.37 ms per token,    69.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5372.03 ms /    79 runs   (   68.00 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =   13998.87 ms /   672 tokens\n",
      " 70%|███████   | 71/101 [15:17<07:14, 14.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.38 ms /    87 runs   (    0.07 ms per token, 13644.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11457.66 ms /   897 tokens (   12.77 ms per token,    78.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5998.21 ms /    86 runs   (   69.75 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =   17568.60 ms /   983 tokens\n",
      " 71%|███████▏  | 72/101 [15:35<07:26, 15.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.32 ms /    74 runs   (    0.07 ms per token, 13915.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5639.66 ms /   473 tokens (   11.92 ms per token,    83.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5956.81 ms /    73 runs   (   81.60 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =   11692.81 ms /   546 tokens\n",
      " 72%|███████▏  | 73/101 [15:46<06:40, 14.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.19 ms /   132 runs   (    0.07 ms per token, 14369.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =  356557.04 ms /   244 tokens ( 1461.30 ms per token,     0.68 tokens per second)\n",
      "llama_print_timings:        eval time =    8954.80 ms /   131 runs   (   68.36 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =  365678.39 ms /   375 tokens\n",
      " 73%|███████▎  | 74/101 [21:52<53:52, 119.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.75 ms /   149 runs   (    0.08 ms per token, 12676.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11508.01 ms /   859 tokens (   13.40 ms per token,    74.64 tokens per second)\n",
      "llama_print_timings:        eval time =   11060.66 ms /   148 runs   (   74.73 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =   22766.81 ms /  1007 tokens\n",
      " 74%|███████▍  | 75/101 [22:15<39:16, 90.63s/it] Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.42 ms /    61 runs   (    0.07 ms per token, 13816.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4074.67 ms /   303 tokens (   13.45 ms per token,    74.36 tokens per second)\n",
      "llama_print_timings:        eval time =    4168.39 ms /    60 runs   (   69.47 ms per token,    14.39 tokens per second)\n",
      "llama_print_timings:       total time =    8319.99 ms /   363 tokens\n",
      " 75%|███████▌  | 76/101 [22:23<27:28, 65.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    61 runs   (    0.09 ms per token, 11570.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   24307.61 ms /  1584 tokens (   15.35 ms per token,    65.16 tokens per second)\n",
      "llama_print_timings:        eval time =   16621.23 ms /    60 runs   (  277.02 ms per token,     3.61 tokens per second)\n",
      "llama_print_timings:       total time =   41044.39 ms /  1644 tokens\n",
      " 76%|███████▌  | 77/101 [23:06<23:34, 58.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.61 ms /   179 runs   (    0.09 ms per token, 11468.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7013.49 ms /   506 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =   18646.15 ms /   178 runs   (  104.75 ms per token,     9.55 tokens per second)\n",
      "llama_print_timings:       total time =   25931.07 ms /   684 tokens\n",
      " 77%|███████▋  | 78/101 [23:32<18:48, 49.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.25 ms /    38 runs   (    0.09 ms per token, 11695.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2525.99 ms /    99 tokens (   25.52 ms per token,    39.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3814.23 ms /    37 runs   (  103.09 ms per token,     9.70 tokens per second)\n",
      "llama_print_timings:       total time =    6401.95 ms /   136 tokens\n",
      " 78%|███████▊  | 79/101 [23:38<13:17, 36.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.71 ms /    71 runs   (    0.08 ms per token, 12434.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15360.18 ms /  1019 tokens (   15.07 ms per token,    66.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5798.19 ms /    70 runs   (   82.83 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   21262.04 ms /  1089 tokens\n",
      " 79%|███████▉  | 80/101 [23:59<11:06, 31.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.72 ms /    83 runs   (    0.08 ms per token, 12356.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4700.67 ms /   335 tokens (   14.03 ms per token,    71.27 tokens per second)\n",
      "llama_print_timings:        eval time =    6717.41 ms /    82 runs   (   81.92 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   11533.44 ms /   417 tokens\n",
      " 80%|████████  | 81/101 [24:11<08:33, 25.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.15 ms /    89 runs   (    0.08 ms per token, 12452.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17058.64 ms /  1161 tokens (   14.69 ms per token,    68.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7136.93 ms /    88 runs   (   81.10 ms per token,    12.33 tokens per second)\n",
      "llama_print_timings:       total time =   24328.10 ms /  1249 tokens\n",
      " 81%|████████  | 82/101 [24:35<08:00, 25.29s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    82 runs   (    0.08 ms per token, 13022.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   25919.98 ms /  1758 tokens (   14.74 ms per token,    67.82 tokens per second)\n",
      "llama_print_timings:        eval time =    6725.02 ms /    81 runs   (   83.02 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =   32769.70 ms /  1839 tokens\n",
      " 82%|████████▏ | 83/101 [25:08<08:15, 27.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.96 ms /    50 runs   (    0.08 ms per token, 12623.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2859.20 ms /   121 tokens (   23.63 ms per token,    42.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4765.58 ms /    49 runs   (   97.26 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =    7714.10 ms /   170 tokens\n",
      " 83%|████████▎ | 84/101 [25:16<06:07, 21.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.73 ms /    86 runs   (    0.08 ms per token, 12782.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12521.34 ms /   844 tokens (   14.84 ms per token,    67.40 tokens per second)\n",
      "llama_print_timings:        eval time =    6325.04 ms /    85 runs   (   74.41 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   18967.12 ms /   929 tokens\n",
      " 84%|████████▍ | 85/101 [25:35<05:32, 20.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      18.30 ms /   231 runs   (    0.08 ms per token, 12623.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5423.32 ms /   442 tokens (   12.27 ms per token,    81.50 tokens per second)\n",
      "llama_print_timings:        eval time =   17240.24 ms /   230 runs   (   74.96 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =   22977.78 ms /   672 tokens\n",
      " 85%|████████▌ | 86/101 [25:58<05:21, 21.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.55 ms /   120 runs   (    0.08 ms per token, 12560.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3063.80 ms /   188 tokens (   16.30 ms per token,    61.36 tokens per second)\n",
      "llama_print_timings:        eval time =    8128.14 ms /   119 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   11345.97 ms /   307 tokens\n",
      " 86%|████████▌ | 87/101 [26:09<04:17, 18.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    82 runs   (    0.07 ms per token, 14238.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8081.68 ms /   566 tokens (   14.28 ms per token,    70.03 tokens per second)\n",
      "llama_print_timings:        eval time =    5899.44 ms /    81 runs   (   72.83 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time =   14082.55 ms /   647 tokens\n",
      " 87%|████████▋ | 88/101 [26:23<03:42, 17.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    83 runs   (    0.07 ms per token, 13828.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5635.79 ms /   417 tokens (   13.52 ms per token,    73.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6015.82 ms /    82 runs   (   73.36 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   11757.59 ms /   499 tokens\n",
      " 88%|████████▊ | 89/101 [26:35<03:06, 15.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    86 runs   (    0.07 ms per token, 14266.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4664.00 ms /   379 tokens (   12.31 ms per token,    81.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5959.27 ms /    85 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   10729.90 ms /   464 tokens\n",
      " 89%|████████▉ | 90/101 [26:46<02:34, 14.08s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    67 runs   (    0.08 ms per token, 13214.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2893.83 ms /   169 tokens (   17.12 ms per token,    58.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4405.55 ms /    66 runs   (   66.75 ms per token,    14.98 tokens per second)\n",
      "llama_print_timings:       total time =    7389.63 ms /   235 tokens\n",
      " 90%|█████████ | 91/101 [26:53<02:00, 12.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.08 ms /   125 runs   (    0.07 ms per token, 13768.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11879.31 ms /   947 tokens (   12.54 ms per token,    79.72 tokens per second)\n",
      "llama_print_timings:        eval time =    9665.86 ms /   124 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   21711.44 ms /  1071 tokens\n",
      " 91%|█████████ | 92/101 [27:15<02:14, 14.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.17 ms /    55 runs   (    0.08 ms per token, 13205.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4606.09 ms /   337 tokens (   13.67 ms per token,    73.16 tokens per second)\n",
      "llama_print_timings:        eval time =    3958.55 ms /    54 runs   (   73.31 ms per token,    13.64 tokens per second)\n",
      "llama_print_timings:       total time =    8636.29 ms /   391 tokens\n",
      " 92%|█████████▏| 93/101 [27:23<01:44, 13.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    67 runs   (    0.07 ms per token, 14096.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3499.24 ms /   249 tokens (   14.05 ms per token,    71.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4537.54 ms /    66 runs   (   68.75 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =    8120.09 ms /   315 tokens\n",
      " 93%|█████████▎| 94/101 [27:32<01:21, 11.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.63 ms /    51 runs   (    0.07 ms per token, 14061.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2899.99 ms /   169 tokens (   17.16 ms per token,    58.28 tokens per second)\n",
      "llama_print_timings:        eval time =    3309.40 ms /    50 runs   (   66.19 ms per token,    15.11 tokens per second)\n",
      "llama_print_timings:       total time =    6273.09 ms /   219 tokens\n",
      " 94%|█████████▍| 95/101 [27:38<00:59,  9.99s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.56 ms /   103 runs   (    0.07 ms per token, 13622.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11306.60 ms /   889 tokens (   12.72 ms per token,    78.63 tokens per second)\n",
      "llama_print_timings:        eval time =    8064.82 ms /   102 runs   (   79.07 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =   19508.79 ms /   991 tokens\n",
      " 95%|█████████▌| 96/101 [27:57<01:04, 12.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.68 ms /   105 runs   (    0.07 ms per token, 13668.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9216.07 ms /   648 tokens (   14.22 ms per token,    70.31 tokens per second)\n",
      "llama_print_timings:        eval time =    7695.00 ms /   104 runs   (   73.99 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   17051.61 ms /   752 tokens\n",
      " 96%|█████████▌| 97/101 [28:14<00:56, 14.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.59 ms /    80 runs   (    0.08 ms per token, 12135.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5456.51 ms /   438 tokens (   12.46 ms per token,    80.27 tokens per second)\n",
      "llama_print_timings:        eval time =    8668.05 ms /    79 runs   (  109.72 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =   14244.87 ms /   517 tokens\n",
      " 97%|█████████▋| 98/101 [28:29<00:42, 14.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    96 runs   (    0.07 ms per token, 13588.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33454.15 ms /  2127 tokens (   15.73 ms per token,    63.58 tokens per second)\n",
      "llama_print_timings:        eval time =    9081.20 ms /    95 runs   (   95.59 ms per token,    10.46 tokens per second)\n",
      "llama_print_timings:       total time =   42674.84 ms /  2222 tokens\n",
      " 98%|█████████▊| 99/101 [29:11<00:45, 22.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    69 runs   (    0.09 ms per token, 11318.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5112.34 ms /   399 tokens (   12.81 ms per token,    78.05 tokens per second)\n",
      "llama_print_timings:        eval time =   11435.72 ms /    68 runs   (  168.17 ms per token,     5.95 tokens per second)\n",
      "llama_print_timings:       total time =   16669.98 ms /   467 tokens\n",
      " 99%|█████████▉| 100/101 [29:28<00:20, 20.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.52 ms /    60 runs   (    0.08 ms per token, 13286.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3736.08 ms /   228 tokens (   16.39 ms per token,    61.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4187.31 ms /    59 runs   (   70.97 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    8002.00 ms /   287 tokens\n",
      "100%|██████████| 101/101 [29:36<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_splitter_512 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/261 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    65 runs   (    0.08 ms per token, 11909.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10631.16 ms /   254 tokens (   41.85 ms per token,    23.89 tokens per second)\n",
      "llama_print_timings:        eval time =    5145.43 ms /    64 runs   (   80.40 ms per token,    12.44 tokens per second)\n",
      "llama_print_timings:       total time =   15882.94 ms /   318 tokens\n",
      "  0%|          | 1/261 [00:15<1:08:51, 15.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.19 ms /    52 runs   (    0.08 ms per token, 12419.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7575.13 ms /   539 tokens (   14.05 ms per token,    71.15 tokens per second)\n",
      "llama_print_timings:        eval time =    3721.38 ms /    51 runs   (   72.97 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =   11428.87 ms /   590 tokens\n",
      "  1%|          | 2/261 [00:27<57:16, 13.27s/it]  Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.71 ms /    81 runs   (    0.19 ms per token,  5156.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3974.54 ms /   269 tokens (   14.78 ms per token,    67.68 tokens per second)\n",
      "llama_print_timings:        eval time =    6267.09 ms /    80 runs   (   78.34 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:       total time =   10398.67 ms /   349 tokens\n",
      "  1%|          | 3/261 [00:37<51:26, 11.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    69 runs   (    0.08 ms per token, 12916.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8241.73 ms /   565 tokens (   14.59 ms per token,    68.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4935.57 ms /    68 runs   (   72.58 ms per token,    13.78 tokens per second)\n",
      "llama_print_timings:       total time =   13309.67 ms /   633 tokens\n",
      "  2%|▏         | 4/261 [00:51<53:31, 12.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    70 runs   (    0.07 ms per token, 13894.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4323.99 ms /   338 tokens (   12.79 ms per token,    78.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5043.81 ms /    69 runs   (   73.10 ms per token,    13.68 tokens per second)\n",
      "llama_print_timings:       total time =    9505.97 ms /   407 tokens\n",
      "  2%|▏         | 5/261 [01:00<48:43, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    84 runs   (    0.07 ms per token, 13480.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6294.07 ms /   512 tokens (   12.29 ms per token,    81.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5996.12 ms /    83 runs   (   72.24 ms per token,    13.84 tokens per second)\n",
      "llama_print_timings:       total time =   12421.82 ms /   595 tokens\n",
      "  2%|▏         | 6/261 [01:12<49:59, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.72 ms /    63 runs   (    0.07 ms per token, 13336.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.36 ms /   223 tokens (   14.58 ms per token,    68.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4227.03 ms /    62 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    7558.20 ms /   285 tokens\n",
      "  3%|▎         | 7/261 [01:20<43:58, 10.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.84 ms /    64 runs   (    0.08 ms per token, 13209.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4918.22 ms /   400 tokens (   12.30 ms per token,    81.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4890.90 ms /    63 runs   (   77.63 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =    9891.32 ms /   463 tokens\n",
      "  3%|▎         | 8/261 [01:30<43:08, 10.23s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.96 ms /   141 runs   (    0.08 ms per token, 12861.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4109.07 ms /   306 tokens (   13.43 ms per token,    74.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9915.92 ms /   140 runs   (   70.83 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:       total time =   14206.90 ms /   446 tokens\n",
      "  3%|▎         | 9/261 [01:44<48:11, 11.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    68 runs   (    0.07 ms per token, 13473.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3242.05 ms /   205 tokens (   15.81 ms per token,    63.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4733.33 ms /    67 runs   (   70.65 ms per token,    14.15 tokens per second)\n",
      "llama_print_timings:       total time =    8085.81 ms /   272 tokens\n",
      "  4%|▍         | 10/261 [01:52<43:38, 10.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    68 runs   (    0.08 ms per token, 13099.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3251.81 ms /   193 tokens (   16.85 ms per token,    59.35 tokens per second)\n",
      "llama_print_timings:        eval time =    4668.30 ms /    67 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =    8018.88 ms /   260 tokens\n",
      "  4%|▍         | 11/261 [02:00<40:23,  9.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    68 runs   (    0.07 ms per token, 13829.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3169.41 ms /   197 tokens (   16.09 ms per token,    62.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4502.80 ms /    67 runs   (   67.21 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    7757.01 ms /   264 tokens\n",
      "  5%|▍         | 12/261 [02:08<37:47,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    67 runs   (    0.08 ms per token, 12786.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2907.96 ms /   174 tokens (   16.71 ms per token,    59.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4693.95 ms /    66 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    7705.81 ms /   240 tokens\n",
      "  5%|▍         | 13/261 [02:16<35:53,  8.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    65 runs   (    0.08 ms per token, 13249.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3228.57 ms /   196 tokens (   16.47 ms per token,    60.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4378.58 ms /    64 runs   (   68.42 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    7690.11 ms /   260 tokens\n",
      "  5%|▌         | 14/261 [02:23<34:30,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.45 ms /    62 runs   (    0.07 ms per token, 13926.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2982.27 ms /   188 tokens (   15.86 ms per token,    63.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4048.48 ms /    61 runs   (   66.37 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    7108.21 ms /   249 tokens\n",
      "  6%|▌         | 15/261 [02:31<32:48,  8.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.40 ms /    69 runs   (    0.08 ms per token, 12780.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3946.05 ms /   191 tokens (   20.66 ms per token,    48.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5208.14 ms /    68 runs   (   76.59 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =    9245.16 ms /   259 tokens\n",
      "  6%|▌         | 16/261 [02:40<34:12,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    95 runs   (    0.08 ms per token, 12496.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3959.28 ms /   238 tokens (   16.64 ms per token,    60.11 tokens per second)\n",
      "llama_print_timings:        eval time =    8742.84 ms /    94 runs   (   93.01 ms per token,    10.75 tokens per second)\n",
      "llama_print_timings:       total time =   12827.67 ms /   332 tokens\n",
      "  7%|▋         | 17/261 [02:53<39:31,  9.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.51 ms /   131 runs   (    0.08 ms per token, 12470.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3925.85 ms /   225 tokens (   17.45 ms per token,    57.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9114.75 ms /   130 runs   (   70.11 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =   13215.00 ms /   355 tokens\n",
      "  7%|▋         | 18/261 [03:06<43:37, 10.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.57 ms /    58 runs   (    0.08 ms per token, 12680.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3488.18 ms /   234 tokens (   14.91 ms per token,    67.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4293.73 ms /    57 runs   (   75.33 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =    7856.09 ms /   291 tokens\n",
      "  7%|▋         | 19/261 [03:14<39:54,  9.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    80 runs   (    0.08 ms per token, 13251.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3442.11 ms /   221 tokens (   15.58 ms per token,    64.20 tokens per second)\n",
      "llama_print_timings:        eval time =    5498.26 ms /    79 runs   (   69.60 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =    9047.02 ms /   300 tokens\n",
      "  8%|▊         | 20/261 [03:23<38:43,  9.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.47 ms /   155 runs   (    0.07 ms per token, 13511.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3483.27 ms /   225 tokens (   15.48 ms per token,    64.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10694.69 ms /   154 runs   (   69.45 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =   14380.89 ms /   379 tokens\n",
      "  8%|▊         | 21/261 [03:37<44:15, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.32 ms /   110 runs   (    0.08 ms per token, 13217.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4329.61 ms /   325 tokens (   13.32 ms per token,    75.06 tokens per second)\n",
      "llama_print_timings:        eval time =    8062.68 ms /   109 runs   (   73.97 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   12537.27 ms /   434 tokens\n",
      "  8%|▊         | 22/261 [03:50<45:50, 11.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.53 ms /    99 runs   (    0.08 ms per token, 13154.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8185.48 ms /   575 tokens (   14.24 ms per token,    70.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7170.99 ms /    98 runs   (   73.17 ms per token,    13.67 tokens per second)\n",
      "llama_print_timings:       total time =   15529.99 ms /   673 tokens\n",
      "  9%|▉         | 23/261 [04:05<50:26, 12.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.62 ms /   100 runs   (    0.08 ms per token, 13125.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4947.78 ms /   406 tokens (   12.19 ms per token,    82.06 tokens per second)\n",
      "llama_print_timings:        eval time =    7052.95 ms /    99 runs   (   71.24 ms per token,    14.04 tokens per second)\n",
      "llama_print_timings:       total time =   12187.47 ms /   505 tokens\n",
      "  9%|▉         | 24/261 [04:17<49:36, 12.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    68 runs   (    0.08 ms per token, 12722.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8457.57 ms /   588 tokens (   14.38 ms per token,    69.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5332.14 ms /    67 runs   (   79.58 ms per token,    12.57 tokens per second)\n",
      "llama_print_timings:       total time =   13879.12 ms /   655 tokens\n",
      " 10%|▉         | 25/261 [04:31<50:57, 12.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    64 runs   (    0.07 ms per token, 13813.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4208.51 ms /   270 tokens (   15.59 ms per token,    64.16 tokens per second)\n",
      "llama_print_timings:        eval time =    4366.68 ms /    63 runs   (   69.31 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =    8657.59 ms /   333 tokens\n",
      " 10%|▉         | 26/261 [04:40<45:41, 11.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.62 ms /    63 runs   (    0.07 ms per token, 13645.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.49 ms /   198 tokens (   16.68 ms per token,    59.94 tokens per second)\n",
      "llama_print_timings:        eval time =    4204.22 ms /    62 runs   (   67.81 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =    7589.21 ms /   260 tokens\n",
      " 10%|█         | 27/261 [04:48<40:44, 10.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.28 ms /    69 runs   (    0.09 ms per token, 10989.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2976.72 ms /   191 tokens (   15.58 ms per token,    64.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5045.40 ms /    68 runs   (   74.20 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =    8113.48 ms /   259 tokens\n",
      " 11%|█         | 28/261 [04:56<37:50,  9.75s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    73 runs   (    0.08 ms per token, 13089.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3256.15 ms /   198 tokens (   16.45 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =    4993.65 ms /    72 runs   (   69.36 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =    8346.09 ms /   270 tokens\n",
      " 11%|█         | 29/261 [05:04<36:04,  9.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    71 runs   (    0.07 ms per token, 13797.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3186.58 ms /   205 tokens (   15.54 ms per token,    64.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4794.13 ms /    70 runs   (   68.49 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =    8071.14 ms /   275 tokens\n",
      " 11%|█▏        | 30/261 [05:12<34:27,  8.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    65 runs   (    0.08 ms per token, 13067.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3377.01 ms /   201 tokens (   16.80 ms per token,    59.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.98 ms /    64 runs   (   68.92 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =    7872.31 ms /   265 tokens\n",
      " 12%|█▏        | 31/261 [05:20<33:04,  8.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.21 ms /    69 runs   (    0.08 ms per token, 13246.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3308.14 ms /   222 tokens (   14.90 ms per token,    67.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4636.48 ms /    68 runs   (   68.18 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    8033.95 ms /   290 tokens\n",
      " 12%|█▏        | 32/261 [05:28<32:15,  8.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    73 runs   (    0.07 ms per token, 13576.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3187.01 ms /   205 tokens (   15.55 ms per token,    64.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5036.58 ms /    72 runs   (   69.95 ms per token,    14.30 tokens per second)\n",
      "llama_print_timings:       total time =    8317.36 ms /   277 tokens\n",
      " 13%|█▎        | 33/261 [05:36<31:57,  8.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.12 ms /    97 runs   (    0.07 ms per token, 13629.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3169.31 ms /   196 tokens (   16.17 ms per token,    61.84 tokens per second)\n",
      "llama_print_timings:        eval time =    6578.85 ms /    96 runs   (   68.53 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    9871.90 ms /   292 tokens\n",
      " 13%|█▎        | 34/261 [05:46<33:29,  8.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.50 ms /    59 runs   (    0.08 ms per token, 13114.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.02 ms /   192 tokens (   15.68 ms per token,    63.79 tokens per second)\n",
      "llama_print_timings:        eval time =    3978.30 ms /    58 runs   (   68.59 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    7064.70 ms /   250 tokens\n",
      " 13%|█▎        | 35/261 [05:53<31:19,  8.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    69 runs   (    0.08 ms per token, 13188.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2954.97 ms /   185 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4857.67 ms /    68 runs   (   71.44 ms per token,    14.00 tokens per second)\n",
      "llama_print_timings:       total time =    7902.68 ms /   253 tokens\n",
      " 14%|█▍        | 36/261 [06:01<30:43,  8.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    84 runs   (    0.07 ms per token, 13723.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3054.22 ms /   191 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5637.88 ms /    83 runs   (   67.93 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    8800.81 ms /   274 tokens\n",
      " 14%|█▍        | 37/261 [06:10<31:16,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.81 ms /    75 runs   (    0.08 ms per token, 12899.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3241.15 ms /   200 tokens (   16.21 ms per token,    61.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4995.33 ms /    74 runs   (   67.50 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    8332.55 ms /   274 tokens\n",
      " 15%|█▍        | 38/261 [06:18<31:05,  8.36s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    68 runs   (    0.07 ms per token, 13401.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3166.35 ms /   196 tokens (   16.15 ms per token,    61.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4484.63 ms /    67 runs   (   66.93 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =    7736.37 ms /   263 tokens\n",
      " 15%|█▍        | 39/261 [06:26<30:15,  8.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.35 ms /    68 runs   (    0.11 ms per token,  9247.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4052.97 ms /   197 tokens (   20.57 ms per token,    48.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7927.53 ms /    67 runs   (  118.32 ms per token,     8.45 tokens per second)\n",
      "llama_print_timings:       total time =   12082.13 ms /   264 tokens\n",
      " 15%|█▌        | 40/261 [06:38<34:26,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.54 ms /    70 runs   (    0.08 ms per token, 12646.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3273.02 ms /   185 tokens (   17.69 ms per token,    56.52 tokens per second)\n",
      "llama_print_timings:        eval time =    6584.79 ms /    69 runs   (   95.43 ms per token,    10.48 tokens per second)\n",
      "llama_print_timings:       total time =    9958.66 ms /   254 tokens\n",
      " 16%|█▌        | 41/261 [06:48<34:57,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    62 runs   (    0.08 ms per token, 12678.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3923.63 ms /   242 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4745.37 ms /    61 runs   (   77.79 ms per token,    12.85 tokens per second)\n",
      "llama_print_timings:       total time =    8754.01 ms /   303 tokens\n",
      " 16%|█▌        | 42/261 [06:57<33:57,  9.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.51 ms /    71 runs   (    0.08 ms per token, 12888.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7748.72 ms /   538 tokens (   14.40 ms per token,    69.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4855.68 ms /    70 runs   (   69.37 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =   12701.05 ms /   608 tokens\n",
      " 16%|█▋        | 43/261 [07:10<37:30, 10.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    68 runs   (    0.07 ms per token, 13452.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3044.58 ms /   189 tokens (   16.11 ms per token,    62.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4415.92 ms /    67 runs   (   65.91 ms per token,    15.17 tokens per second)\n",
      "llama_print_timings:       total time =    7549.73 ms /   256 tokens\n",
      " 17%|█▋        | 44/261 [07:17<34:19,  9.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.58 ms /    94 runs   (    0.08 ms per token, 12405.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8678.54 ms /   585 tokens (   14.84 ms per token,    67.41 tokens per second)\n",
      "llama_print_timings:        eval time =    8413.40 ms /    93 runs   (   90.47 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =   17239.89 ms /   678 tokens\n",
      " 17%|█▋        | 45/261 [07:34<42:32, 11.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.60 ms /    94 runs   (    0.08 ms per token, 12374.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4009.57 ms /   246 tokens (   16.30 ms per token,    61.35 tokens per second)\n",
      "llama_print_timings:        eval time =    7817.61 ms /    93 runs   (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   11965.44 ms /   339 tokens\n",
      " 18%|█▊        | 46/261 [07:46<42:30, 11.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.08 ms /    56 runs   (    0.07 ms per token, 13718.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5019.11 ms /   394 tokens (   12.74 ms per token,    78.50 tokens per second)\n",
      "llama_print_timings:        eval time =    3832.22 ms /    55 runs   (   69.68 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =    8925.29 ms /   449 tokens\n",
      " 18%|█▊        | 47/261 [07:55<39:10, 10.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.67 ms /    75 runs   (    0.08 ms per token, 13225.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2960.61 ms /   187 tokens (   15.83 ms per token,    63.16 tokens per second)\n",
      "llama_print_timings:        eval time =    5006.40 ms /    74 runs   (   67.65 ms per token,    14.78 tokens per second)\n",
      "llama_print_timings:       total time =    8063.43 ms /   261 tokens\n",
      " 18%|█▊        | 48/261 [08:03<35:56, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.53 ms /    64 runs   (    0.07 ms per token, 14121.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2935.23 ms /   187 tokens (   15.70 ms per token,    63.71 tokens per second)\n",
      "llama_print_timings:        eval time =    4264.65 ms /    63 runs   (   67.69 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =    7280.66 ms /   250 tokens\n",
      " 19%|█▉        | 49/261 [08:11<32:45,  9.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    64 runs   (    0.07 ms per token, 13400.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2945.36 ms /   192 tokens (   15.34 ms per token,    65.19 tokens per second)\n",
      "llama_print_timings:        eval time =    4196.65 ms /    63 runs   (   66.61 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7223.21 ms /   255 tokens\n",
      " 19%|█▉        | 50/261 [08:18<30:26,  8.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      20.50 ms /   275 runs   (    0.07 ms per token, 13413.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2944.09 ms /   192 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_print_timings:        eval time =   19488.19 ms /   274 runs   (   71.12 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =   22809.72 ms /   466 tokens\n",
      " 20%|█▉        | 51/261 [08:41<45:10, 12.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.92 ms /    79 runs   (    0.07 ms per token, 13349.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3317.10 ms /   197 tokens (   16.84 ms per token,    59.39 tokens per second)\n",
      "llama_print_timings:        eval time =    5336.55 ms /    78 runs   (   68.42 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    8756.78 ms /   275 tokens\n",
      " 20%|█▉        | 52/261 [08:49<40:37, 11.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.38 ms /    96 runs   (    0.08 ms per token, 13004.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3203.32 ms /   208 tokens (   15.40 ms per token,    64.93 tokens per second)\n",
      "llama_print_timings:        eval time =    6438.42 ms /    95 runs   (   67.77 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    9763.62 ms /   303 tokens\n",
      " 20%|██        | 53/261 [08:59<38:27, 11.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    61 runs   (    0.08 ms per token, 12482.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3234.08 ms /   196 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    4543.36 ms /    60 runs   (   75.72 ms per token,    13.21 tokens per second)\n",
      "llama_print_timings:       total time =    7856.53 ms /   256 tokens\n",
      " 21%|██        | 54/261 [09:07<34:55, 10.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.42 ms /    60 runs   (    0.09 ms per token, 11074.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3018.79 ms /   189 tokens (   15.97 ms per token,    62.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6476.65 ms /    59 runs   (  109.77 ms per token,     9.11 tokens per second)\n",
      "llama_print_timings:       total time =    9584.51 ms /   248 tokens\n",
      " 21%|██        | 55/261 [09:17<34:12,  9.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.72 ms /   102 runs   (    0.08 ms per token, 13209.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4543.17 ms /   350 tokens (   12.98 ms per token,    77.04 tokens per second)\n",
      "llama_print_timings:        eval time =    7363.59 ms /   101 runs   (   72.91 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =   12043.96 ms /   451 tokens\n",
      " 21%|██▏       | 56/261 [09:29<36:10, 10.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.70 ms /   150 runs   (    0.08 ms per token, 12821.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3580.31 ms /   238 tokens (   15.04 ms per token,    66.47 tokens per second)\n",
      "llama_print_timings:        eval time =   11303.96 ms /   149 runs   (   75.87 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =   15085.55 ms /   387 tokens\n",
      " 22%|██▏       | 57/261 [09:44<40:35, 11.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.70 ms /    92 runs   (    0.07 ms per token, 13731.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10223.72 ms /   596 tokens (   17.15 ms per token,    58.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6712.64 ms /    91 runs   (   73.77 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   17069.02 ms /   687 tokens\n",
      " 22%|██▏       | 58/261 [10:01<45:36, 13.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.26 ms /    59 runs   (    0.07 ms per token, 13849.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3833.09 ms /   272 tokens (   14.09 ms per token,    70.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4021.08 ms /    58 runs   (   69.33 ms per token,    14.42 tokens per second)\n",
      "llama_print_timings:       total time =    7930.65 ms /   330 tokens\n",
      " 23%|██▎       | 59/261 [10:09<39:46, 11.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    58 runs   (    0.08 ms per token, 13299.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5554.52 ms /   460 tokens (   12.08 ms per token,    82.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4055.45 ms /    57 runs   (   71.15 ms per token,    14.06 tokens per second)\n",
      "llama_print_timings:       total time =    9694.16 ms /   517 tokens\n",
      " 23%|██▎       | 60/261 [10:19<37:27, 11.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.72 ms /    76 runs   (    0.08 ms per token, 13279.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2950.53 ms /   178 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5081.89 ms /    75 runs   (   67.76 ms per token,    14.76 tokens per second)\n",
      "llama_print_timings:       total time =    8129.13 ms /   253 tokens\n",
      " 23%|██▎       | 61/261 [10:27<34:13, 10.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.39 ms /    87 runs   (    0.07 ms per token, 13619.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2971.99 ms /   190 tokens (   15.64 ms per token,    63.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5813.53 ms /    86 runs   (   67.60 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    8893.57 ms /   276 tokens\n",
      " 24%|██▍       | 62/261 [10:36<32:41,  9.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    58 runs   (    0.08 ms per token, 12666.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2955.04 ms /   191 tokens (   15.47 ms per token,    64.64 tokens per second)\n",
      "llama_print_timings:        eval time =    3914.25 ms /    57 runs   (   68.67 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    6941.59 ms /   248 tokens\n",
      " 24%|██▍       | 63/261 [10:43<29:38,  8.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    66 runs   (    0.07 ms per token, 13333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3150.23 ms /   185 tokens (   17.03 ms per token,    58.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4475.13 ms /    65 runs   (   68.85 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    7710.30 ms /   250 tokens\n",
      " 25%|██▍       | 64/261 [10:50<28:14,  8.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.83 ms /    60 runs   (    0.08 ms per token, 12412.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2977.97 ms /   184 tokens (   16.18 ms per token,    61.79 tokens per second)\n",
      "llama_print_timings:        eval time =    5099.58 ms /    59 runs   (   86.43 ms per token,    11.57 tokens per second)\n",
      "llama_print_timings:       total time =    8157.34 ms /   243 tokens\n",
      " 25%|██▍       | 65/261 [10:58<27:40,  8.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.90 ms /    52 runs   (    0.08 ms per token, 13329.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3159.40 ms /   186 tokens (   16.99 ms per token,    58.87 tokens per second)\n",
      "llama_print_timings:        eval time =    3478.34 ms /    51 runs   (   68.20 ms per token,    14.66 tokens per second)\n",
      "llama_print_timings:       total time =    6702.54 ms /   237 tokens\n",
      " 25%|██▌       | 66/261 [11:05<25:48,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    74 runs   (    0.07 ms per token, 13831.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3167.79 ms /   196 tokens (   16.16 ms per token,    61.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4901.41 ms /    73 runs   (   67.14 ms per token,    14.89 tokens per second)\n",
      "llama_print_timings:       total time =    8162.90 ms /   269 tokens\n",
      " 26%|██▌       | 67/261 [11:13<25:53,  8.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    72 runs   (    0.07 ms per token, 13483.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2929.41 ms /   188 tokens (   15.58 ms per token,    64.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4710.77 ms /    71 runs   (   66.35 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =    7729.51 ms /   259 tokens\n",
      " 26%|██▌       | 68/261 [11:21<25:29,  7.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    60 runs   (    0.07 ms per token, 13808.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2892.72 ms /   173 tokens (   16.72 ms per token,    59.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3913.36 ms /    59 runs   (   66.33 ms per token,    15.08 tokens per second)\n",
      "llama_print_timings:       total time =    6881.00 ms /   232 tokens\n",
      " 26%|██▋       | 69/261 [11:28<24:21,  7.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    67 runs   (    0.07 ms per token, 13554.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2910.90 ms /   182 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4410.21 ms /    66 runs   (   66.82 ms per token,    14.97 tokens per second)\n",
      "llama_print_timings:       total time =    7403.86 ms /   248 tokens\n",
      " 27%|██▋       | 70/261 [11:35<24:02,  7.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    83 runs   (    0.08 ms per token, 11869.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2919.82 ms /   183 tokens (   15.96 ms per token,    62.68 tokens per second)\n",
      "llama_print_timings:        eval time =    7144.80 ms /    82 runs   (   87.13 ms per token,    11.48 tokens per second)\n",
      "llama_print_timings:       total time =   10180.37 ms /   265 tokens\n",
      " 27%|██▋       | 71/261 [11:45<26:24,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.62 ms /    50 runs   (    0.07 ms per token, 13812.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3078.51 ms /   180 tokens (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3241.37 ms /    49 runs   (   66.15 ms per token,    15.12 tokens per second)\n",
      "llama_print_timings:       total time =    6385.00 ms /   229 tokens\n",
      " 28%|██▊       | 72/261 [11:52<24:25,  7.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.03 ms /    67 runs   (    0.08 ms per token, 13325.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2998.15 ms /   187 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4335.02 ms /    66 runs   (   65.68 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:       total time =    7416.69 ms /   253 tokens\n",
      " 28%|██▊       | 73/261 [11:59<23:59,  7.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    63 runs   (    0.08 ms per token, 13288.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3224.07 ms /   198 tokens (   16.28 ms per token,    61.41 tokens per second)\n",
      "llama_print_timings:        eval time =    4066.30 ms /    62 runs   (   65.59 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    7370.51 ms /   260 tokens\n",
      " 28%|██▊       | 74/261 [12:07<23:35,  7.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.47 ms /    72 runs   (    0.08 ms per token, 13160.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2994.69 ms /   191 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =    4655.77 ms /    71 runs   (   65.57 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    7740.98 ms /   262 tokens\n",
      " 29%|██▊       | 75/261 [12:14<23:37,  7.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.02 ms /    55 runs   (    0.07 ms per token, 13691.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3005.75 ms /   187 tokens (   16.07 ms per token,    62.21 tokens per second)\n",
      "llama_print_timings:        eval time =    3540.37 ms /    54 runs   (   65.56 ms per token,    15.25 tokens per second)\n",
      "llama_print_timings:       total time =    6613.81 ms /   241 tokens\n",
      " 29%|██▉       | 76/261 [12:21<22:34,  7.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.89 ms /    65 runs   (    0.08 ms per token, 13289.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2990.47 ms /   187 tokens (   15.99 ms per token,    62.53 tokens per second)\n",
      "llama_print_timings:        eval time =    4895.93 ms /    64 runs   (   76.50 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:       total time =    7969.42 ms /   251 tokens\n",
      " 30%|██▉       | 77/261 [12:29<23:03,  7.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    78 runs   (    0.08 ms per token, 13209.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3466.39 ms /   197 tokens (   17.60 ms per token,    56.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5064.98 ms /    77 runs   (   65.78 ms per token,    15.20 tokens per second)\n",
      "llama_print_timings:       total time =    8630.85 ms /   274 tokens\n",
      " 30%|██▉       | 78/261 [12:38<23:56,  7.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.97 ms /    81 runs   (    0.07 ms per token, 13570.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3000.61 ms /   189 tokens (   15.88 ms per token,    62.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5249.49 ms /    80 runs   (   65.62 ms per token,    15.24 tokens per second)\n",
      "llama_print_timings:       total time =    8351.43 ms /   269 tokens\n",
      " 30%|███       | 79/261 [12:46<24:16,  8.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    78 runs   (    0.08 ms per token, 12926.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.88 ms /   189 tokens (   15.93 ms per token,    62.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5115.89 ms /    77 runs   (   66.44 ms per token,    15.05 tokens per second)\n",
      "llama_print_timings:       total time =    8224.18 ms /   266 tokens\n",
      " 31%|███       | 80/261 [12:54<24:20,  8.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.57 ms /    63 runs   (    0.07 ms per token, 13776.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3323.29 ms /   187 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4106.81 ms /    62 runs   (   66.24 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    7508.11 ms /   249 tokens\n",
      " 31%|███       | 81/261 [13:02<23:42,  7.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    62 runs   (    0.07 ms per token, 13638.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3026.99 ms /   185 tokens (   16.36 ms per token,    61.12 tokens per second)\n",
      "llama_print_timings:        eval time =    4078.33 ms /    61 runs   (   66.86 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =    7182.68 ms /   246 tokens\n",
      " 31%|███▏      | 82/261 [13:09<22:56,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.91 ms /    50 runs   (    0.08 ms per token, 12787.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3094.47 ms /   178 tokens (   17.38 ms per token,    57.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3628.64 ms /    49 runs   (   74.05 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =    6788.08 ms /   227 tokens\n",
      " 32%|███▏      | 83/261 [13:16<22:00,  7.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.77 ms /    91 runs   (    0.07 ms per token, 13443.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6695.61 ms /   520 tokens (   12.88 ms per token,    77.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6570.00 ms /    90 runs   (   73.00 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =   13383.19 ms /   610 tokens\n",
      " 32%|███▏      | 84/261 [13:29<27:10,  9.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.53 ms /   128 runs   (    0.07 ms per token, 13429.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5304.31 ms /   426 tokens (   12.45 ms per token,    80.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9074.01 ms /   127 runs   (   71.45 ms per token,    14.00 tokens per second)\n",
      "llama_print_timings:       total time =   14542.72 ms /   553 tokens\n",
      " 33%|███▎      | 85/261 [13:44<31:42, 10.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    86 runs   (    0.08 ms per token, 13216.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8219.22 ms /   561 tokens (   14.65 ms per token,    68.25 tokens per second)\n",
      "llama_print_timings:        eval time =    6193.99 ms /    85 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =   14522.39 ms /   646 tokens\n",
      " 33%|███▎      | 86/261 [13:58<34:46, 11.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.77 ms /    66 runs   (    0.07 ms per token, 13833.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4039.18 ms /   289 tokens (   13.98 ms per token,    71.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4518.98 ms /    65 runs   (   69.52 ms per token,    14.38 tokens per second)\n",
      "llama_print_timings:       total time =    8639.30 ms /   354 tokens\n",
      " 33%|███▎      | 87/261 [14:07<31:43, 10.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.97 ms /   131 runs   (    0.08 ms per token, 11946.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5838.42 ms /   483 tokens (   12.09 ms per token,    82.73 tokens per second)\n",
      "llama_print_timings:        eval time =    9374.86 ms /   130 runs   (   72.11 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =   15377.58 ms /   613 tokens\n",
      " 34%|███▎      | 88/261 [14:22<35:23, 12.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    84 runs   (    0.08 ms per token, 13015.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4616.75 ms /   371 tokens (   12.44 ms per token,    80.36 tokens per second)\n",
      "llama_print_timings:        eval time =    6177.16 ms /    83 runs   (   74.42 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:       total time =   10902.26 ms /   454 tokens\n",
      " 34%|███▍      | 89/261 [14:33<34:00, 11.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.09 ms /    95 runs   (    0.09 ms per token, 11737.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8348.77 ms /   606 tokens (   13.78 ms per token,    72.59 tokens per second)\n",
      "llama_print_timings:        eval time =   10342.80 ms /    94 runs   (  110.03 ms per token,     9.09 tokens per second)\n",
      "llama_print_timings:       total time =   18853.94 ms /   700 tokens\n",
      " 34%|███▍      | 90/261 [14:52<39:47, 13.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.61 ms /    63 runs   (    0.10 ms per token,  9528.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2767.79 ms /   143 tokens (   19.36 ms per token,    51.67 tokens per second)\n",
      "llama_print_timings:        eval time =   13509.22 ms /    62 runs   (  217.89 ms per token,     4.59 tokens per second)\n",
      "llama_print_timings:       total time =   16400.01 ms /   205 tokens\n",
      " 35%|███▍      | 91/261 [15:08<41:38, 14.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    85 runs   (    0.07 ms per token, 13485.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8293.32 ms /   569 tokens (   14.58 ms per token,    68.61 tokens per second)\n",
      "llama_print_timings:        eval time =    6212.67 ms /    84 runs   (   73.96 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   14615.07 ms /   653 tokens\n",
      " 35%|███▌      | 92/261 [15:23<41:19, 14.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.69 ms /   124 runs   (    0.08 ms per token, 12799.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.85 ms /   192 tokens (   16.12 ms per token,    62.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8882.12 ms /   123 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =   12138.74 ms /   315 tokens\n",
      " 36%|███▌      | 93/261 [15:35<38:57, 13.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.60 ms /    70 runs   (    0.08 ms per token, 12504.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3208.89 ms /   194 tokens (   16.54 ms per token,    60.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5235.59 ms /    69 runs   (   75.88 ms per token,    13.18 tokens per second)\n",
      "llama_print_timings:       total time =    8534.89 ms /   263 tokens\n",
      " 36%|███▌      | 94/261 [15:44<34:14, 12.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.83 ms /    49 runs   (    0.08 ms per token, 12790.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3239.14 ms /   182 tokens (   17.80 ms per token,    56.19 tokens per second)\n",
      "llama_print_timings:        eval time =    3631.40 ms /    48 runs   (   75.65 ms per token,    13.22 tokens per second)\n",
      "llama_print_timings:       total time =    6934.28 ms /   230 tokens\n",
      " 36%|███▋      | 95/261 [15:51<29:35, 10.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    57 runs   (    0.08 ms per token, 12250.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3159.65 ms /   180 tokens (   17.55 ms per token,    56.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4593.45 ms /    56 runs   (   82.03 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =    7827.63 ms /   236 tokens\n",
      " 37%|███▋      | 96/261 [15:58<27:03,  9.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.07 ms /    66 runs   (    0.08 ms per token, 13030.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3564.94 ms /   199 tokens (   17.91 ms per token,    55.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4485.41 ms /    65 runs   (   69.01 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =    8133.34 ms /   264 tokens\n",
      " 37%|███▋      | 97/261 [16:07<25:29,  9.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    80 runs   (    0.07 ms per token, 13335.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2957.73 ms /   187 tokens (   15.82 ms per token,    63.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5416.39 ms /    79 runs   (   68.56 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    8474.08 ms /   266 tokens\n",
      " 38%|███▊      | 98/261 [16:15<24:38,  9.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    51 runs   (    0.08 ms per token, 13253.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2990.91 ms /   187 tokens (   15.99 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    3441.53 ms /    50 runs   (   68.83 ms per token,    14.53 tokens per second)\n",
      "llama_print_timings:       total time =    6494.40 ms /   237 tokens\n",
      " 38%|███▊      | 99/261 [16:22<22:24,  8.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.77 ms /    46 runs   (    0.08 ms per token, 12188.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2948.21 ms /   187 tokens (   15.77 ms per token,    63.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3627.53 ms /    45 runs   (   80.61 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =    6634.80 ms /   232 tokens\n",
      " 38%|███▊      | 100/261 [16:28<20:56,  7.81s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    57 runs   (    0.09 ms per token, 11143.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3277.28 ms /   183 tokens (   17.91 ms per token,    55.84 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.84 ms /    56 runs   (   81.00 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:       total time =    7892.76 ms /   239 tokens\n",
      " 39%|███▊      | 101/261 [16:36<20:53,  7.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.16 ms /    60 runs   (    0.10 ms per token,  9733.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3408.51 ms /   192 tokens (   17.75 ms per token,    56.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6198.84 ms /    59 runs   (  105.06 ms per token,     9.52 tokens per second)\n",
      "llama_print_timings:       total time =    9695.78 ms /   251 tokens\n",
      " 39%|███▉      | 102/261 [16:46<22:14,  8.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.77 ms /    69 runs   (    0.11 ms per token,  8879.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3492.83 ms /   194 tokens (   18.00 ms per token,    55.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7976.07 ms /    68 runs   (  117.30 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:       total time =   11575.91 ms /   262 tokens\n",
      " 39%|███▉      | 103/261 [16:57<24:37,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    61 runs   (    0.09 ms per token, 10718.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3789.78 ms /   199 tokens (   19.04 ms per token,    52.51 tokens per second)\n",
      "llama_print_timings:        eval time =    6055.88 ms /    60 runs   (  100.93 ms per token,     9.91 tokens per second)\n",
      "llama_print_timings:       total time =    9931.88 ms /   259 tokens\n",
      " 40%|███▉      | 104/261 [17:07<24:56,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    69 runs   (    0.08 ms per token, 11987.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3226.79 ms /   190 tokens (   16.98 ms per token,    58.88 tokens per second)\n",
      "llama_print_timings:        eval time =    5602.34 ms /    68 runs   (   82.39 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    8920.83 ms /   258 tokens\n",
      " 40%|████      | 105/261 [17:16<24:18,  9.35s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.57 ms /    75 runs   (    0.07 ms per token, 13474.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2989.90 ms /   190 tokens (   15.74 ms per token,    63.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4975.89 ms /    74 runs   (   67.24 ms per token,    14.87 tokens per second)\n",
      "llama_print_timings:       total time =    8059.69 ms /   264 tokens\n",
      " 41%|████      | 106/261 [17:24<23:09,  8.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.35 ms /    71 runs   (    0.08 ms per token, 13278.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3236.85 ms /   199 tokens (   16.27 ms per token,    61.48 tokens per second)\n",
      "llama_print_timings:        eval time =    4791.88 ms /    70 runs   (   68.46 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =    8117.23 ms /   269 tokens\n",
      " 41%|████      | 107/261 [17:32<22:21,  8.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /   100 runs   (    0.08 ms per token, 13331.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3367.40 ms /   195 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6709.68 ms /    99 runs   (   67.77 ms per token,    14.75 tokens per second)\n",
      "llama_print_timings:       total time =   10201.19 ms /   294 tokens\n",
      " 41%|████▏     | 108/261 [17:43<23:21,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.38 ms /    59 runs   (    0.07 ms per token, 13482.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3010.82 ms /   190 tokens (   15.85 ms per token,    63.11 tokens per second)\n",
      "llama_print_timings:        eval time =    3927.00 ms /    58 runs   (   67.71 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =    7009.13 ms /   248 tokens\n",
      " 42%|████▏     | 109/261 [17:50<21:34,  8.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.41 ms /    73 runs   (    0.07 ms per token, 13503.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7069.57 ms /   526 tokens (   13.44 ms per token,    74.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5209.11 ms /    72 runs   (   72.35 ms per token,    13.82 tokens per second)\n",
      "llama_print_timings:       total time =   12371.09 ms /   598 tokens\n",
      " 42%|████▏     | 110/261 [18:02<24:20,  9.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    71 runs   (    0.07 ms per token, 13383.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4016.35 ms /   292 tokens (   13.75 ms per token,    72.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4849.58 ms /    70 runs   (   69.28 ms per token,    14.43 tokens per second)\n",
      "llama_print_timings:       total time =    8954.08 ms /   362 tokens\n",
      " 43%|████▎     | 111/261 [18:11<23:38,  9.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.76 ms /    93 runs   (    0.07 ms per token, 13751.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3154.79 ms /   196 tokens (   16.10 ms per token,    62.13 tokens per second)\n",
      "llama_print_timings:        eval time =    6192.40 ms /    92 runs   (   67.31 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =    9459.27 ms /   288 tokens\n",
      " 43%|████▎     | 112/261 [18:20<23:29,  9.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.52 ms /    48 runs   (    0.07 ms per token, 13648.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3218.66 ms /   193 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3112.81 ms /    47 runs   (   66.23 ms per token,    15.10 tokens per second)\n",
      "llama_print_timings:       total time =    6389.06 ms /   240 tokens\n",
      " 43%|████▎     | 113/261 [18:27<21:03,  8.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    52 runs   (    0.08 ms per token, 12351.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2960.85 ms /   192 tokens (   15.42 ms per token,    64.85 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.23 ms /    51 runs   (   88.93 ms per token,    11.25 tokens per second)\n",
      "llama_print_timings:       total time =    7573.56 ms /   243 tokens\n",
      " 44%|████▎     | 114/261 [18:34<20:13,  8.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    59 runs   (    0.08 ms per token, 12452.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3754.95 ms /   193 tokens (   19.46 ms per token,    51.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4043.73 ms /    58 runs   (   69.72 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =    7877.23 ms /   251 tokens\n",
      " 44%|████▍     | 115/261 [18:42<19:48,  8.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    75 runs   (    0.08 ms per token, 13171.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3100.61 ms /   185 tokens (   16.76 ms per token,    59.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4974.69 ms /    74 runs   (   67.23 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    8170.25 ms /   259 tokens\n",
      " 44%|████▍     | 116/261 [18:50<19:41,  8.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.97 ms /    63 runs   (    0.08 ms per token, 12670.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3185.35 ms /   189 tokens (   16.85 ms per token,    59.33 tokens per second)\n",
      "llama_print_timings:        eval time =    4240.80 ms /    62 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    7506.49 ms /   251 tokens\n",
      " 45%|████▍     | 117/261 [18:58<19:06,  7.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    72 runs   (    0.08 ms per token, 13026.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.22 ms /   191 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5332.62 ms /    71 runs   (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =    8724.57 ms /   262 tokens\n",
      " 45%|████▌     | 118/261 [19:07<19:31,  8.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.82 ms /    84 runs   (    0.08 ms per token, 12322.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3089.73 ms /   176 tokens (   17.56 ms per token,    56.96 tokens per second)\n",
      "llama_print_timings:        eval time =    6574.77 ms /    83 runs   (   79.21 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    9774.57 ms /   259 tokens\n",
      " 46%|████▌     | 119/261 [19:17<20:30,  8.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    58 runs   (    0.07 ms per token, 13364.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3340.44 ms /   196 tokens (   17.04 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =    3838.63 ms /    57 runs   (   67.34 ms per token,    14.85 tokens per second)\n",
      "llama_print_timings:       total time =    7252.83 ms /   253 tokens\n",
      " 46%|████▌     | 120/261 [19:24<19:22,  8.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    51 runs   (    0.08 ms per token, 12318.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3096.86 ms /   191 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4108.34 ms /    50 runs   (   82.17 ms per token,    12.17 tokens per second)\n",
      "llama_print_timings:       total time =    7272.65 ms /   241 tokens\n",
      " 46%|████▋     | 121/261 [19:31<18:33,  7.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    58 runs   (    0.07 ms per token, 13504.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3116.21 ms /   190 tokens (   16.40 ms per token,    60.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3868.67 ms /    57 runs   (   67.87 ms per token,    14.73 tokens per second)\n",
      "llama_print_timings:       total time =    7057.97 ms /   247 tokens\n",
      " 47%|████▋     | 122/261 [19:38<17:48,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    62 runs   (    0.08 ms per token, 12304.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3271.08 ms /   193 tokens (   16.95 ms per token,    59.00 tokens per second)\n",
      "llama_print_timings:        eval time =    4675.45 ms /    61 runs   (   76.65 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:       total time =    8028.12 ms /   254 tokens\n",
      " 47%|████▋     | 123/261 [19:46<17:55,  7.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.08 ms /    74 runs   (    0.10 ms per token, 10456.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3444.88 ms /   198 tokens (   17.40 ms per token,    57.48 tokens per second)\n",
      "llama_print_timings:        eval time =    8575.06 ms /    73 runs   (  117.47 ms per token,     8.51 tokens per second)\n",
      "llama_print_timings:       total time =   12154.83 ms /   271 tokens\n",
      " 48%|████▊     | 124/261 [19:58<20:46,  9.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.55 ms /    69 runs   (    0.08 ms per token, 12421.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3463.82 ms /   197 tokens (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:        eval time =    4964.07 ms /    68 runs   (   73.00 ms per token,    13.70 tokens per second)\n",
      "llama_print_timings:       total time =    8516.58 ms /   265 tokens\n",
      " 48%|████▊     | 125/261 [20:07<20:14,  8.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.73 ms /    73 runs   (    0.08 ms per token, 12746.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3316.05 ms /   194 tokens (   17.09 ms per token,    58.50 tokens per second)\n",
      "llama_print_timings:        eval time =    5600.63 ms /    72 runs   (   77.79 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =    9011.85 ms /   266 tokens\n",
      " 48%|████▊     | 126/261 [20:16<20:08,  8.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.29 ms /    56 runs   (    0.08 ms per token, 13068.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3225.04 ms /   202 tokens (   15.97 ms per token,    62.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3941.98 ms /    55 runs   (   71.67 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =    7240.00 ms /   257 tokens\n",
      " 49%|████▊     | 127/261 [20:23<18:51,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.56 ms /    64 runs   (    0.07 ms per token, 14028.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3193.22 ms /   196 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4211.77 ms /    63 runs   (   66.85 ms per token,    14.96 tokens per second)\n",
      "llama_print_timings:       total time =    7482.89 ms /   259 tokens\n",
      " 49%|████▉     | 128/261 [20:31<18:04,  8.15s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.16 ms /    82 runs   (    0.08 ms per token, 13320.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.08 ms /   195 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5620.04 ms /    81 runs   (   69.38 ms per token,    14.41 tokens per second)\n",
      "llama_print_timings:       total time =    8897.83 ms /   276 tokens\n",
      " 49%|████▉     | 129/261 [20:39<18:26,  8.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    59 runs   (    0.08 ms per token, 12159.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3201.65 ms /   190 tokens (   16.85 ms per token,    59.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4945.29 ms /    58 runs   (   85.26 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    8229.56 ms /   248 tokens\n",
      " 50%|████▉     | 130/261 [20:48<18:11,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    71 runs   (    0.08 ms per token, 11807.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2974.02 ms /   180 tokens (   16.52 ms per token,    60.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5550.24 ms /    70 runs   (   79.29 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:       total time =    8617.09 ms /   250 tokens\n",
      " 50%|█████     | 131/261 [20:56<18:14,  8.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.36 ms /    78 runs   (    0.09 ms per token, 10597.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4069.31 ms /   204 tokens (   19.95 ms per token,    50.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8142.18 ms /    77 runs   (  105.74 ms per token,     9.46 tokens per second)\n",
      "llama_print_timings:       total time =   12342.45 ms /   281 tokens\n",
      " 51%|█████     | 132/261 [21:09<20:38,  9.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    85 runs   (    0.07 ms per token, 13558.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3210.48 ms /   193 tokens (   16.63 ms per token,    60.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5859.90 ms /    84 runs   (   69.76 ms per token,    14.33 tokens per second)\n",
      "llama_print_timings:       total time =    9176.46 ms /   277 tokens\n",
      " 51%|█████     | 133/261 [21:18<20:12,  9.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    68 runs   (    0.08 ms per token, 12905.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3189.17 ms /   202 tokens (   15.79 ms per token,    63.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4989.03 ms /    67 runs   (   74.46 ms per token,    13.43 tokens per second)\n",
      "llama_print_timings:       total time =    8262.82 ms /   269 tokens\n",
      " 51%|█████▏    | 134/261 [21:26<19:17,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.82 ms /    77 runs   (    0.08 ms per token, 13232.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2962.83 ms /   186 tokens (   15.93 ms per token,    62.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5215.62 ms /    76 runs   (   68.63 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =    8272.26 ms /   262 tokens\n",
      " 52%|█████▏    | 135/261 [21:34<18:36,  8.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    72 runs   (    0.07 ms per token, 13875.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3172.84 ms /   198 tokens (   16.02 ms per token,    62.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4774.68 ms /    71 runs   (   67.25 ms per token,    14.87 tokens per second)\n",
      "llama_print_timings:       total time =    8033.20 ms /   269 tokens\n",
      " 52%|█████▏    | 136/261 [21:42<17:56,  8.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.36 ms /    72 runs   (    0.07 ms per token, 13430.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3172.77 ms /   204 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4817.56 ms /    71 runs   (   67.85 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =    8077.05 ms /   275 tokens\n",
      " 52%|█████▏    | 137/261 [21:51<17:28,  8.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    74 runs   (    0.08 ms per token, 12774.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3235.19 ms /   201 tokens (   16.10 ms per token,    62.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5391.93 ms /    73 runs   (   73.86 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:       total time =    8721.30 ms /   274 tokens\n",
      " 53%|█████▎    | 138/261 [21:59<17:29,  8.54s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.67 ms /   157 runs   (    0.08 ms per token, 12390.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3526.27 ms /   197 tokens (   17.90 ms per token,    55.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11894.97 ms /   156 runs   (   76.25 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =   15633.79 ms /   353 tokens\n",
      " 53%|█████▎    | 139/261 [22:15<21:41, 10.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.66 ms /    97 runs   (    0.08 ms per token, 12669.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3240.64 ms /   193 tokens (   16.79 ms per token,    59.56 tokens per second)\n",
      "llama_print_timings:        eval time =    7065.49 ms /    96 runs   (   73.60 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   10433.05 ms /   289 tokens\n",
      " 54%|█████▎    | 140/261 [22:25<21:22, 10.60s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.75 ms /    58 runs   (    0.08 ms per token, 12218.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3207.56 ms /   196 tokens (   16.37 ms per token,    61.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4347.15 ms /    57 runs   (   76.27 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:       total time =    7632.97 ms /   253 tokens\n",
      " 54%|█████▍    | 141/261 [22:33<19:25,  9.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    52 runs   (    0.08 ms per token, 11956.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3347.73 ms /   201 tokens (   16.66 ms per token,    60.04 tokens per second)\n",
      "llama_print_timings:        eval time =    4573.40 ms /    51 runs   (   89.67 ms per token,    11.15 tokens per second)\n",
      "llama_print_timings:       total time =    7992.38 ms /   252 tokens\n",
      " 54%|█████▍    | 142/261 [22:41<18:14,  9.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    62 runs   (    0.08 ms per token, 12548.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3681.87 ms /   197 tokens (   18.69 ms per token,    53.51 tokens per second)\n",
      "llama_print_timings:        eval time =    4829.19 ms /    61 runs   (   79.17 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =    8592.79 ms /   258 tokens\n",
      " 55%|█████▍    | 143/261 [22:50<17:44,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.71 ms /    54 runs   (    0.09 ms per token, 11457.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3285.25 ms /   190 tokens (   17.29 ms per token,    57.83 tokens per second)\n",
      "llama_print_timings:        eval time =    4481.73 ms /    53 runs   (   84.56 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    7841.26 ms /   243 tokens\n",
      " 55%|█████▌    | 144/261 [22:57<16:54,  8.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.65 ms /    60 runs   (    0.08 ms per token, 12911.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3368.95 ms /   190 tokens (   17.73 ms per token,    56.40 tokens per second)\n",
      "llama_print_timings:        eval time =    5341.18 ms /    59 runs   (   90.53 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    8789.57 ms /   249 tokens\n",
      " 56%|█████▌    | 145/261 [23:06<16:49,  8.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.20 ms /    68 runs   (    0.08 ms per token, 13079.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2991.62 ms /   189 tokens (   15.83 ms per token,    63.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4976.39 ms /    67 runs   (   74.27 ms per token,    13.46 tokens per second)\n",
      "llama_print_timings:       total time =    8063.11 ms /   256 tokens\n",
      " 56%|█████▌    | 146/261 [23:14<16:19,  8.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    72 runs   (    0.08 ms per token, 13196.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3528.01 ms /   204 tokens (   17.29 ms per token,    57.82 tokens per second)\n",
      "llama_print_timings:        eval time =    5040.13 ms /    71 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    8657.80 ms /   275 tokens\n",
      " 56%|█████▋    | 147/261 [23:23<16:15,  8.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.30 ms /    72 runs   (    0.07 ms per token, 13587.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2954.75 ms /   190 tokens (   15.55 ms per token,    64.30 tokens per second)\n",
      "llama_print_timings:        eval time =    4795.10 ms /    71 runs   (   67.54 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    7838.53 ms /   261 tokens\n",
      " 57%|█████▋    | 148/261 [23:31<15:42,  8.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.75 ms /   106 runs   (    0.07 ms per token, 13672.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2931.22 ms /   188 tokens (   15.59 ms per token,    64.14 tokens per second)\n",
      "llama_print_timings:        eval time =    7608.36 ms /   105 runs   (   72.46 ms per token,    13.80 tokens per second)\n",
      "llama_print_timings:       total time =   10670.52 ms /   293 tokens\n",
      " 57%|█████▋    | 149/261 [23:41<16:52,  9.04s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.42 ms /    61 runs   (    0.07 ms per token, 13816.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3312.52 ms /   193 tokens (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:        eval time =    4313.56 ms /    60 runs   (   71.89 ms per token,    13.91 tokens per second)\n",
      "llama_print_timings:       total time =    7706.62 ms /   253 tokens\n",
      " 57%|█████▋    | 150/261 [23:49<15:59,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.22 ms /    73 runs   (    0.07 ms per token, 13992.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.86 ms /   191 tokens (   16.21 ms per token,    61.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4852.53 ms /    72 runs   (   67.40 ms per token,    14.84 tokens per second)\n",
      "llama_print_timings:       total time =    8038.32 ms /   263 tokens\n",
      " 58%|█████▊    | 151/261 [23:57<15:30,  8.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    57 runs   (    0.09 ms per token, 10586.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3419.24 ms /   185 tokens (   18.48 ms per token,    54.11 tokens per second)\n",
      "llama_print_timings:        eval time =    7764.24 ms /    56 runs   (  138.65 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:       total time =   11271.02 ms /   241 tokens\n",
      " 58%|█████▊    | 152/261 [24:08<16:54,  9.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    68 runs   (    0.08 ms per token, 12442.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3768.54 ms /   193 tokens (   19.53 ms per token,    51.21 tokens per second)\n",
      "llama_print_timings:        eval time =    5825.70 ms /    67 runs   (   86.95 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =    9684.11 ms /   260 tokens\n",
      " 59%|█████▊    | 153/261 [24:18<16:57,  9.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.80 ms /   100 runs   (    0.08 ms per token, 12815.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3293.65 ms /   202 tokens (   16.31 ms per token,    61.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7374.70 ms /    99 runs   (   74.49 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   10798.79 ms /   301 tokens\n",
      " 59%|█████▉    | 154/261 [24:29<17:30,  9.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    72 runs   (    0.08 ms per token, 12841.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3032.42 ms /   191 tokens (   15.88 ms per token,    62.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5448.99 ms /    71 runs   (   76.75 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =    8576.79 ms /   262 tokens\n",
      " 59%|█████▉    | 155/261 [24:37<16:41,  9.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.25 ms /    57 runs   (    0.07 ms per token, 13408.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.67 ms /   198 tokens (   16.66 ms per token,    60.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3975.23 ms /    56 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    7347.05 ms /   254 tokens\n",
      " 60%|█████▉    | 156/261 [24:45<15:25,  8.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.29 ms /    63 runs   (    0.08 ms per token, 11904.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3224.87 ms /   185 tokens (   17.43 ms per token,    57.37 tokens per second)\n",
      "llama_print_timings:        eval time =    4911.93 ms /    62 runs   (   79.22 ms per token,    12.62 tokens per second)\n",
      "llama_print_timings:       total time =    8221.62 ms /   247 tokens\n",
      " 60%|██████    | 157/261 [24:53<14:58,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.86 ms /    51 runs   (    0.08 ms per token, 13202.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3146.43 ms /   191 tokens (   16.47 ms per token,    60.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3321.75 ms /    50 runs   (   66.44 ms per token,    15.05 tokens per second)\n",
      "llama_print_timings:       total time =    6533.44 ms /   241 tokens\n",
      " 61%|██████    | 158/261 [25:00<13:45,  8.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.33 ms /    85 runs   (    0.07 ms per token, 13423.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3352.28 ms /   196 tokens (   17.10 ms per token,    58.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5622.42 ms /    84 runs   (   66.93 ms per token,    14.94 tokens per second)\n",
      "llama_print_timings:       total time =    9084.17 ms /   280 tokens\n",
      " 61%|██████    | 159/261 [25:09<14:10,  8.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    70 runs   (    0.07 ms per token, 13765.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3288.13 ms /   203 tokens (   16.20 ms per token,    61.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4535.24 ms /    69 runs   (   65.73 ms per token,    15.21 tokens per second)\n",
      "llama_print_timings:       total time =    7910.93 ms /   272 tokens\n",
      " 61%|██████▏   | 160/261 [25:17<13:48,  8.21s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.87 ms /    51 runs   (    0.08 ms per token, 13181.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3007.83 ms /   192 tokens (   15.67 ms per token,    63.83 tokens per second)\n",
      "llama_print_timings:        eval time =    3273.49 ms /    50 runs   (   65.47 ms per token,    15.27 tokens per second)\n",
      "llama_print_timings:       total time =    6344.66 ms /   242 tokens\n",
      " 62%|██████▏   | 161/261 [25:23<12:44,  7.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.84 ms /   168 runs   (    0.08 ms per token, 13085.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3259.47 ms /   198 tokens (   16.46 ms per token,    60.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11143.29 ms /   167 runs   (   66.73 ms per token,    14.99 tokens per second)\n",
      "llama_print_timings:       total time =   14617.62 ms /   365 tokens\n",
      " 62%|██████▏   | 162/261 [25:38<16:04,  9.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    75 runs   (    0.08 ms per token, 12244.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3202.06 ms /   184 tokens (   17.40 ms per token,    57.46 tokens per second)\n",
      "llama_print_timings:        eval time =    5328.82 ms /    74 runs   (   72.01 ms per token,    13.89 tokens per second)\n",
      "llama_print_timings:       total time =    8630.73 ms /   258 tokens\n",
      " 62%|██████▏   | 163/261 [25:46<15:22,  9.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.22 ms /    68 runs   (    0.08 ms per token, 13031.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3538.78 ms /   194 tokens (   18.24 ms per token,    54.82 tokens per second)\n",
      "llama_print_timings:        eval time =    4709.15 ms /    67 runs   (   70.29 ms per token,    14.23 tokens per second)\n",
      "llama_print_timings:       total time =    8340.31 ms /   261 tokens\n",
      " 63%|██████▎   | 164/261 [25:55<14:41,  9.09s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.02 ms /    70 runs   (    0.07 ms per token, 13935.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3175.78 ms /   198 tokens (   16.04 ms per token,    62.35 tokens per second)\n",
      "llama_print_timings:        eval time =    5176.07 ms /    69 runs   (   75.02 ms per token,    13.33 tokens per second)\n",
      "llama_print_timings:       total time =    8439.25 ms /   267 tokens\n",
      " 63%|██████▎   | 165/261 [26:03<14:13,  8.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.51 ms /    78 runs   (    0.07 ms per token, 14163.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3236.38 ms /   193 tokens (   16.77 ms per token,    59.63 tokens per second)\n",
      "llama_print_timings:        eval time =    5199.53 ms /    77 runs   (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    8531.65 ms /   270 tokens\n",
      " 64%|██████▎   | 166/261 [26:12<13:54,  8.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.07 ms /    76 runs   (    0.08 ms per token, 12522.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2943.13 ms /   184 tokens (   16.00 ms per token,    62.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5039.74 ms /    75 runs   (   67.20 ms per token,    14.88 tokens per second)\n",
      "llama_print_timings:       total time =    8078.13 ms /   259 tokens\n",
      " 64%|██████▍   | 167/261 [26:20<13:26,  8.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /   140 runs   (    0.08 ms per token, 12821.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3262.28 ms /   198 tokens (   16.48 ms per token,    60.69 tokens per second)\n",
      "llama_print_timings:        eval time =    9564.14 ms /   139 runs   (   68.81 ms per token,    14.53 tokens per second)\n",
      "llama_print_timings:       total time =   13006.52 ms /   337 tokens\n",
      " 64%|██████▍   | 168/261 [26:33<15:21,  9.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.09 ms /    84 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3316.79 ms /   201 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5604.65 ms /    83 runs   (   67.53 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    9027.80 ms /   284 tokens\n",
      " 65%|██████▍   | 169/261 [26:42<14:47,  9.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.59 ms /    68 runs   (    0.08 ms per token, 12153.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4272.47 ms /   193 tokens (   22.14 ms per token,    45.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5467.36 ms /    67 runs   (   81.60 ms per token,    12.25 tokens per second)\n",
      "llama_print_timings:       total time =    9829.59 ms /   260 tokens\n",
      " 65%|██████▌   | 170/261 [26:52<14:43,  9.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    67 runs   (    0.07 ms per token, 13378.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3273.53 ms /   193 tokens (   16.96 ms per token,    58.96 tokens per second)\n",
      "llama_print_timings:        eval time =    4517.24 ms /    66 runs   (   68.44 ms per token,    14.61 tokens per second)\n",
      "llama_print_timings:       total time =    7878.62 ms /   259 tokens\n",
      " 66%|██████▌   | 171/261 [26:59<13:44,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.78 ms /    64 runs   (    0.07 ms per token, 13386.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3200.67 ms /   194 tokens (   16.50 ms per token,    60.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4223.64 ms /    63 runs   (   67.04 ms per token,    14.92 tokens per second)\n",
      "llama_print_timings:       total time =    7503.55 ms /   257 tokens\n",
      " 66%|██████▌   | 172/261 [27:07<12:50,  8.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    69 runs   (    0.08 ms per token, 12839.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3318.14 ms /   199 tokens (   16.67 ms per token,    59.97 tokens per second)\n",
      "llama_print_timings:        eval time =    4954.85 ms /    68 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =    8361.56 ms /   267 tokens\n",
      " 66%|██████▋   | 173/261 [27:15<12:34,  8.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.29 ms /   141 runs   (    0.08 ms per token, 12483.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3000.85 ms /   192 tokens (   15.63 ms per token,    63.98 tokens per second)\n",
      "llama_print_timings:        eval time =    9610.98 ms /   140 runs   (   68.65 ms per token,    14.57 tokens per second)\n",
      "llama_print_timings:       total time =   12795.79 ms /   332 tokens\n",
      " 67%|██████▋   | 174/261 [27:28<14:16,  9.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    60 runs   (    0.08 ms per token, 12376.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2978.93 ms /   190 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_print_timings:        eval time =    5405.87 ms /    59 runs   (   91.62 ms per token,    10.91 tokens per second)\n",
      "llama_print_timings:       total time =    8466.56 ms /   249 tokens\n",
      " 67%|██████▋   | 175/261 [27:37<13:31,  9.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    77 runs   (    0.08 ms per token, 12618.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3429.40 ms /   194 tokens (   17.68 ms per token,    56.57 tokens per second)\n",
      "llama_print_timings:        eval time =    5599.42 ms /    76 runs   (   73.68 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =    9134.10 ms /   270 tokens\n",
      " 67%|██████▋   | 176/261 [27:46<13:14,  9.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    54 runs   (    0.09 ms per token, 10627.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3495.79 ms /   192 tokens (   18.21 ms per token,    54.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5070.98 ms /    53 runs   (   95.68 ms per token,    10.45 tokens per second)\n",
      "llama_print_timings:       total time =    8655.89 ms /   245 tokens\n",
      " 68%|██████▊   | 177/261 [27:54<12:47,  9.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /    63 runs   (    0.08 ms per token, 13105.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3388.38 ms /   201 tokens (   16.86 ms per token,    59.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4603.73 ms /    62 runs   (   74.25 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =    8073.51 ms /   263 tokens\n",
      " 68%|██████▊   | 178/261 [28:02<12:12,  8.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    51 runs   (    0.08 ms per token, 12384.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2373.30 ms /   124 tokens (   19.14 ms per token,    52.25 tokens per second)\n",
      "llama_print_timings:        eval time =    3392.01 ms /    50 runs   (   67.84 ms per token,    14.74 tokens per second)\n",
      "llama_print_timings:       total time =    5850.33 ms /   174 tokens\n",
      " 69%|██████▊   | 179/261 [28:08<10:50,  7.93s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.06 ms /    82 runs   (    0.07 ms per token, 13540.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5725.02 ms /   469 tokens (   12.21 ms per token,    81.92 tokens per second)\n",
      "llama_print_timings:        eval time =    5734.69 ms /    81 runs   (   70.80 ms per token,    14.12 tokens per second)\n",
      "llama_print_timings:       total time =   11571.14 ms /   550 tokens\n",
      " 69%|██████▉   | 180/261 [28:20<12:10,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.79 ms /    81 runs   (    0.07 ms per token, 13982.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3027.55 ms /   187 tokens (   16.19 ms per token,    61.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5396.90 ms /    80 runs   (   67.46 ms per token,    14.82 tokens per second)\n",
      "llama_print_timings:       total time =    8526.67 ms /   267 tokens\n",
      " 69%|██████▉   | 181/261 [28:28<11:50,  8.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.63 ms /    65 runs   (    0.07 ms per token, 14051.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.12 ms /   187 tokens (   15.65 ms per token,    63.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4262.53 ms /    64 runs   (   66.60 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7268.78 ms /   251 tokens\n",
      " 70%|██████▉   | 182/261 [28:36<11:03,  8.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    82 runs   (    0.07 ms per token, 13582.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3335.45 ms /   200 tokens (   16.68 ms per token,    59.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5555.10 ms /    81 runs   (   68.58 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    8992.59 ms /   281 tokens\n",
      " 70%|███████   | 183/261 [28:45<11:08,  8.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    71 runs   (    0.07 ms per token, 13682.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3222.89 ms /   208 tokens (   15.49 ms per token,    64.54 tokens per second)\n",
      "llama_print_timings:        eval time =    4808.29 ms /    70 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    8121.29 ms /   278 tokens\n",
      " 70%|███████   | 184/261 [28:53<10:49,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    89 runs   (    0.07 ms per token, 13660.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.93 ms /   200 tokens (   16.52 ms per token,    60.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6019.23 ms /    88 runs   (   68.40 ms per token,    14.62 tokens per second)\n",
      "llama_print_timings:       total time =    9436.25 ms /   288 tokens\n",
      " 71%|███████   | 185/261 [29:02<11:04,  8.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.96 ms /    65 runs   (    0.08 ms per token, 13091.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3180.40 ms /   212 tokens (   15.00 ms per token,    66.66 tokens per second)\n",
      "llama_print_timings:        eval time =    4354.76 ms /    64 runs   (   68.04 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =    7616.37 ms /   276 tokens\n",
      " 71%|███████▏  | 186/261 [29:10<10:30,  8.40s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.43 ms /    60 runs   (    0.07 ms per token, 13544.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3258.36 ms /   196 tokens (   16.62 ms per token,    60.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4056.44 ms /    59 runs   (   68.75 ms per token,    14.54 tokens per second)\n",
      "llama_print_timings:       total time =    7390.30 ms /   255 tokens\n",
      " 72%|███████▏  | 187/261 [29:17<09:59,  8.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    67 runs   (    0.09 ms per token, 10780.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3303.17 ms /   190 tokens (   17.39 ms per token,    57.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5019.44 ms /    66 runs   (   76.05 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:       total time =    8419.12 ms /   256 tokens\n",
      " 72%|███████▏  | 188/261 [29:26<09:58,  8.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /    64 runs   (    0.08 ms per token, 11911.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3588.28 ms /   198 tokens (   18.12 ms per token,    55.18 tokens per second)\n",
      "llama_print_timings:        eval time =    4747.46 ms /    63 runs   (   75.36 ms per token,    13.27 tokens per second)\n",
      "llama_print_timings:       total time =    8420.98 ms /   261 tokens\n",
      " 72%|███████▏  | 189/261 [29:34<09:55,  8.27s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       2.95 ms /    41 runs   (    0.07 ms per token, 13907.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3184.12 ms /   200 tokens (   15.92 ms per token,    62.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2722.53 ms /    40 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    5959.17 ms /   240 tokens\n",
      " 73%|███████▎  | 190/261 [29:40<08:57,  7.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.38 ms /    57 runs   (    0.08 ms per token, 13001.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3226.00 ms /   198 tokens (   16.29 ms per token,    61.38 tokens per second)\n",
      "llama_print_timings:        eval time =    4654.29 ms /    56 runs   (   83.11 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =    7955.80 ms /   254 tokens\n",
      " 73%|███████▎  | 191/261 [29:48<08:58,  7.69s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /    66 runs   (    0.08 ms per token, 12607.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3216.83 ms /   189 tokens (   17.02 ms per token,    58.75 tokens per second)\n",
      "llama_print_timings:        eval time =    4720.36 ms /    65 runs   (   72.62 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    8026.08 ms /   254 tokens\n",
      " 74%|███████▎  | 192/261 [29:56<08:57,  7.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.23 ms /    71 runs   (    0.07 ms per token, 13583.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3223.17 ms /   196 tokens (   16.44 ms per token,    60.81 tokens per second)\n",
      "llama_print_timings:        eval time =    5036.25 ms /    70 runs   (   71.95 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =    8353.11 ms /   266 tokens\n",
      " 74%|███████▍  | 193/261 [30:04<09:01,  7.96s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.85 ms /    46 runs   (    0.08 ms per token, 11960.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2952.78 ms /   191 tokens (   15.46 ms per token,    64.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3279.14 ms /    45 runs   (   72.87 ms per token,    13.72 tokens per second)\n",
      "llama_print_timings:       total time =    6292.76 ms /   236 tokens\n",
      " 74%|███████▍  | 194/261 [30:11<08:20,  7.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.09 ms /    94 runs   (    0.08 ms per token, 13252.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3304.44 ms /   194 tokens (   17.03 ms per token,    58.71 tokens per second)\n",
      "llama_print_timings:        eval time =    6551.38 ms /    93 runs   (   70.44 ms per token,    14.20 tokens per second)\n",
      "llama_print_timings:       total time =    9977.38 ms /   287 tokens\n",
      " 75%|███████▍  | 195/261 [30:21<09:02,  8.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.25 ms /    84 runs   (    0.07 ms per token, 13442.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3022.11 ms /   189 tokens (   15.99 ms per token,    62.54 tokens per second)\n",
      "llama_print_timings:        eval time =    5812.02 ms /    83 runs   (   70.02 ms per token,    14.28 tokens per second)\n",
      "llama_print_timings:       total time =    8940.99 ms /   272 tokens\n",
      " 75%|███████▌  | 196/261 [30:30<09:08,  8.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.95 ms /    53 runs   (    0.07 ms per token, 13407.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2701.47 ms /   152 tokens (   17.77 ms per token,    56.27 tokens per second)\n",
      "llama_print_timings:        eval time =    3541.85 ms /    52 runs   (   68.11 ms per token,    14.68 tokens per second)\n",
      "llama_print_timings:       total time =    6310.35 ms /   204 tokens\n",
      " 75%|███████▌  | 197/261 [30:36<08:19,  7.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.31 ms /    71 runs   (    0.09 ms per token, 11259.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3781.91 ms /   266 tokens (   14.22 ms per token,    70.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5655.17 ms /    70 runs   (   80.79 ms per token,    12.38 tokens per second)\n",
      "llama_print_timings:       total time =    9539.32 ms /   336 tokens\n",
      " 76%|███████▌  | 198/261 [30:45<08:44,  8.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.39 ms /   134 runs   (    0.09 ms per token, 11763.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3459.18 ms /   203 tokens (   17.04 ms per token,    58.68 tokens per second)\n",
      "llama_print_timings:        eval time =   13104.31 ms /   133 runs   (   98.53 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =   16748.76 ms /   336 tokens\n",
      " 76%|███████▌  | 199/261 [31:02<11:12, 10.85s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.55 ms /   171 runs   (    0.08 ms per token, 12620.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4112.25 ms /   210 tokens (   19.58 ms per token,    51.07 tokens per second)\n",
      "llama_print_timings:        eval time =   12933.35 ms /   170 runs   (   76.08 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:       total time =   17278.11 ms /   380 tokens\n",
      " 77%|███████▋  | 200/261 [31:20<12:59, 12.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /    91 runs   (    0.09 ms per token, 11656.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3535.66 ms /   233 tokens (   15.17 ms per token,    65.90 tokens per second)\n",
      "llama_print_timings:        eval time =    6905.32 ms /    90 runs   (   76.73 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:       total time =   10564.91 ms /   323 tokens\n",
      " 77%|███████▋  | 201/261 [31:30<12:07, 12.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    79 runs   (    0.11 ms per token,  9364.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4459.49 ms /   235 tokens (   18.98 ms per token,    52.70 tokens per second)\n",
      "llama_print_timings:        eval time =    8446.66 ms /    78 runs   (  108.29 ms per token,     9.23 tokens per second)\n",
      "llama_print_timings:       total time =   13023.63 ms /   313 tokens\n",
      " 77%|███████▋  | 202/261 [31:43<12:11, 12.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.82 ms /    66 runs   (    0.07 ms per token, 13704.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5424.11 ms /   200 tokens (   27.12 ms per token,    36.87 tokens per second)\n",
      "llama_print_timings:        eval time =    5664.81 ms /    65 runs   (   87.15 ms per token,    11.47 tokens per second)\n",
      "llama_print_timings:       total time =   11178.22 ms /   265 tokens\n",
      " 78%|███████▊  | 203/261 [31:54<11:37, 12.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    81 runs   (    0.08 ms per token, 12490.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3421.03 ms /   194 tokens (   17.63 ms per token,    56.71 tokens per second)\n",
      "llama_print_timings:        eval time =    7011.94 ms /    80 runs   (   87.65 ms per token,    11.41 tokens per second)\n",
      "llama_print_timings:       total time =   10547.12 ms /   274 tokens\n",
      " 78%|███████▊  | 204/261 [32:05<11:00, 11.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.14 ms /    82 runs   (    0.07 ms per token, 13363.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3366.81 ms /   193 tokens (   17.44 ms per token,    57.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5750.02 ms /    81 runs   (   70.99 ms per token,    14.09 tokens per second)\n",
      "llama_print_timings:       total time =    9232.52 ms /   274 tokens\n",
      " 79%|███████▊  | 205/261 [32:14<10:09, 10.89s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.30 ms /    85 runs   (    0.07 ms per token, 13492.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3458.45 ms /   204 tokens (   16.95 ms per token,    58.99 tokens per second)\n",
      "llama_print_timings:        eval time =    6065.78 ms /    84 runs   (   72.21 ms per token,    13.85 tokens per second)\n",
      "llama_print_timings:       total time =    9642.93 ms /   288 tokens\n",
      " 79%|███████▉  | 206/261 [32:24<09:38, 10.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      16.78 ms /   210 runs   (    0.08 ms per token, 12516.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4006.66 ms /   227 tokens (   17.65 ms per token,    56.66 tokens per second)\n",
      "llama_print_timings:        eval time =   15515.04 ms /   209 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   19836.37 ms /   436 tokens\n",
      " 79%|███████▉  | 207/261 [32:44<11:58, 13.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.08 ms /   137 runs   (    0.08 ms per token, 12361.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3295.34 ms /   221 tokens (   14.91 ms per token,    67.06 tokens per second)\n",
      "llama_print_timings:        eval time =   11429.36 ms /   136 runs   (   84.04 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =   14927.99 ms /   357 tokens\n",
      " 80%|███████▉  | 208/261 [32:59<12:11, 13.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.31 ms /    59 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3015.04 ms /   191 tokens (   15.79 ms per token,    63.35 tokens per second)\n",
      "llama_print_timings:        eval time =    3940.80 ms /    58 runs   (   67.94 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    7036.83 ms /   249 tokens\n",
      " 80%|████████  | 209/261 [33:06<10:12, 11.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    80 runs   (    0.07 ms per token, 13538.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3190.69 ms /   193 tokens (   16.53 ms per token,    60.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6507.59 ms /    79 runs   (   82.37 ms per token,    12.14 tokens per second)\n",
      "llama_print_timings:       total time =    9813.43 ms /   272 tokens\n",
      " 80%|████████  | 210/261 [33:15<09:30, 11.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.10 ms /    65 runs   (    0.08 ms per token, 12747.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3350.39 ms /   201 tokens (   16.67 ms per token,    59.99 tokens per second)\n",
      "llama_print_timings:        eval time =    5793.10 ms /    64 runs   (   90.52 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =    9239.76 ms /   265 tokens\n",
      " 81%|████████  | 211/261 [33:25<08:50, 10.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.89 ms /    53 runs   (    0.07 ms per token, 13621.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3185.87 ms /   187 tokens (   17.04 ms per token,    58.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3855.06 ms /    52 runs   (   74.14 ms per token,    13.49 tokens per second)\n",
      "llama_print_timings:       total time =    7115.15 ms /   239 tokens\n",
      " 81%|████████  | 212/261 [33:32<07:48,  9.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.98 ms /    81 runs   (    0.07 ms per token, 13554.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2970.02 ms /   188 tokens (   15.80 ms per token,    63.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6024.85 ms /    80 runs   (   75.31 ms per token,    13.28 tokens per second)\n",
      "llama_print_timings:       total time =    9106.79 ms /   268 tokens\n",
      " 82%|████████▏ | 213/261 [33:41<07:32,  9.43s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    71 runs   (    0.07 ms per token, 13477.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3286.08 ms /   205 tokens (   16.03 ms per token,    62.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5756.99 ms /    70 runs   (   82.24 ms per token,    12.16 tokens per second)\n",
      "llama_print_timings:       total time =    9144.60 ms /   275 tokens\n",
      " 82%|████████▏ | 214/261 [33:50<07:19,  9.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.74 ms /    61 runs   (    0.08 ms per token, 12861.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3349.74 ms /   194 tokens (   17.27 ms per token,    57.91 tokens per second)\n",
      "llama_print_timings:        eval time =    5294.32 ms /    60 runs   (   88.24 ms per token,    11.33 tokens per second)\n",
      "llama_print_timings:       total time =    8730.82 ms /   254 tokens\n",
      " 82%|████████▏ | 215/261 [33:59<07:01,  9.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.00 ms /    81 runs   (    0.07 ms per token, 13511.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3239.36 ms /   195 tokens (   16.61 ms per token,    60.20 tokens per second)\n",
      "llama_print_timings:        eval time =    5485.20 ms /    80 runs   (   68.56 ms per token,    14.58 tokens per second)\n",
      "llama_print_timings:       total time =    8833.57 ms /   275 tokens\n",
      " 83%|████████▎ | 216/261 [34:08<06:47,  9.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    69 runs   (    0.07 ms per token, 13497.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.82 ms /   196 tokens (   16.21 ms per token,    61.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4635.19 ms /    68 runs   (   68.16 ms per token,    14.67 tokens per second)\n",
      "llama_print_timings:       total time =    7905.82 ms /   264 tokens\n",
      " 83%|████████▎ | 217/261 [34:16<06:23,  8.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.45 ms /    76 runs   (    0.07 ms per token, 13934.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3206.68 ms /   208 tokens (   15.42 ms per token,    64.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5166.87 ms /    75 runs   (   68.89 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    8476.99 ms /   283 tokens\n",
      " 84%|████████▎ | 218/261 [34:24<06:11,  8.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.10 ms /    85 runs   (    0.07 ms per token, 13939.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.44 ms /   201 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =    5770.20 ms /    84 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    9063.78 ms /   285 tokens\n",
      " 84%|████████▍ | 219/261 [34:33<06:08,  8.77s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.27 ms /    59 runs   (    0.07 ms per token, 13827.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3177.31 ms /   201 tokens (   15.81 ms per token,    63.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3947.50 ms /    58 runs   (   68.06 ms per token,    14.69 tokens per second)\n",
      "llama_print_timings:       total time =    7204.73 ms /   259 tokens\n",
      " 84%|████████▍ | 220/261 [34:40<05:40,  8.30s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.80 ms /    65 runs   (    0.07 ms per token, 13527.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2656.89 ms /   158 tokens (   16.82 ms per token,    59.47 tokens per second)\n",
      "llama_print_timings:        eval time =    4350.16 ms /    64 runs   (   67.97 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =    7096.98 ms /   222 tokens\n",
      " 85%|████████▍ | 221/261 [34:47<05:17,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.10 ms /    69 runs   (    0.07 ms per token, 13529.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3220.41 ms /   210 tokens (   15.34 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4647.50 ms /    68 runs   (   68.35 ms per token,    14.63 tokens per second)\n",
      "llama_print_timings:       total time =    7961.11 ms /   278 tokens\n",
      " 85%|████████▌ | 222/261 [34:55<05:10,  7.95s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.15 ms /    73 runs   (    0.07 ms per token, 14172.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2936.64 ms /   187 tokens (   15.70 ms per token,    63.68 tokens per second)\n",
      "llama_print_timings:        eval time =    4889.72 ms /    72 runs   (   67.91 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    7923.92 ms /   259 tokens\n",
      " 85%|████████▌ | 223/261 [35:03<05:01,  7.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.28 ms /    60 runs   (    0.07 ms per token, 14015.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.50 ms /   187 tokens (   15.65 ms per token,    63.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4044.84 ms /    59 runs   (   68.56 ms per token,    14.59 tokens per second)\n",
      "llama_print_timings:       total time =    7053.99 ms /   246 tokens\n",
      " 86%|████████▌ | 224/261 [35:10<04:44,  7.68s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.77 ms /   131 runs   (    0.07 ms per token, 13405.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3178.65 ms /   196 tokens (   16.22 ms per token,    61.66 tokens per second)\n",
      "llama_print_timings:        eval time =    8969.51 ms /   130 runs   (   69.00 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:       total time =   12330.77 ms /   326 tokens\n",
      " 86%|████████▌ | 225/261 [35:23<05:26,  9.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    72 runs   (    0.07 ms per token, 13556.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2656.36 ms /   160 tokens (   16.60 ms per token,    60.23 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.89 ms /    71 runs   (   67.51 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    7546.81 ms /   231 tokens\n",
      " 87%|████████▋ | 226/261 [35:30<05:01,  8.62s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      30.05 ms /   406 runs   (    0.07 ms per token, 13513.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5545.48 ms /   460 tokens (   12.06 ms per token,    82.95 tokens per second)\n",
      "llama_print_timings:        eval time =   30338.58 ms /   405 runs   (   74.91 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =   36668.67 ms /   865 tokens\n",
      " 87%|████████▋ | 227/261 [36:07<09:39, 17.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    87 runs   (    0.08 ms per token, 13039.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4218.53 ms /   230 tokens (   18.34 ms per token,    54.52 tokens per second)\n",
      "llama_print_timings:        eval time =  194155.29 ms /    86 runs   ( 2257.62 ms per token,     0.44 tokens per second)\n",
      "llama_print_timings:       total time =  198522.77 ms /   316 tokens\n",
      " 87%|████████▋ | 228/261 [39:25<39:18, 71.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.18 ms /    66 runs   (    0.08 ms per token, 12746.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7586.46 ms /   538 tokens (   14.10 ms per token,    70.92 tokens per second)\n",
      "llama_print_timings:        eval time =    4909.96 ms /    65 runs   (   75.54 ms per token,    13.24 tokens per second)\n",
      "llama_print_timings:       total time =   12592.50 ms /   603 tokens\n",
      " 88%|████████▊ | 229/261 [39:38<28:42, 53.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      16.74 ms /   214 runs   (    0.08 ms per token, 12783.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8122.72 ms /   532 tokens (   15.27 ms per token,    65.50 tokens per second)\n",
      "llama_print_timings:        eval time =   17438.91 ms /   213 runs   (   81.87 ms per token,    12.21 tokens per second)\n",
      "llama_print_timings:       total time =   25872.42 ms /   745 tokens\n",
      " 88%|████████▊ | 230/261 [40:04<23:28, 45.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    75 runs   (    0.07 ms per token, 13943.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9411.16 ms /   567 tokens (   16.60 ms per token,    60.25 tokens per second)\n",
      "llama_print_timings:        eval time =    5391.56 ms /    74 runs   (   72.86 ms per token,    13.73 tokens per second)\n",
      "llama_print_timings:       total time =   14909.19 ms /   641 tokens\n",
      " 89%|████████▊ | 231/261 [40:19<18:08, 36.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /    72 runs   (    0.08 ms per token, 12190.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3095.81 ms /   153 tokens (   20.23 ms per token,    49.42 tokens per second)\n",
      "llama_print_timings:        eval time =    7307.02 ms /    71 runs   (  102.92 ms per token,     9.72 tokens per second)\n",
      "llama_print_timings:       total time =   10515.69 ms /   224 tokens\n",
      " 89%|████████▉ | 232/261 [40:29<13:48, 28.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.55 ms /   102 runs   (    0.07 ms per token, 13517.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10286.93 ms /   585 tokens (   17.58 ms per token,    56.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7586.20 ms /   101 runs   (   75.11 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   18016.36 ms /   686 tokens\n",
      " 89%|████████▉ | 233/261 [40:47<11:51, 25.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /    80 runs   (    0.08 ms per token, 13260.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6930.84 ms /   514 tokens (   13.48 ms per token,    74.16 tokens per second)\n",
      "llama_print_timings:        eval time =    6048.76 ms /    79 runs   (   76.57 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   13095.42 ms /   593 tokens\n",
      " 90%|████████▉ | 234/261 [41:00<09:46, 21.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.94 ms /    66 runs   (    0.07 ms per token, 13349.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8656.37 ms /   598 tokens (   14.48 ms per token,    69.08 tokens per second)\n",
      "llama_print_timings:        eval time =    4796.28 ms /    65 runs   (   73.79 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   13543.96 ms /   663 tokens\n",
      " 90%|█████████ | 235/261 [41:14<08:20, 19.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.04 ms /    80 runs   (    0.08 ms per token, 13247.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4298.72 ms /   312 tokens (   13.78 ms per token,    72.58 tokens per second)\n",
      "llama_print_timings:        eval time =    5820.29 ms /    79 runs   (   73.67 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   10231.68 ms /   391 tokens\n",
      " 90%|█████████ | 236/261 [41:24<06:53, 16.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.72 ms /   131 runs   (    0.07 ms per token, 13480.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6501.85 ms /   499 tokens (   13.03 ms per token,    76.75 tokens per second)\n",
      "llama_print_timings:        eval time =    9908.00 ms /   130 runs   (   76.22 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:       total time =   16589.37 ms /   629 tokens\n",
      " 91%|█████████ | 237/261 [41:41<06:37, 16.57s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.38 ms /    74 runs   (    0.07 ms per token, 13754.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3231.57 ms /   192 tokens (   16.83 ms per token,    59.41 tokens per second)\n",
      "llama_print_timings:        eval time =    6232.46 ms /    73 runs   (   85.38 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =    9574.67 ms /   265 tokens\n",
      " 91%|█████████ | 238/261 [41:50<05:32, 14.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.96 ms /    63 runs   (    0.08 ms per token, 12704.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3457.18 ms /   203 tokens (   17.03 ms per token,    58.72 tokens per second)\n",
      "llama_print_timings:        eval time =    5631.75 ms /    62 runs   (   90.83 ms per token,    11.01 tokens per second)\n",
      "llama_print_timings:       total time =    9179.49 ms /   265 tokens\n",
      " 92%|█████████▏| 239/261 [42:00<04:43, 12.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.87 ms /    92 runs   (    0.07 ms per token, 13399.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3502.60 ms /   211 tokens (   16.60 ms per token,    60.24 tokens per second)\n",
      "llama_print_timings:        eval time =    6406.11 ms /    91 runs   (   70.40 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =   10033.55 ms /   302 tokens\n",
      " 92%|█████████▏| 240/261 [42:10<04:12, 12.03s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.63 ms /    77 runs   (    0.07 ms per token, 13671.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3436.93 ms /   199 tokens (   17.27 ms per token,    57.90 tokens per second)\n",
      "llama_print_timings:        eval time =    5300.31 ms /    76 runs   (   69.74 ms per token,    14.34 tokens per second)\n",
      "llama_print_timings:       total time =    8840.47 ms /   275 tokens\n",
      " 92%|█████████▏| 241/261 [42:18<03:41, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.12 ms /    55 runs   (    0.07 ms per token, 13356.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3298.94 ms /   195 tokens (   16.92 ms per token,    59.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4047.95 ms /    54 runs   (   74.96 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =    7422.87 ms /   249 tokens\n",
      " 93%|█████████▎| 242/261 [42:26<03:09,  9.98s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.19 ms /    57 runs   (    0.07 ms per token, 13616.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3269.84 ms /   208 tokens (   15.72 ms per token,    63.61 tokens per second)\n",
      "llama_print_timings:        eval time =    3730.95 ms /    56 runs   (   66.62 ms per token,    15.01 tokens per second)\n",
      "llama_print_timings:       total time =    7077.74 ms /   264 tokens\n",
      " 93%|█████████▎| 243/261 [42:33<02:43,  9.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.39 ms /    61 runs   (    0.07 ms per token, 13901.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3408.52 ms /   195 tokens (   17.48 ms per token,    57.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4056.30 ms /    60 runs   (   67.61 ms per token,    14.79 tokens per second)\n",
      "llama_print_timings:       total time =    7545.32 ms /   255 tokens\n",
      " 93%|█████████▎| 244/261 [42:41<02:26,  8.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    81 runs   (    0.07 ms per token, 13484.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3250.48 ms /   201 tokens (   16.17 ms per token,    61.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5997.32 ms /    80 runs   (   74.97 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:       total time =    9356.41 ms /   281 tokens\n",
      " 94%|█████████▍| 245/261 [42:50<02:21,  8.86s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.46 ms /    59 runs   (    0.08 ms per token, 13225.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3539.99 ms /   200 tokens (   17.70 ms per token,    56.50 tokens per second)\n",
      "llama_print_timings:        eval time =    4691.89 ms /    58 runs   (   80.89 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:       total time =    8317.48 ms /   258 tokens\n",
      " 94%|█████████▍| 246/261 [42:58<02:10,  8.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.65 ms /    98 runs   (    0.09 ms per token, 11325.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3498.59 ms /   190 tokens (   18.41 ms per token,    54.31 tokens per second)\n",
      "llama_print_timings:        eval time =   10496.63 ms /    97 runs   (  108.21 ms per token,     9.24 tokens per second)\n",
      "llama_print_timings:       total time =   14141.59 ms /   287 tokens\n",
      " 95%|█████████▍| 247/261 [43:12<02:24, 10.33s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.04 ms /    63 runs   (    0.08 ms per token, 12497.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3910.30 ms /   202 tokens (   19.36 ms per token,    51.66 tokens per second)\n",
      "llama_print_timings:        eval time =    6953.48 ms /    62 runs   (  112.15 ms per token,     8.92 tokens per second)\n",
      "llama_print_timings:       total time =   10958.13 ms /   264 tokens\n",
      " 95%|█████████▌| 248/261 [43:23<02:16, 10.52s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.93 ms /    51 runs   (    0.08 ms per token, 12993.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3198.86 ms /   191 tokens (   16.75 ms per token,    59.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3935.09 ms /    50 runs   (   78.70 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:       total time =    7205.78 ms /   241 tokens\n",
      " 95%|█████████▌| 249/261 [43:31<01:54,  9.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.00 ms /    68 runs   (    0.07 ms per token, 13605.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3457.43 ms /   196 tokens (   17.64 ms per token,    56.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5000.84 ms /    67 runs   (   74.64 ms per token,    13.40 tokens per second)\n",
      "llama_print_timings:       total time =    8550.76 ms /   263 tokens\n",
      " 96%|█████████▌| 250/261 [43:39<01:41,  9.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    85 runs   (    0.07 ms per token, 13576.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3170.47 ms /   186 tokens (   17.05 ms per token,    58.67 tokens per second)\n",
      "llama_print_timings:        eval time =    6024.18 ms /    84 runs   (   71.72 ms per token,    13.94 tokens per second)\n",
      "llama_print_timings:       total time =    9311.59 ms /   270 tokens\n",
      " 96%|█████████▌| 251/261 [43:48<01:32,  9.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.14 ms /    67 runs   (    0.08 ms per token, 13040.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3291.78 ms /   189 tokens (   17.42 ms per token,    57.42 tokens per second)\n",
      "llama_print_timings:        eval time =    5075.55 ms /    66 runs   (   76.90 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:       total time =    8459.48 ms /   255 tokens\n",
      " 97%|█████████▋| 252/261 [43:57<01:21,  9.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.20 ms /    52 runs   (    0.08 ms per token, 12375.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3581.89 ms /   198 tokens (   18.09 ms per token,    55.28 tokens per second)\n",
      "llama_print_timings:        eval time =    6144.84 ms /    51 runs   (  120.49 ms per token,     8.30 tokens per second)\n",
      "llama_print_timings:       total time =    9803.88 ms /   249 tokens\n",
      " 97%|█████████▋| 253/261 [44:07<01:14,  9.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.11 ms /    66 runs   (    0.08 ms per token, 12913.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4194.97 ms /   261 tokens (   16.07 ms per token,    62.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5379.33 ms /    65 runs   (   82.76 ms per token,    12.08 tokens per second)\n",
      "llama_print_timings:       total time =    9665.62 ms /   326 tokens\n",
      " 97%|█████████▋| 254/261 [44:16<01:05,  9.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.03 ms /   107 runs   (    0.08 ms per token, 13323.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8971.55 ms /   585 tokens (   15.34 ms per token,    65.21 tokens per second)\n",
      "llama_print_timings:        eval time =    8394.06 ms /   106 runs   (   79.19 ms per token,    12.63 tokens per second)\n",
      "llama_print_timings:       total time =   17520.47 ms /   691 tokens\n",
      " 98%|█████████▊| 255/261 [44:34<01:10, 11.83s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.83 ms /   143 runs   (    0.08 ms per token, 13202.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9004.50 ms /   597 tokens (   15.08 ms per token,    66.30 tokens per second)\n",
      "llama_print_timings:        eval time =   11337.83 ms /   142 runs   (   79.84 ms per token,    12.52 tokens per second)\n",
      "llama_print_timings:       total time =   20548.34 ms /   739 tokens\n",
      " 98%|█████████▊| 256/261 [44:54<01:12, 14.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.20 ms /    69 runs   (    0.08 ms per token, 13269.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7775.48 ms /   533 tokens (   14.59 ms per token,    68.55 tokens per second)\n",
      "llama_print_timings:        eval time =    5727.12 ms /    68 runs   (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =   13611.20 ms /   601 tokens\n",
      " 98%|█████████▊| 257/261 [45:08<00:56, 14.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      14.52 ms /   195 runs   (    0.07 ms per token, 13427.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5961.61 ms /   487 tokens (   12.24 ms per token,    81.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15081.12 ms /   194 runs   (   77.74 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:       total time =   21322.46 ms /   681 tokens\n",
      " 99%|█████████▉| 258/261 [45:29<00:49, 16.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   176 runs   (    0.08 ms per token, 12646.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8205.51 ms /   555 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:        eval time =   12901.10 ms /   175 runs   (   73.72 ms per token,    13.56 tokens per second)\n",
      "llama_print_timings:       total time =   21351.56 ms /   730 tokens\n",
      " 99%|█████████▉| 259/261 [45:51<00:35, 17.84s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    88 runs   (    0.07 ms per token, 13499.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8538.24 ms /   589 tokens (   14.50 ms per token,    68.98 tokens per second)\n",
      "llama_print_timings:        eval time =    6381.37 ms /    87 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   15039.46 ms /   676 tokens\n",
      "100%|█████████▉| 260/261 [46:06<00:17, 17.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.19 ms /    83 runs   (    0.07 ms per token, 13404.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8633.82 ms /   609 tokens (   14.18 ms per token,    70.54 tokens per second)\n",
      "llama_print_timings:        eval time =    7969.59 ms /    82 runs   (   97.19 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   16725.52 ms /   691 tokens\n",
      "100%|██████████| 261/261 [46:23<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_splitter_1024 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105 [00:00<?, ?it/s]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      14.24 ms /   177 runs   (    0.08 ms per token, 12428.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   20554.30 ms /   857 tokens (   23.98 ms per token,    41.69 tokens per second)\n",
      "llama_print_timings:        eval time =   15493.65 ms /   176 runs   (   88.03 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   36469.64 ms /  1033 tokens\n",
      "  1%|          | 1/105 [00:36<1:03:13, 36.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    67 runs   (    0.08 ms per token, 13096.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9079.79 ms /   565 tokens (   16.07 ms per token,    62.23 tokens per second)\n",
      "llama_print_timings:        eval time =    6003.13 ms /    66 runs   (   90.96 ms per token,    10.99 tokens per second)\n",
      "llama_print_timings:       total time =   15245.31 ms /   631 tokens\n",
      "  2%|▏         | 2/105 [00:51<41:11, 23.99s/it]  Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.69 ms /    78 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11197.73 ms /   882 tokens (   12.70 ms per token,    78.77 tokens per second)\n",
      "llama_print_timings:        eval time =    5666.11 ms /    77 runs   (   73.59 ms per token,    13.59 tokens per second)\n",
      "llama_print_timings:       total time =   16966.86 ms /   959 tokens\n",
      "  3%|▎         | 3/105 [01:08<35:20, 20.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    61 runs   (    0.08 ms per token, 13310.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4921.91 ms /   400 tokens (   12.30 ms per token,    81.27 tokens per second)\n",
      "llama_print_timings:        eval time =    4297.47 ms /    60 runs   (   71.62 ms per token,    13.96 tokens per second)\n",
      "llama_print_timings:       total time =    9362.03 ms /   460 tokens\n",
      "  4%|▍         | 4/105 [01:18<27:24, 16.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.55 ms /    85 runs   (    0.08 ms per token, 12975.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15085.05 ms /  1102 tokens (   13.69 ms per token,    73.05 tokens per second)\n",
      "llama_print_timings:        eval time =    6155.90 ms /    84 runs   (   73.28 ms per token,    13.65 tokens per second)\n",
      "llama_print_timings:       total time =   21442.97 ms /  1186 tokens\n",
      "  5%|▍         | 5/105 [01:39<30:14, 18.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.42 ms /   116 runs   (    0.07 ms per token, 13784.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12298.48 ms /   934 tokens (   13.17 ms per token,    75.94 tokens per second)\n",
      "llama_print_timings:        eval time =   10598.08 ms /   115 runs   (   92.16 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   23073.97 ms /  1049 tokens\n",
      "  6%|▌         | 6/105 [02:02<32:42, 19.82s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.41 ms /    88 runs   (    0.07 ms per token, 13732.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10763.48 ms /   850 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6412.32 ms /    87 runs   (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   17300.87 ms /   937 tokens\n",
      "  7%|▋         | 7/105 [02:19<31:01, 19.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.34 ms /    68 runs   (    0.08 ms per token, 12741.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9308.88 ms /   588 tokens (   15.83 ms per token,    63.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5804.33 ms /    67 runs   (   86.63 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =   15232.36 ms /   655 tokens\n",
      "  8%|▊         | 8/105 [02:35<28:46, 17.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.51 ms /    65 runs   (    0.07 ms per token, 14418.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10800.46 ms /   853 tokens (   12.66 ms per token,    78.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4700.40 ms /    64 runs   (   73.44 ms per token,    13.62 tokens per second)\n",
      "llama_print_timings:       total time =   15585.06 ms /   917 tokens\n",
      "  9%|▊         | 9/105 [02:50<27:22, 17.11s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    96 runs   (    0.08 ms per token, 12917.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5084.15 ms /   358 tokens (   14.20 ms per token,    70.41 tokens per second)\n",
      "llama_print_timings:        eval time =   13159.93 ms /    95 runs   (  138.53 ms per token,     7.22 tokens per second)\n",
      "llama_print_timings:       total time =   18405.04 ms /   453 tokens\n",
      " 10%|▉         | 10/105 [03:09<27:43, 17.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.93 ms /    63 runs   (    0.08 ms per token, 12778.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4320.70 ms /   315 tokens (   13.72 ms per token,    72.90 tokens per second)\n",
      "llama_print_timings:        eval time =    4887.71 ms /    62 runs   (   78.83 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:       total time =    9326.21 ms /   377 tokens\n",
      " 10%|█         | 11/105 [03:18<23:30, 15.01s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.94 ms /    77 runs   (    0.08 ms per token, 12954.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4824.52 ms /   322 tokens (   14.98 ms per token,    66.74 tokens per second)\n",
      "llama_print_timings:        eval time =    6467.26 ms /    76 runs   (   85.10 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =   11399.64 ms /   398 tokens\n",
      " 11%|█▏        | 12/105 [03:29<21:33, 13.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.43 ms /    71 runs   (    0.08 ms per token, 13080.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4767.07 ms /   323 tokens (   14.76 ms per token,    67.76 tokens per second)\n",
      "llama_print_timings:        eval time =    6075.86 ms /    70 runs   (   86.80 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =   10945.08 ms /   393 tokens\n",
      " 12%|█▏        | 13/105 [03:40<19:57, 13.02s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.15 ms /    71 runs   (    0.09 ms per token, 11550.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5151.63 ms /   378 tokens (   13.63 ms per token,    73.37 tokens per second)\n",
      "llama_print_timings:        eval time =   14830.27 ms /    70 runs   (  211.86 ms per token,     4.72 tokens per second)\n",
      "llama_print_timings:       total time =   20112.81 ms /   448 tokens\n",
      " 13%|█▎        | 14/105 [04:00<22:59, 15.16s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.16 ms /    67 runs   (    0.08 ms per token, 12984.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9398.53 ms /   644 tokens (   14.59 ms per token,    68.52 tokens per second)\n",
      "llama_print_timings:        eval time =    5722.45 ms /    66 runs   (   86.70 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =   15259.00 ms /   710 tokens\n",
      " 14%|█▍        | 15/105 [04:16<22:47, 15.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.84 ms /    82 runs   (    0.08 ms per token, 11993.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11035.31 ms /   756 tokens (   14.60 ms per token,    68.51 tokens per second)\n",
      "llama_print_timings:        eval time =    8669.82 ms /    81 runs   (  107.03 ms per token,     9.34 tokens per second)\n",
      "llama_print_timings:       total time =   19885.55 ms /   837 tokens\n",
      " 15%|█▌        | 16/105 [04:36<24:38, 16.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.14 ms /    56 runs   (    0.07 ms per token, 13526.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4582.43 ms /   338 tokens (   13.56 ms per token,    73.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4088.23 ms /    55 runs   (   74.33 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =    8745.56 ms /   393 tokens\n",
      " 16%|█▌        | 17/105 [04:44<20:53, 14.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    82 runs   (    0.07 ms per token, 13746.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12017.75 ms /   949 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_print_timings:        eval time =    6062.76 ms /    81 runs   (   74.85 ms per token,    13.36 tokens per second)\n",
      "llama_print_timings:       total time =   18207.33 ms /  1030 tokens\n",
      " 17%|█▋        | 18/105 [05:03<22:23, 15.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    81 runs   (    0.07 ms per token, 13846.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4484.09 ms /   341 tokens (   13.15 ms per token,    76.05 tokens per second)\n",
      "llama_print_timings:        eval time =    5660.04 ms /    80 runs   (   70.75 ms per token,    14.13 tokens per second)\n",
      "llama_print_timings:       total time =   10251.57 ms /   421 tokens\n",
      " 18%|█▊        | 19/105 [05:13<19:53, 13.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      15.51 ms /   182 runs   (    0.09 ms per token, 11735.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3515.39 ms /   238 tokens (   14.77 ms per token,    67.70 tokens per second)\n",
      "llama_print_timings:        eval time =   15927.05 ms /   181 runs   (   87.99 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   19766.08 ms /   419 tokens\n",
      " 19%|█▉        | 20/105 [05:33<22:10, 15.65s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.07 ms /   101 runs   (    0.08 ms per token, 12518.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15536.59 ms /  1083 tokens (   14.35 ms per token,    69.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9227.53 ms /   100 runs   (   92.28 ms per token,    10.84 tokens per second)\n",
      "llama_print_timings:       total time =   24946.59 ms /  1183 tokens\n",
      " 20%|██        | 21/105 [05:58<25:49, 18.44s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.85 ms /   110 runs   (    0.08 ms per token, 12432.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12172.04 ms /   808 tokens (   15.06 ms per token,    66.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9465.85 ms /   109 runs   (   86.84 ms per token,    11.52 tokens per second)\n",
      "llama_print_timings:       total time =   21811.00 ms /   917 tokens\n",
      " 21%|██        | 22/105 [06:19<26:54, 19.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    71 runs   (    0.07 ms per token, 13373.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4226.15 ms /   293 tokens (   14.42 ms per token,    69.33 tokens per second)\n",
      "llama_print_timings:        eval time =    5265.66 ms /    70 runs   (   75.22 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:       total time =    9590.16 ms /   363 tokens\n",
      " 22%|██▏       | 23/105 [06:29<22:32, 16.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.70 ms /    78 runs   (    0.07 ms per token, 13696.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4247.79 ms /   296 tokens (   14.35 ms per token,    69.68 tokens per second)\n",
      "llama_print_timings:        eval time =    5520.36 ms /    77 runs   (   71.69 ms per token,    13.95 tokens per second)\n",
      "llama_print_timings:       total time =    9869.79 ms /   373 tokens\n",
      " 23%|██▎       | 24/105 [06:39<19:35, 14.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    64 runs   (    0.07 ms per token, 13976.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4120.01 ms /   308 tokens (   13.38 ms per token,    74.76 tokens per second)\n",
      "llama_print_timings:        eval time =    4339.20 ms /    63 runs   (   68.88 ms per token,    14.52 tokens per second)\n",
      "llama_print_timings:       total time =    8540.67 ms /   371 tokens\n",
      " 24%|██▍       | 25/105 [06:47<16:57, 12.72s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.78 ms /    78 runs   (    0.07 ms per token, 13499.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4065.81 ms /   315 tokens (   12.91 ms per token,    77.48 tokens per second)\n",
      "llama_print_timings:        eval time =    5348.35 ms /    77 runs   (   69.46 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:       total time =    9512.26 ms /   392 tokens\n",
      " 25%|██▍       | 26/105 [06:57<15:28, 11.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.22 ms /    56 runs   (    0.08 ms per token, 13263.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3847.68 ms /   259 tokens (   14.86 ms per token,    67.31 tokens per second)\n",
      "llama_print_timings:        eval time =    5008.56 ms /    55 runs   (   91.06 ms per token,    10.98 tokens per second)\n",
      "llama_print_timings:       total time =    8936.40 ms /   314 tokens\n",
      " 26%|██▌       | 27/105 [07:06<14:11, 10.91s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.99 ms /    81 runs   (    0.07 ms per token, 13518.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17521.50 ms /  1057 tokens (   16.58 ms per token,    60.33 tokens per second)\n",
      "llama_print_timings:        eval time =    7041.06 ms /    80 runs   (   88.01 ms per token,    11.36 tokens per second)\n",
      "llama_print_timings:       total time =   24709.36 ms /  1137 tokens\n",
      " 27%|██▋       | 28/105 [07:31<19:19, 15.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.92 ms /   123 runs   (    0.08 ms per token, 12396.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12603.20 ms /   930 tokens (   13.55 ms per token,    73.79 tokens per second)\n",
      "llama_print_timings:        eval time =   10309.66 ms /   122 runs   (   84.51 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =   23095.25 ms /  1052 tokens\n",
      " 28%|██▊       | 29/105 [07:54<22:07, 17.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.87 ms /   104 runs   (    0.08 ms per token, 13211.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9833.53 ms /   697 tokens (   14.11 ms per token,    70.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7737.02 ms /   103 runs   (   75.12 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:       total time =   17768.61 ms /   800 tokens\n",
      " 29%|██▊       | 30/105 [08:11<21:57, 17.56s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    84 runs   (    0.07 ms per token, 13401.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4805.73 ms /   371 tokens (   12.95 ms per token,    77.20 tokens per second)\n",
      "llama_print_timings:        eval time =    6118.11 ms /    83 runs   (   73.71 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   11038.59 ms /   454 tokens\n",
      " 30%|██▉       | 31/105 [08:23<19:14, 15.61s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       8.20 ms /   111 runs   (    0.07 ms per token, 13531.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9006.80 ms /   626 tokens (   14.39 ms per token,    69.50 tokens per second)\n",
      "llama_print_timings:        eval time =    8161.48 ms /   110 runs   (   74.20 ms per token,    13.48 tokens per second)\n",
      "llama_print_timings:       total time =   17318.64 ms /   736 tokens\n",
      " 30%|███       | 32/105 [08:40<19:36, 16.12s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.11 ms /   127 runs   (    0.07 ms per token, 13939.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9731.61 ms /   711 tokens (   13.69 ms per token,    73.06 tokens per second)\n",
      "llama_print_timings:        eval time =    9324.93 ms /   126 runs   (   74.01 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   19227.06 ms /   837 tokens\n",
      " 31%|███▏      | 33/105 [08:59<20:27, 17.06s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    61 runs   (    0.07 ms per token, 13994.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4126.23 ms /   304 tokens (   13.57 ms per token,    73.67 tokens per second)\n",
      "llama_print_timings:        eval time =    4175.11 ms /    60 runs   (   69.59 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =    8379.34 ms /   364 tokens\n",
      " 32%|███▏      | 34/105 [09:07<17:06, 14.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.27 ms /    72 runs   (    0.07 ms per token, 13667.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4071.03 ms /   316 tokens (   12.88 ms per token,    77.62 tokens per second)\n",
      "llama_print_timings:        eval time =    5331.68 ms /    71 runs   (   75.09 ms per token,    13.32 tokens per second)\n",
      "llama_print_timings:       total time =    9496.14 ms /   387 tokens\n",
      " 33%|███▎      | 35/105 [09:17<15:07, 12.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.88 ms /    61 runs   (    0.08 ms per token, 12487.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4322.47 ms /   314 tokens (   13.77 ms per token,    72.64 tokens per second)\n",
      "llama_print_timings:        eval time =    5330.58 ms /    60 runs   (   88.84 ms per token,    11.26 tokens per second)\n",
      "llama_print_timings:       total time =    9741.41 ms /   374 tokens\n",
      " 34%|███▍      | 36/105 [09:27<13:48, 12.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    68 runs   (    0.07 ms per token, 13580.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4607.35 ms /   310 tokens (   14.86 ms per token,    67.28 tokens per second)\n",
      "llama_print_timings:        eval time =    5317.82 ms /    67 runs   (   79.37 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:       total time =   10021.82 ms /   377 tokens\n",
      " 35%|███▌      | 37/105 [09:37<12:55, 11.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.14 ms /    98 runs   (    0.07 ms per token, 13721.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4367.67 ms /   328 tokens (   13.32 ms per token,    75.10 tokens per second)\n",
      "llama_print_timings:        eval time =    7200.18 ms /    97 runs   (   74.23 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:       total time =   11699.65 ms /   425 tokens\n",
      " 36%|███▌      | 38/105 [09:48<12:50, 11.50s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.70 ms /    76 runs   (    0.13 ms per token,  7836.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4418.74 ms /   314 tokens (   14.07 ms per token,    71.06 tokens per second)\n",
      "llama_print_timings:        eval time =    6237.05 ms /    75 runs   (   83.16 ms per token,    12.02 tokens per second)\n",
      "llama_print_timings:       total time =   10771.34 ms /   389 tokens\n",
      " 37%|███▋      | 39/105 [09:59<12:24, 11.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.60 ms /    86 runs   (    0.08 ms per token, 13022.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4441.21 ms /   324 tokens (   13.71 ms per token,    72.95 tokens per second)\n",
      "llama_print_timings:        eval time =    6506.57 ms /    85 runs   (   76.55 ms per token,    13.06 tokens per second)\n",
      "llama_print_timings:       total time =   11060.68 ms /   409 tokens\n",
      " 38%|███▊      | 40/105 [10:10<12:09, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.55 ms /    62 runs   (    0.07 ms per token, 13632.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3620.27 ms /   194 tokens (   18.66 ms per token,    53.59 tokens per second)\n",
      "llama_print_timings:        eval time =    4190.22 ms /    61 runs   (   68.69 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:       total time =    7942.11 ms /   255 tokens\n",
      " 39%|███▉      | 41/105 [10:18<10:55, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.80 ms /    68 runs   (    0.07 ms per token, 14157.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10070.95 ms /   775 tokens (   12.99 ms per token,    76.95 tokens per second)\n",
      "llama_print_timings:        eval time =    4937.98 ms /    67 runs   (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   15098.97 ms /   842 tokens\n",
      " 40%|████      | 42/105 [10:33<12:16, 11.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.33 ms /    73 runs   (    0.07 ms per token, 13693.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4436.19 ms /   315 tokens (   14.08 ms per token,    71.01 tokens per second)\n",
      "llama_print_timings:        eval time =    5065.36 ms /    72 runs   (   70.35 ms per token,    14.21 tokens per second)\n",
      "llama_print_timings:       total time =    9597.98 ms /   387 tokens\n",
      " 41%|████      | 43/105 [10:43<11:26, 11.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    69 runs   (    0.07 ms per token, 13942.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4486.76 ms /   311 tokens (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:        eval time =    4871.93 ms /    68 runs   (   71.65 ms per token,    13.96 tokens per second)\n",
      "llama_print_timings:       total time =    9452.34 ms /   379 tokens\n",
      " 42%|████▏     | 44/105 [10:52<10:45, 10.59s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    64 runs   (    0.07 ms per token, 13736.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4213.04 ms /   310 tokens (   13.59 ms per token,    73.58 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.82 ms /    63 runs   (   79.03 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:       total time =    9285.74 ms /   373 tokens\n",
      " 43%|████▎     | 45/105 [11:02<10:11, 10.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.06 ms /    55 runs   (    0.07 ms per token, 13546.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4146.68 ms /   309 tokens (   13.42 ms per token,    74.52 tokens per second)\n",
      "llama_print_timings:        eval time =    4321.90 ms /    54 runs   (   80.04 ms per token,    12.49 tokens per second)\n",
      "llama_print_timings:       total time =    8542.50 ms /   363 tokens\n",
      " 44%|████▍     | 46/105 [11:10<09:32,  9.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.12 ms /    64 runs   (    0.08 ms per token, 12497.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4440.80 ms /   319 tokens (   13.92 ms per token,    71.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5867.98 ms /    63 runs   (   93.14 ms per token,    10.74 tokens per second)\n",
      "llama_print_timings:       total time =   10408.36 ms /   382 tokens\n",
      " 45%|████▍     | 47/105 [11:21<09:35,  9.92s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    81 runs   (    0.07 ms per token, 13477.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4493.69 ms /   324 tokens (   13.87 ms per token,    72.10 tokens per second)\n",
      "llama_print_timings:        eval time =    5919.42 ms /    80 runs   (   73.99 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:       total time =   10522.54 ms /   404 tokens\n",
      " 46%|████▌     | 48/105 [11:31<09:35, 10.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.61 ms /    76 runs   (    0.07 ms per token, 13556.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4740.74 ms /   324 tokens (   14.63 ms per token,    68.34 tokens per second)\n",
      "llama_print_timings:        eval time =    5555.56 ms /    75 runs   (   74.07 ms per token,    13.50 tokens per second)\n",
      "llama_print_timings:       total time =   10400.82 ms /   399 tokens\n",
      " 47%|████▋     | 49/105 [11:42<09:31, 10.20s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.92 ms /    66 runs   (    0.07 ms per token, 13403.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4572.53 ms /   333 tokens (   13.73 ms per token,    72.83 tokens per second)\n",
      "llama_print_timings:        eval time =    5701.34 ms /    65 runs   (   87.71 ms per token,    11.40 tokens per second)\n",
      "llama_print_timings:       total time =   10362.27 ms /   398 tokens\n",
      " 48%|████▊     | 50/105 [11:52<09:23, 10.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.62 ms /    77 runs   (    0.07 ms per token, 13688.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4190.80 ms /   314 tokens (   13.35 ms per token,    74.93 tokens per second)\n",
      "llama_print_timings:        eval time =    5755.48 ms /    76 runs   (   75.73 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:       total time =   10053.69 ms /   390 tokens\n",
      " 49%|████▊     | 51/105 [12:02<09:10, 10.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.26 ms /   131 runs   (    0.08 ms per token, 12768.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4514.67 ms /   332 tokens (   13.60 ms per token,    73.54 tokens per second)\n",
      "llama_print_timings:        eval time =    9574.88 ms /   130 runs   (   73.65 ms per token,    13.58 tokens per second)\n",
      "llama_print_timings:       total time =   14270.87 ms /   462 tokens\n",
      " 50%|████▉     | 52/105 [12:16<10:05, 11.42s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.77 ms /    81 runs   (    0.07 ms per token, 14043.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4195.86 ms /   320 tokens (   13.11 ms per token,    76.27 tokens per second)\n",
      "llama_print_timings:        eval time =    5613.53 ms /    80 runs   (   70.17 ms per token,    14.25 tokens per second)\n",
      "llama_print_timings:       total time =    9914.02 ms /   400 tokens\n",
      " 50%|█████     | 53/105 [12:26<09:30, 10.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.95 ms /    67 runs   (    0.07 ms per token, 13546.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4648.98 ms /   331 tokens (   14.05 ms per token,    71.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4758.04 ms /    66 runs   (   72.09 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:       total time =    9496.04 ms /   397 tokens\n",
      " 51%|█████▏    | 54/105 [12:36<08:56, 10.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    91 runs   (    0.07 ms per token, 14110.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4488.54 ms /   338 tokens (   13.28 ms per token,    75.30 tokens per second)\n",
      "llama_print_timings:        eval time =    6494.47 ms /    90 runs   (   72.16 ms per token,    13.86 tokens per second)\n",
      "llama_print_timings:       total time =   11102.74 ms /   428 tokens\n",
      " 52%|█████▏    | 55/105 [12:47<08:55, 10.70s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    71 runs   (    0.08 ms per token, 13001.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4220.82 ms /   319 tokens (   13.23 ms per token,    75.58 tokens per second)\n",
      "llama_print_timings:        eval time =    7029.03 ms /    70 runs   (  100.41 ms per token,     9.96 tokens per second)\n",
      "llama_print_timings:       total time =   11354.66 ms /   389 tokens\n",
      " 53%|█████▎    | 56/105 [12:58<08:54, 10.90s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.00 ms /    67 runs   (    0.07 ms per token, 13389.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4883.53 ms /   335 tokens (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5277.21 ms /    66 runs   (   79.96 ms per token,    12.51 tokens per second)\n",
      "llama_print_timings:       total time =   10257.99 ms /   401 tokens\n",
      " 54%|█████▍    | 57/105 [13:08<08:34, 10.71s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.53 ms /    61 runs   (    0.09 ms per token, 11026.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6070.87 ms /   315 tokens (   19.27 ms per token,    51.89 tokens per second)\n",
      "llama_print_timings:        eval time =   10581.76 ms /    60 runs   (  176.36 ms per token,     5.67 tokens per second)\n",
      "llama_print_timings:       total time =   16765.97 ms /   375 tokens\n",
      " 55%|█████▌    | 58/105 [13:25<09:48, 12.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.01 ms /    75 runs   (    0.08 ms per token, 12481.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4882.95 ms /   328 tokens (   14.89 ms per token,    67.17 tokens per second)\n",
      "llama_print_timings:        eval time =    6817.42 ms /    74 runs   (   92.13 ms per token,    10.85 tokens per second)\n",
      "llama_print_timings:       total time =   11813.78 ms /   402 tokens\n",
      " 56%|█████▌    | 59/105 [13:37<09:26, 12.32s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.76 ms /    69 runs   (    0.08 ms per token, 11977.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4382.81 ms /   310 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_print_timings:        eval time =    8225.75 ms /    68 runs   (  120.97 ms per token,     8.27 tokens per second)\n",
      "llama_print_timings:       total time =   12738.98 ms /   378 tokens\n",
      " 57%|█████▋    | 60/105 [13:50<09:20, 12.45s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.26 ms /    68 runs   (    0.08 ms per token, 12920.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4881.43 ms /   323 tokens (   15.11 ms per token,    66.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5568.03 ms /    67 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   10549.72 ms /   390 tokens\n",
      " 58%|█████▊    | 61/105 [14:00<08:42, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.07 ms /    88 runs   (    0.08 ms per token, 12454.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5372.31 ms /   313 tokens (   17.16 ms per token,    58.26 tokens per second)\n",
      "llama_print_timings:        eval time =    8534.59 ms /    87 runs   (   98.10 ms per token,    10.19 tokens per second)\n",
      "llama_print_timings:       total time =   14034.08 ms /   400 tokens\n",
      " 59%|█████▉    | 62/105 [14:14<08:58, 12.53s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.22 ms /    79 runs   (    0.08 ms per token, 12705.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5511.11 ms /   321 tokens (   17.17 ms per token,    58.25 tokens per second)\n",
      "llama_print_timings:        eval time =    7583.33 ms /    78 runs   (   97.22 ms per token,    10.29 tokens per second)\n",
      "llama_print_timings:       total time =   13208.67 ms /   399 tokens\n",
      " 60%|██████    | 63/105 [14:28<08:54, 12.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.18 ms /    68 runs   (    0.08 ms per token, 13137.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5034.60 ms /   314 tokens (   16.03 ms per token,    62.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5964.39 ms /    67 runs   (   89.02 ms per token,    11.23 tokens per second)\n",
      "llama_print_timings:       total time =   11094.65 ms /   381 tokens\n",
      " 61%|██████    | 64/105 [14:39<08:22, 12.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.08 ms /    67 runs   (    0.08 ms per token, 13196.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4702.23 ms /   328 tokens (   14.34 ms per token,    69.75 tokens per second)\n",
      "llama_print_timings:        eval time =    5756.94 ms /    66 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =   10554.38 ms /   394 tokens\n",
      " 62%|██████▏   | 65/105 [14:49<07:49, 11.74s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.98 ms /    70 runs   (    0.07 ms per token, 14061.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4376.77 ms /   322 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_print_timings:        eval time =    4817.30 ms /    69 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =    9284.69 ms /   391 tokens\n",
      " 63%|██████▎   | 66/105 [14:59<07:09, 11.00s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      10.92 ms /   145 runs   (    0.08 ms per token, 13274.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5049.51 ms /   320 tokens (   15.78 ms per token,    63.37 tokens per second)\n",
      "llama_print_timings:        eval time =   13034.02 ms /   144 runs   (   90.51 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =   18287.23 ms /   464 tokens\n",
      " 64%|██████▍   | 67/105 [15:17<08:21, 13.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.58 ms /    62 runs   (    0.07 ms per token, 13540.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4503.94 ms /   322 tokens (   13.99 ms per token,    71.49 tokens per second)\n",
      "llama_print_timings:        eval time =    5259.46 ms /    61 runs   (   86.22 ms per token,    11.60 tokens per second)\n",
      "llama_print_timings:       total time =    9850.95 ms /   383 tokens\n",
      " 65%|██████▍   | 68/105 [15:27<07:31, 12.19s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    85 runs   (    0.07 ms per token, 13634.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4479.93 ms /   305 tokens (   14.69 ms per token,    68.08 tokens per second)\n",
      "llama_print_timings:        eval time =    6544.20 ms /    84 runs   (   77.91 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:       total time =   11138.60 ms /   389 tokens\n",
      " 66%|██████▌   | 69/105 [15:38<07:07, 11.88s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.44 ms /   122 runs   (    0.08 ms per token, 12919.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15791.34 ms /  1139 tokens (   13.86 ms per token,    72.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8874.82 ms /   121 runs   (   73.35 ms per token,    13.63 tokens per second)\n",
      "llama_print_timings:       total time =   24903.48 ms /  1260 tokens\n",
      " 67%|██████▋   | 70/105 [16:03<09:12, 15.79s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.96 ms /    78 runs   (    0.08 ms per token, 13087.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8680.26 ms /   589 tokens (   14.74 ms per token,    67.86 tokens per second)\n",
      "llama_print_timings:        eval time =    6454.06 ms /    77 runs   (   83.82 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =   15250.63 ms /   666 tokens\n",
      " 68%|██████▊   | 71/105 [16:18<08:51, 15.63s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      12.60 ms /   173 runs   (    0.07 ms per token, 13732.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4675.94 ms /   327 tokens (   14.30 ms per token,    69.93 tokens per second)\n",
      "llama_print_timings:        eval time =   12378.50 ms /   172 runs   (   71.97 ms per token,    13.90 tokens per second)\n",
      "llama_print_timings:       total time =   17326.14 ms /   499 tokens\n",
      " 69%|██████▊   | 72/105 [16:35<08:52, 16.14s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.78 ms /   158 runs   (    0.07 ms per token, 13408.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4552.41 ms /   350 tokens (   13.01 ms per token,    76.88 tokens per second)\n",
      "llama_print_timings:        eval time =   12186.29 ms /   157 runs   (   77.62 ms per token,    12.88 tokens per second)\n",
      "llama_print_timings:       total time =   16960.40 ms /   507 tokens\n",
      " 70%|██████▉   | 73/105 [16:52<08:44, 16.39s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.07 ms /    84 runs   (    0.07 ms per token, 13831.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4379.36 ms /   336 tokens (   13.03 ms per token,    76.72 tokens per second)\n",
      "llama_print_timings:        eval time =    5774.36 ms /    83 runs   (   69.57 ms per token,    14.37 tokens per second)\n",
      "llama_print_timings:       total time =   10261.96 ms /   419 tokens\n",
      " 70%|███████   | 74/105 [17:03<07:31, 14.55s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.35 ms /    60 runs   (    0.07 ms per token, 13793.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4432.49 ms /   337 tokens (   13.15 ms per token,    76.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4128.95 ms /    59 runs   (   69.98 ms per token,    14.29 tokens per second)\n",
      "llama_print_timings:       total time =    8640.77 ms /   396 tokens\n",
      " 71%|███████▏  | 75/105 [17:11<06:23, 12.78s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.16 ms /    58 runs   (    0.07 ms per token, 13949.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4367.05 ms /   327 tokens (   13.35 ms per token,    74.88 tokens per second)\n",
      "llama_print_timings:        eval time =    3929.28 ms /    57 runs   (   68.93 ms per token,    14.51 tokens per second)\n",
      "llama_print_timings:       total time =    8370.63 ms /   384 tokens\n",
      " 72%|███████▏  | 76/105 [17:20<05:32, 11.46s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.05 ms /    75 runs   (    0.08 ms per token, 12400.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5002.88 ms /   320 tokens (   15.63 ms per token,    63.96 tokens per second)\n",
      "llama_print_timings:        eval time =    5766.64 ms /    74 runs   (   77.93 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   10875.72 ms /   394 tokens\n",
      " 73%|███████▎  | 77/105 [17:31<05:15, 11.28s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.97 ms /    76 runs   (    0.08 ms per token, 12732.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4720.40 ms /   318 tokens (   14.84 ms per token,    67.37 tokens per second)\n",
      "llama_print_timings:        eval time =    6327.79 ms /    75 runs   (   84.37 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =   11162.61 ms /   393 tokens\n",
      " 74%|███████▍  | 78/105 [17:42<05:03, 11.25s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       3.80 ms /    52 runs   (    0.07 ms per token, 13691.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2654.93 ms /   146 tokens (   18.18 ms per token,    54.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3493.33 ms /    51 runs   (   68.50 ms per token,    14.60 tokens per second)\n",
      "llama_print_timings:       total time =    6273.22 ms /   197 tokens\n",
      " 75%|███████▌  | 79/105 [17:48<04:13,  9.76s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.67 ms /    92 runs   (    0.07 ms per token, 13795.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5193.46 ms /   407 tokens (   12.76 ms per token,    78.37 tokens per second)\n",
      "llama_print_timings:        eval time =    7173.16 ms /    91 runs   (   78.83 ms per token,    12.69 tokens per second)\n",
      "llama_print_timings:       total time =   12492.71 ms /   498 tokens\n",
      " 76%|███████▌  | 80/105 [18:00<04:24, 10.58s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.34 ms /    59 runs   (    0.07 ms per token, 13597.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5549.97 ms /   387 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4262.49 ms /    58 runs   (   73.49 ms per token,    13.61 tokens per second)\n",
      "llama_print_timings:       total time =    9893.96 ms /   445 tokens\n",
      " 77%|███████▋  | 81/105 [18:10<04:09, 10.38s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /    75 runs   (    0.07 ms per token, 13748.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4683.48 ms /   355 tokens (   13.19 ms per token,    75.80 tokens per second)\n",
      "llama_print_timings:        eval time =    5188.92 ms /    74 runs   (   70.12 ms per token,    14.26 tokens per second)\n",
      "llama_print_timings:       total time =    9971.89 ms /   429 tokens\n",
      " 78%|███████▊  | 82/105 [18:20<03:55, 10.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.88 ms /    76 runs   (    0.08 ms per token, 12929.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4334.12 ms /   334 tokens (   12.98 ms per token,    77.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5577.45 ms /    75 runs   (   74.37 ms per token,    13.45 tokens per second)\n",
      "llama_print_timings:       total time =   10013.67 ms /   409 tokens\n",
      " 79%|███████▉  | 83/105 [18:30<03:44, 10.18s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.35 ms /   126 runs   (    0.07 ms per token, 13477.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4757.94 ms /   380 tokens (   12.52 ms per token,    79.87 tokens per second)\n",
      "llama_print_timings:        eval time =    9321.91 ms /   125 runs   (   74.58 ms per token,    13.41 tokens per second)\n",
      "llama_print_timings:       total time =   14254.99 ms /   505 tokens\n",
      " 80%|████████  | 84/105 [18:45<03:59, 11.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    90 runs   (    0.08 ms per token, 13090.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4541.60 ms /   327 tokens (   13.89 ms per token,    72.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7609.56 ms /    89 runs   (   85.50 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   12289.77 ms /   416 tokens\n",
      " 81%|████████  | 85/105 [18:57<03:53, 11.67s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.35 ms /    88 runs   (    0.07 ms per token, 13862.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4376.23 ms /   320 tokens (   13.68 ms per token,    73.12 tokens per second)\n",
      "llama_print_timings:        eval time =    6058.78 ms /    87 runs   (   69.64 ms per token,    14.36 tokens per second)\n",
      "llama_print_timings:       total time =   10552.40 ms /   407 tokens\n",
      " 82%|████████▏ | 86/105 [19:07<03:35, 11.34s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.66 ms /    62 runs   (    0.08 ms per token, 13301.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4415.28 ms /   325 tokens (   13.59 ms per token,    73.61 tokens per second)\n",
      "llama_print_timings:        eval time =    4560.50 ms /    61 runs   (   74.76 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =    9060.40 ms /   386 tokens\n",
      " 83%|████████▎ | 87/105 [19:17<03:11, 10.66s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.05 ms /    54 runs   (    0.08 ms per token, 13326.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4559.60 ms /   321 tokens (   14.20 ms per token,    70.40 tokens per second)\n",
      "llama_print_timings:        eval time =    4064.79 ms /    53 runs   (   76.69 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:       total time =    8701.39 ms /   374 tokens\n",
      " 84%|████████▍ | 88/105 [19:25<02:51, 10.07s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.20 ms /    78 runs   (    0.08 ms per token, 12590.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4573.68 ms /   341 tokens (   13.41 ms per token,    74.56 tokens per second)\n",
      "llama_print_timings:        eval time =    6698.34 ms /    77 runs   (   86.99 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =   11387.57 ms /   418 tokens\n",
      " 85%|████████▍ | 89/105 [19:37<02:47, 10.47s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.17 ms /    67 runs   (    0.08 ms per token, 12961.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4819.09 ms /   365 tokens (   13.20 ms per token,    75.74 tokens per second)\n",
      "llama_print_timings:        eval time =    4792.91 ms /    66 runs   (   72.62 ms per token,    13.77 tokens per second)\n",
      "llama_print_timings:       total time =    9707.50 ms /   431 tokens\n",
      " 86%|████████▌ | 90/105 [19:46<02:33, 10.24s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.65 ms /    75 runs   (    0.08 ms per token, 13281.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14919.27 ms /  1064 tokens (   14.02 ms per token,    71.32 tokens per second)\n",
      "llama_print_timings:        eval time =    5777.17 ms /    74 runs   (   78.07 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:       total time =   20799.71 ms /  1138 tokens\n",
      " 87%|████████▋ | 91/105 [20:07<03:07, 13.41s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.51 ms /    91 runs   (    0.07 ms per token, 13982.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15692.59 ms /  1106 tokens (   14.19 ms per token,    70.48 tokens per second)\n",
      "llama_print_timings:        eval time =    6643.57 ms /    90 runs   (   73.82 ms per token,    13.55 tokens per second)\n",
      "llama_print_timings:       total time =   22460.06 ms /  1196 tokens\n",
      " 88%|████████▊ | 92/105 [20:30<03:29, 16.13s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.85 ms /   106 runs   (    0.07 ms per token, 13503.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15377.62 ms /  1147 tokens (   13.41 ms per token,    74.59 tokens per second)\n",
      "llama_print_timings:        eval time =    8463.62 ms /   105 runs   (   80.61 ms per token,    12.41 tokens per second)\n",
      "llama_print_timings:       total time =   23991.32 ms /  1252 tokens\n",
      " 89%|████████▊ | 93/105 [20:54<03:41, 18.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.89 ms /    90 runs   (    0.08 ms per token, 13062.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17751.96 ms /  1205 tokens (   14.73 ms per token,    67.88 tokens per second)\n",
      "llama_print_timings:        eval time =    8864.87 ms /    89 runs   (   99.61 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =   26761.35 ms /  1294 tokens\n",
      " 90%|████████▉ | 94/105 [21:20<03:50, 20.97s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       9.60 ms /   113 runs   (    0.08 ms per token, 11769.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15265.20 ms /  1086 tokens (   14.06 ms per token,    71.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9282.77 ms /   112 runs   (   82.88 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =   24721.91 ms /  1198 tokens\n",
      " 90%|█████████ | 95/105 [21:45<03:41, 22.10s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /    64 runs   (    0.08 ms per token, 13195.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4773.98 ms /   336 tokens (   14.21 ms per token,    70.38 tokens per second)\n",
      "llama_print_timings:        eval time =    5235.06 ms /    63 runs   (   83.10 ms per token,    12.03 tokens per second)\n",
      "llama_print_timings:       total time =   10118.43 ms /   399 tokens\n",
      " 91%|█████████▏| 96/105 [21:55<02:46, 18.51s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.73 ms /    84 runs   (    0.08 ms per token, 12477.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4878.62 ms /   330 tokens (   14.78 ms per token,    67.64 tokens per second)\n",
      "llama_print_timings:        eval time =    7240.31 ms /    83 runs   (   87.23 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:       total time =   12242.22 ms /   413 tokens\n",
      " 92%|█████████▏| 97/105 [22:08<02:13, 16.64s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.19 ms /    73 runs   (    0.07 ms per token, 14070.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4952.04 ms /   331 tokens (   14.96 ms per token,    66.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5470.87 ms /    72 runs   (   75.98 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:       total time =   10520.57 ms /   403 tokens\n",
      " 93%|█████████▎| 98/105 [22:18<01:43, 14.80s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       6.40 ms /    89 runs   (    0.07 ms per token, 13901.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4116.96 ms /   320 tokens (   12.87 ms per token,    77.73 tokens per second)\n",
      "llama_print_timings:        eval time =    6144.33 ms /    88 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =   10377.72 ms /   408 tokens\n",
      " 94%|█████████▍| 99/105 [22:28<01:20, 13.48s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       4.91 ms /    67 runs   (    0.07 ms per token, 13640.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4136.23 ms /   320 tokens (   12.93 ms per token,    77.37 tokens per second)\n",
      "llama_print_timings:        eval time =    5365.87 ms /    66 runs   (   81.30 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:       total time =    9592.95 ms /   386 tokens\n",
      " 95%|█████████▌| 100/105 [22:38<01:01, 12.31s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       5.01 ms /    66 runs   (    0.08 ms per token, 13163.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3915.14 ms /   273 tokens (   14.34 ms per token,    69.73 tokens per second)\n",
      "llama_print_timings:        eval time =    4644.90 ms /    65 runs   (   71.46 ms per token,    13.99 tokens per second)\n",
      "llama_print_timings:       total time =    8652.12 ms /   338 tokens\n",
      " 96%|█████████▌| 101/105 [22:47<00:44, 11.22s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    91 runs   (    0.08 ms per token, 12250.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15110.19 ms /  1108 tokens (   13.64 ms per token,    73.33 tokens per second)\n",
      "llama_print_timings:        eval time =    9447.23 ms /    90 runs   (  104.97 ms per token,     9.53 tokens per second)\n",
      "llama_print_timings:       total time =   24695.74 ms /  1198 tokens\n",
      " 97%|█████████▋| 102/105 [23:11<00:45, 15.26s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =      11.87 ms /   163 runs   (    0.07 ms per token, 13735.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11897.76 ms /   946 tokens (   12.58 ms per token,    79.51 tokens per second)\n",
      "llama_print_timings:        eval time =   12070.48 ms /   162 runs   (   74.51 ms per token,    13.42 tokens per second)\n",
      "llama_print_timings:       total time =   24195.03 ms /  1108 tokens\n",
      " 98%|█████████▊| 103/105 [23:36<00:35, 17.94s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.10 ms /    98 runs   (    0.07 ms per token, 13806.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14670.87 ms /  1086 tokens (   13.51 ms per token,    74.02 tokens per second)\n",
      "llama_print_timings:        eval time =    8289.89 ms /    97 runs   (   85.46 ms per token,    11.70 tokens per second)\n",
      "llama_print_timings:       total time =   23099.07 ms /  1183 tokens\n",
      " 99%|█████████▉| 104/105 [23:59<00:19, 19.49s/it]Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   13122.86 ms\n",
      "llama_print_timings:      sample time =       7.50 ms /    99 runs   (    0.08 ms per token, 13201.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10012.97 ms /   681 tokens (   14.70 ms per token,    68.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7857.40 ms /    98 runs   (   80.18 ms per token,    12.47 tokens per second)\n",
      "llama_print_timings:       total time =   18013.18 ms /   779 tokens\n",
      "100%|██████████| 105/105 [24:17<00:00, 13.88s/it]\n"
     ]
    }
   ],
   "source": [
    "eval_results_dict = {}\n",
    "for parser_name, parser in parsers.items():\n",
    "    print(parser_name, \"\\n\")\n",
    "\n",
    "    nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    qa_dataset = generate_question_context_pairs(\n",
    "        nodes,\n",
    "        llm=llm,\n",
    "        num_questions_per_chunk=2\n",
    "    )\n",
    "\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "    retriever_evaluator = RetrieverEvaluator.from_metric_names(\n",
    "    [\"mrr\", \"hit_rate\"], retriever=retriever)\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = await retriever_evaluator.aevaluate_dataset(qa_dataset)  # Can't put this line in a function otherwise it raises an error\n",
    "    eval_results_dict[parser_name] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.935185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter       1.0  0.935185\n",
       "1   token_splitter_512       1.0  0.892857\n",
       "2  token_splitter_1024       1.0  0.875000"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1_results = os.path.join(results_folder, \"results_2_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = pd.read_csv(batch_1_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Splitter accuracy was way below others that is why it is not going to be used (cf results_2_docs_Sentence_Semantic_Token.csv)\n",
    "\n",
    "On two documents (7 chunks for SemanticSplitter, 5 for Token512 and 2 for Token1024) : Semantic has the higher score and increased token size for token splitter seems to lower HR and MRR\n",
    "\n",
    "Try to evaluate on more documents/chunks to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>0.801724</td>\n",
       "      <td>0.698276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>0.717143</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.622363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter  0.801724  0.698276\n",
       "1   token_splitter_512  0.717143  0.607143\n",
       "2  token_splitter_1024  0.721519  0.622363"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_2_results = os.path.join(results_folder, \"results_10_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = pd.read_csv(batch_2_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 10 documents (49 chunks for Semantic, 121 for Token512 and 52 for Token1024) : Semantic splitter is still better than TokenSplitter, but the Token1024 is slightly better than the 512\n",
    "\n",
    "Timer : 10.51 for Semantic, 20.40 for Token512, 14.57 for Token1024 ==> Meaning that, with increased database volume the Semantic Splitter is going to be more and more slow than the TokenSplitter BUT the results are way better so it is a trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>Hit Rate</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>semantic_splitter</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.684739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>token_splitter_512</td>\n",
       "      <td>0.766615</td>\n",
       "      <td>0.649923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>token_splitter_1024</td>\n",
       "      <td>0.763359</td>\n",
       "      <td>0.685751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Retriever Name  Hit Rate       MRR\n",
       "0    semantic_splitter  0.783133  0.684739\n",
       "1   token_splitter_512  0.766615  0.649923\n",
       "2  token_splitter_1024  0.763359  0.685751"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_3_results = os.path.join(results_folder, \"results_23_docs_Semantic_Token1024_Token512.csv\")\n",
    "\n",
    "df = compute_results(eval_results_dict)\n",
    "df.to_csv(batch_3_results, index=False, sep=\";\")\n",
    "\n",
    "df = pd.read_csv(batch_3_results, sep=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, on 23 documents (101 chunks for Semantic, 261 for Token512 and 105 for Token 1024) : Semantic splitter is still better than TokenSplitter but TokenSPlitter has increased its HitRate and MRR\n",
    "\n",
    "Timer : 26.36 for Semantic, 46.23 for Token512 and 24.17 for Token1024 ==> Meaning that, with increased database volume the Semantic Splitter is going to be more and more slow than the TokenSplitter and the results tend to decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = token_splitter_512.get_nodes_from_documents(documents)\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "retriever = vector_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1820"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "import glob\n",
    "import base64\n",
    "import pandas as pd\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "from llama_index.core.readers import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SentenceSplitter, SemanticSplitterNodeParser, TokenTextSplitter\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, RetrieverEvaluator, FaithfulnessEvaluator, RelevancyEvaluator, AnswerRelevancyEvaluator\n",
    "from transformers import AutoTokenizer\n",
    "import nest_asyncio\n",
    "from llama_index.llms.mistralai import MistralAI\n",
    "\n",
    "# To allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Define constants\n",
    "results_folder = os.path.join(\"data_evaluation\", \"full_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /Users/Calu/Library/Caches/llama_index/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1263.14 MiB, (13827.16 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1263.14 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 8192\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   704.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   320.00 MiB, (14147.16 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of documents : 2\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init:      Metal KV buffer size =   320.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   560.02 MiB, (14707.17 / 10922.67)ggml_backend_metal_log_allocated_size: warning: current allocated size is greater than the recommended max working set size\n",
      "llama_new_context_with_model:      Metal compute buffer size =   560.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   560.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '1000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.2'}\n",
      "Guessed chat format: mistral-instruct\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "input_folder = \"./data_evaluation/batch_1\"\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='2e0b8065-bc9c-4f24-8404-f9471eccfe02', embedding=None, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7284ba3-b5cd-4a37-bc72-5787e2839307', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='4047a7d64412d3e531cdb9bbe978a693d50df9bdeab913d42c6a1c1fcda8a504'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='46a08156-0cf0-47b3-bcdd-e39803af4cc1', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='7b0ab2d9340dbb7f356d097679549f5aaef9f19645aba40aabb75112b7fc5206')}, text=\"Arnell Hallowleaf : Arnell Hallowleaf is an NPC in Baldur's Gate 3. Arnell Hallowleaf can be found at The Chamber of Loss in the House of Grief. Arnell Hallowleaf is the father of Shadowheart whom she has since lost the memory of. There is a massive amount of NPCs in Baldur's Gate 3, and their ideals, needs and way of living are strictly related to the actions they perform during the course of the game. Their attitude towards you and your party may be affected by the deeds and decisions you and your party have taken on.   \\n\\nWhere to find Arnell Hallowleaf : Can be found at: Act 3 The Chamber of Loss, House of Grief  \\n\\nArnell Hallowleaf Related Quests : Daughter of Darkness\", start_char_idx=0, end_char_idx=681, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='46a08156-0cf0-47b3-bcdd-e39803af4cc1', embedding=None, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7284ba3-b5cd-4a37-bc72-5787e2839307', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='4047a7d64412d3e531cdb9bbe978a693d50df9bdeab913d42c6a1c1fcda8a504'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2e0b8065-bc9c-4f24-8404-f9471eccfe02', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='4dbf0e2f4792157fac210eb2946e02addcf8edf63816f16c772698f6b14b8c4c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='63f8af09-f075-4793-a225-15454b69b97f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='a08a7bc00aeede5fd8175d97c14e67c77bb0087dab19fd6cc563810d3cbcef44')}, text=\"Where to find Arnell Hallowleaf : Can be found at: Act 3 The Chamber of Loss, House of Grief  \\n\\nArnell Hallowleaf Related Quests : Daughter of Darkness  \\n\\nArnell Hallowleaf Dialogue Options : Arnell Hallowleaf is one of the two prisoners inside the Chamber of Loss who become part of Shadowheart's trial is she is still aligned with Shar. Your interactions here will be recorded in your journals for the Daughter of Darkness. Shadowheart can also enter The House of Grief in search of them. Enter the Chamber of Loss within the House of Grief. Here, you will find Shadowheart's parents, Arnell Hallowleaf and Emmeline Hallowleaf. Approach them and a cutscene will begin.   After a cutscene of Shadowheart's parents, Shar, the goddess of Night and Darkness give her two choices. Let her parents die, or let them live but continue to suffer from the wound. You will be given the following dialogue: You have to choose, Shadowheart. Free your parents, or rid yourself of Shar's curse.She wants to kill your parents. Her idea of a parting gift.Talk to your mother and father. See what they wish.  Shadowheart will be torn. Her parents will ask that she let them go so she will be free. You should end their suffering, and yours.Do not lose your parents, not again. This pain can be handled. .This is your choice, Shadowheart. you don't need me to tell you what is right.Remain silent.  If you choose the second option, she will bring up the curse again.  [RELIGION] You have great fatiha and great resolve - all of you. Trust in that. You need not say goodbye here. .[PERSUASION] You can endure it together, as a family. This is what you've been looking for - don't deny yourself.  Make your decision and complete the Daughter of Darkness.  If you saved them, you will find them at the Campsite.  \\n\\nArnell Hallowleaf Stats :\", start_char_idx=530, end_char_idx=2350, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='63f8af09-f075-4793-a225-15454b69b97f', embedding=None, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7284ba3-b5cd-4a37-bc72-5787e2839307', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='4047a7d64412d3e531cdb9bbe978a693d50df9bdeab913d42c6a1c1fcda8a504'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='46a08156-0cf0-47b3-bcdd-e39803af4cc1', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='dad57a4f4b4211963ef7bb26d5065334cb355e48f533c5f2c37ed997b86681b1'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='0510378c-363f-4400-a39e-aef3c92e4129', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='1baa43b9cb3964f0790833007b5d818850445c86dc27659483370f903945b0f4')}, text='Arnell Hallowleaf Stats : \\n\\nArnell Hallowleaf General Information : Race: ???Health: ???AC: ???Base speed: ???Size: ???Weight: ???Attitude: 0\\n\\nArnell Hallowleaf stats : Strength: ???Dexterity: ???Constitution: ???Intelligence: ???Wisdom: ???Charisma: ??? Proficiency Bonus: ??? Initiative: ???\\n\\nArnell Hallowleaf Passive Features : ??? Slashing: ????Piercing: ????Bludgeoning: ????Fire: ????Lightning: ????Psychic: ????Poison: ????\\n\\nArnell Hallowleaf Notable Loot : ????\\n\\nArnell Hallowleaf Notes and Tips : Notes & Tips', start_char_idx=2325, end_char_idx=2844, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='0510378c-363f-4400-a39e-aef3c92e4129', embedding=None, metadata={'file_path': \"/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Help_Kith'rak_Voss.txt\", 'file_name': \"Help_Kith'rak_Voss.txt\", 'file_type': 'text/plain', 'file_size': 1959, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='20d61b5f-e78d-4916-aca9-11778cf65a29', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': \"/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Help_Kith'rak_Voss.txt\", 'file_name': \"Help_Kith'rak_Voss.txt\", 'file_type': 'text/plain', 'file_size': 1959, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='7b723e87f3dbf72119085b2e024a49abbd83aef27e72c01f52b917808a58934f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='63f8af09-f075-4793-a225-15454b69b97f', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': '/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Arnell_Hallowleaf.txt', 'file_name': 'Arnell_Hallowleaf.txt', 'file_type': 'text/plain', 'file_size': 2846, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='036e74e30caaca2fead2cbf92ef15dd17c04844b49ef6a658e22faed456da212'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bc6fad6d-dc08-4628-9f0d-ac23538f4a12', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='16187d7da25a9be553305a7579f504ba18cfb8861122e8824bffc4819e709004')}, text=\"Help Kith'rak Voss : Help Kith'rak Voss is a Quest in Baldur's Gate 3. Help Kith'rak Voss can be acquired during Act TBA. This quest is part of the Personal Quests that you can get in the game.    \\n\\nHelp Kith'rak Voss Objectives : Kith'rak Voss visited our camp and asked us to help him free someone held prisoner within the Astral Prism. We agreed to bring the Prism to Baldur's Gate and meet him there.  \\n\\nHelp Kith'rak Voss Walkthrough : Help Kith'rak Voss takes place during Act 3. Talk to Lae'zel, she I'll say she wants to speak with Kith'rak Voss at Sharess' Caress. Voss can be found inside a room on the third floor of Sharess' Caress next to Raphael. Once Voss leaves the room, starts the new Deal with the DevilQuest (This doesn't deviate us from our main mission).Once we finish Raphael's Quest, we meet Voss again on the second floor of Sharess' Caress. The outcome of this interaction varies based on whether you accepted Raphael's contract or not. If you agree to Raphael's terms and sign the contract, he'll drop the Orphic Hammer. Presenting it to Voss and agreeing to liberate Orpheus will conclude the quest, earning you the Silver Sword of the Astral Plane.Refusing Raphael's Contract: Declining the contract doesn't preclude obtaining the hammer, albeit through a significantly more challenging route. You must infiltrate Raphael's House of Hope, his residence, and steal the hammer. However, this entails confronting and defeating Raphael, widely regarded as one of the toughest adversaries in the game. Instructions on accessing the House of Hope are provided below.Meet Voss at The Sewers and show him the hammer, and if you signed Raphael's contract, the interaction proceeds similarly as before, resulting in the acquisition of the Silver Sword.  \\n\\n How to unlock Help Kith'rak Voss : Rivington  \\n\\nHelp Kith'rak Voss Rewards : Orphic Hammer.Silver Sword of the Astral Plane\", start_char_idx=0, end_char_idx=1899, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " TextNode(id_='bc6fad6d-dc08-4628-9f0d-ac23538f4a12', embedding=None, metadata={'file_path': \"/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Help_Kith'rak_Voss.txt\", 'file_name': \"Help_Kith'rak_Voss.txt\", 'file_type': 'text/plain', 'file_size': 1959, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='20d61b5f-e78d-4916-aca9-11778cf65a29', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': \"/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Help_Kith'rak_Voss.txt\", 'file_name': \"Help_Kith'rak_Voss.txt\", 'file_type': 'text/plain', 'file_size': 1959, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='7b723e87f3dbf72119085b2e024a49abbd83aef27e72c01f52b917808a58934f'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='0510378c-363f-4400-a39e-aef3c92e4129', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': \"/Users/Calu/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/chatbot/data_evaluation/batch_1/Help_Kith'rak_Voss.txt\", 'file_name': \"Help_Kith'rak_Voss.txt\", 'file_type': 'text/plain', 'file_size': 1959, 'creation_date': '2024-05-03', 'last_modified_date': '2024-05-03'}, hash='4344765c13aac0824e7b538d2780b0f959fb6512ce07190897dd017b283ae291')}, text=\"How to unlock Help Kith'rak Voss : Rivington  \\n\\nHelp Kith'rak Voss Rewards : Orphic Hammer.Silver Sword of the Astral Plane  \\n\\nHelp Kith'rak Voss Notes & Tips : Notes & tips go here\", start_char_idx=1776, end_char_idx=1957, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MistralClient' object has no attribute 'complete'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m Settings\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m llm_mixtral\n\u001b[1;32m     12\u001b[0m Settings\u001b[38;5;241m.\u001b[39membed_model \u001b[38;5;241m=\u001b[39m embed_model\n\u001b[0;32m---> 14\u001b[0m qa_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_question_context_pairs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_mixtral\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_questions_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Code/Python/LLM/3.0_BG3_Chatbot_LLM_RAG_App/.venv/lib/python3.12/site-packages/llama_index/core/llama_dataset/legacy/embedding.py:86\u001b[0m, in \u001b[0;36mgenerate_qa_embedding_pairs\u001b[0;34m(nodes, llm, qa_generate_prompt_tmpl, num_questions_per_chunk)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_id, text \u001b[38;5;129;01min\u001b[39;00m tqdm(node_dict\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[1;32m     83\u001b[0m     query \u001b[38;5;241m=\u001b[39m qa_generate_prompt_tmpl\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     84\u001b[0m         context_str\u001b[38;5;241m=\u001b[39mtext, num_questions_per_chunk\u001b[38;5;241m=\u001b[39mnum_questions_per_chunk\n\u001b[1;32m     85\u001b[0m     )\n\u001b[0;32m---> 86\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomplete\u001b[49m(query)\n\u001b[1;32m     88\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(response)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     questions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     90\u001b[0m         re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, question)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[1;32m     91\u001b[0m     ]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MistralClient' object has no attribute 'complete'"
     ]
    }
   ],
   "source": [
    "# Setup your API KEY here\n",
    "api_key = input(\"Put your API key here\")\n",
    "\n",
    "# Load Mixtral 8x7b model\n",
    "llm_mixtral = MistralAI(api_key=api_key, endpoint=\"open-mixtral-8x7b\")\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "Settings.llm = llm_mixtral\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Create context question pairs\n",
    "qa_dataset = generate_question_context_pairs(\n",
    "    nodes,\n",
    "    llm=llm_mixtral,\n",
    "    num_questions_per_chunk=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Craft questions and context pairs which can be used in the assessment of the RAG system of both Retrieval and Response Evaluations\n",
    "input_folder = \"./data_evaluation/batch_1\"\n",
    "documents = SimpleDirectoryReader(input_dir=input_folder, recursive=True).load_data()\n",
    "print(f\"\\n\\nNumber of documents : {len(documents)}\\n\\n\")\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url='https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF/resolve/main/mixtral-8x7b-v0.1.Q4_K_M.gguf',\n",
    "    model_url='https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf',  # Q6_K was used too but quite slow\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=None,\n",
    "    temperature=0.0,  # Model needs to be factual and deterministic\n",
    "    max_new_tokens=512,\n",
    "    # Context size\n",
    "    context_window=8192, # Max is ~32k\n",
    "    # Kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # Set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 10},\n",
    "    # Transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "parsers = {}\n",
    "\n",
    "# Semantic splitter\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "embed_batch_size=128,\n",
    "normalize=True)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Semantic splitter\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "buffer_size=1, \n",
    "breakpoint_percentile_threshold=95, \n",
    "embed_model=embed_model)\n",
    "parsers[\"semantic_splitter\"] = semantic_splitter\n",
    "\n",
    "# Token splitter 512\n",
    "token_splitter_512 = TokenTextSplitter(chunk_size=512, chunk_overlap=50, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_512\"] = token_splitter_512\n",
    "\n",
    "# Token splitter 1024\n",
    "token_splitter_1024 = TokenTextSplitter(chunk_size=1024, chunk_overlap=102, separator=\"\\n\\n\")  # Don't put tokenizer from mistral model as it does not tokenize anything, resulting in a single chunk per document\n",
    "parsers[\"token_splitter_1024\"] = token_splitter_1024\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
